{"pages":[{"title":"标签列表","text":"","link":"/tags/index.html"}],"posts":[{"title":"Docker server gave HTTP response to HTTPS client 处理","text":"Docker server gave HTTP response to HTTPS client 处理 镜像推送 向私有仓库推送镜像的时候，提示server gave HTTP response to HTTPS client，出现这问题的原因是：Docker自从1.3.X之后docker registry交互默认使用的是HTTPS，但是搭建私有镜像默认使用的是HTTP服务，所以与私有镜像交时出现以上错误。 解决方法为： 修改/etc/docker/daemon.json文件 内容如下，加入 insecure-registries 1234{ &quot;registry-mirrors&quot;: [&quot;https://rqyzl64n.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;myregistry.example.com:5000&quot;]} 重启docker 1systemctl restart docker.service 启动失败处理 如果启动失败，可调用命令 systemctl status docker.service 查看原因。如上修改daemon.json文件后，提示错误，可能文件中的&quot;insecure-registries&quot;和其它配置文件冲突。查看 /lib/systemd/system/docker.service 文件，并没有配置 “insecure-registries”, 继续查看 Docker 环境配置文件 /etc/default/docker，发现有段配置: 1DOCKER_OPTS=&quot;-H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 --insecure-registry 192.168.31.249&quot; 将这句注释，再重启docker，就成功了。","link":"/2019/01/14/Docker-server-gave-HTTP-response-to-HTTPS-client-%E5%A4%84%E7%90%86/"},{"title":"GOPATH配置","text":"GOPATH配置 GOPATH是用来放自己的GO工程的。一般该目录下有三个文件夹：src、bin、pkg。环境变量%GOPATH%中可配置多个路径，中间用;隔开。 安装go依赖包 go依赖的包比较多，有些包不好下载，写了个批处理来下载。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@set TOOLS_DIR=%GOPATH%\\src\\golang.org\\x\\tools@set ORIGIANL_DIR=%CD%IF NOT EXIST &quot;%TOOLS_DIR%&quot; ( echo clone tools call git clone --progress -v &quot;https://github.com/golang/tools.git&quot; &quot;%TOOLS_DIR%&quot;) ELSE ( echo updating tools chdir /d &quot;%TOOLS_DIR%&quot; call git pull echo &quot;%ORIGIANL_DIR%&quot; chdir /d &quot;%ORIGIANL_DIR%&quot;)@set LINT_DIR=%GOPATH%\\src\\golang.org\\x\\lintIF NOT EXIST &quot;%LINT_DIR%&quot; ( echo clone lint call git clone --progress -v &quot;https://github.com/golang/lint.git&quot; &quot;%LINT_DIR%&quot;) ELSE ( echo updating lint chdir /d &quot;%LINT_DIR%&quot; call git pull echo &quot;%ORIGIANL_DIR%&quot; chdir /d &quot;%ORIGIANL_DIR%&quot;)echo go getting...:: 代码自动提示 go get -u -v github.com/nsf/gocode:: 代码之间跳转 go get -u -v github.com/rogpeppe/godef:: 搜索参考引用go get -u -v github.com/lukehoban/go-find-references :: go get -u -v github.com/lukehoban/go-outlinego get -u -v github.com/ramya-rao-a/go-outline:: The Vendor Tool for Gogo get -u -v github.com/kardianos/govendor:: delve调试工具 for vscodego get -u -v github.com/derekparker/delve/cmd/dlv:: 语法检查go get -u -v github.com/golang/lint/golint:: go get -u -v golang.org/x/lint/golintgo get -u -v github.com/uudashr/gopkgs/cmd/gopkgsgo get -u -v github.com/acroca/go-symbolsgo get -u -v golang.org/x/tools/cmd/goimportsgo get -u -v golang.org/x/tools/cmd/gorenamego get -u -v github.com/sqs/goreturnsgo get -u -v golang.org/x/tools/cmd/gurugo get -u -v github.com/cweill/gotests/... 批处理文件 点击下载 多工程GOPATH GOPATH 表示工作区路径，如果有多个工作区该如何配置呢？全局一个 GOPATH 和 每个项目一个单独的 GOPATH, 各有优缺点。目前采用的是每项目一个 GOPATH，在环境变量 GOPATH 里面添加多个项目路径。 采用 govendor 做包管理，要注意 vendor.json 中 rootPath 的设置，例如：“rootPath”: “shadowsocksR”， 则表示 vendor 文件夹路径为%GOPATH%\\src\\shadowsocksR。","link":"/2018/04/19/GOPATH%E9%85%8D%E7%BD%AE/"},{"title":"JAVA字符串常量池","text":"JAVA字符串常量池 关于Java字符串常量池的话题很多，先从JVM内存模型说起。 JVM 内存模型 根据 JVM 规范，JVM 内存共分为虚拟机栈、本地方法栈、堆、方法区、程序计数器五个部分。 虚拟机栈 Java虚拟机栈是Java方法执行的内存模型，每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。栈帧(Stack Frame)存储局部变量表，操作数栈，动态链接，方法出口等信息。会抛出StackOverflowError和OOM异常。 本地方法栈 jvm调用操作系统方法所使用的栈。本地方法栈和虚拟机栈非常相似，不同的是虚拟机栈服务的是Java方法，而本地方法栈服务的是Native方法。HotSpot虚拟机直接把本地方法栈和虚拟机栈合二为一。会抛出StackOverflowError和OOM异常。 堆 简单的来讲，堆内存用于存放由new创建的对象和数组，在堆中分配的内存，由java虚拟机自动垃圾回收器来管理。 方法区 方法区主要存储了class的一些信息，包括运行时常量池、Classloader的引用，字段数据，方法数据等。永久代（permanent generation）只是HotSpot VM曾经用来实现方法区的一个空间；在JDK8的HotSpot VM它被元数据区（Metaspace）代替。HotSpot中也称为永久代（Permanent Generation），（存储的是除了Java应用程序创建的对象之外，HotSpot虚拟机创建和使用的对象）。方法区在不同虚拟机中有不同的实现。 jdk7把字符串常量池移到了堆中。 jdk1.8中则把永久代给完全删除了，取而代之的是 MetaSpace。 程序计数器 jvm执行程序的流水线，存放一些跳转指令。 什么是JAVA字符串常量池 Java中的常量池，实际上分为两种形态：静态常量池和运行时常量池。 静态常量池，即*.class文件中的常量池，class文件中的常量池不仅仅包含字符串(数字)字面量，还包含类、方法的信息，占用class文件绝大部分空间。 静态常量池存储在方法区，如下图： 运行时常量池，则是jvm虚拟机在完成类装载操作后，将class文件中的常量池载入到内存中，并保存在方法区中，我们常说的常量池，就是指方法区中的运行时常量池。 运行时常量池（runtime constant pool）其中的引用类型常量（例如CONSTANT_String、CONSTANT_Class、CONSTANT_MethodHandle、CONSTANT_MethodType之类）都存的是引用，实际的对象还是存在Java heap上的。 HotSpot VM的StringTable的本体在native memory里。它持有String对象的引用而不是String对象的本体。被引用的String还是在Java heap里。一直到JDK6，这些被intern的String在permgen里，JDK7开始改为放在普通Java heap里。 运行时常量池，由于运行时常量池在方法区中，我们可以通过jvm参数：-XX:PermSize、-XX:MaxPermSize来设置方法区大小，从而间接限制常量池大小。 1234567//保持引用，防止自动垃圾回收List&lt;String&gt; l = new ArrayList&lt;&gt;();int i = 0;while(true){ //通过intern方法向常量池中手动添加常量 l.add(String.valueOf(i++).intern());} 程序立刻会抛出：Exception in thread “main” java.lang.outOfMemoryError: PermGen space异常。PermGen space正是方法区，足以说明常量池在方法区中。 JDK8 移除了方法区，转而用Metaspace区域替代,使用新的jvm参数：-XX:MaxMetaspaceSize=8M,运行上面的代码，程序没有抛出异常，但是会触发“Metadata GC Threshold”，说明运行时常量池是划分在Metaspace区域中","link":"/2018/02/07/JAVA%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%B8%B8%E9%87%8F%E6%B1%A0/"},{"title":"Java8之Lambda表达式","text":"Java 8 的最大变化是引入了 Lambda 表达式——一种紧凑的、传递行为的方式。Lambda 表达式是 Java 支持函数式编程的基础，也可以称之为闭包。 一个 Lambda 表达式 Java 8 之前排序，对 students 中的学生按照得分进行排序： 12345678public void sort(List&lt;Student&gt; students) { students.sort(new Comparator&lt;Student&gt;() { @Override public int compare(Student s1, Student s2) { return s1.getScore() - s2.getScore(); } });} 在Java 8里面，可以编写更为简洁的代码，这些代码读起来更接近问题的描述： 12345public void sort2(List&lt;Student&gt; students) { students.sort(comparing((s) -&gt; { return s.getScore(); }));} 这就是一个 Lambda 表达式，它念起来就是“给学生排序，比较得分的多少”。 上面的代码可改成： 123public void sort2(List&lt;Student&gt; students) { students.sort(comparing(Student::getScore));} ::语法 是Java 8的方法引用（即“把这个方法作为值”），比上面的写法更简洁。 什么是 Lambda 表达式 可以把 Lambda 表达式理解为简洁地表示可传递的匿名函数的一种方式：它没有名称，但它有参数列表、函数主体、返回类型，可能还有一个可以抛出的异常列表。 匿名 它不像普通的方法那样有一个明确的名称 函数 Lambda 函数不像方法那样属于某个特定的类。但和方法一样，Lambda 有参数列表、函数主体、返回类型，还可能有可以抛出的异常列表。 传递 Lambda 表达式可以作为参数传递给方法或存储在变量中。简单来说，就是在 Java 语法层面允许将函数当作方法的参数，函数可以当做对象。 Lambda 表达式的基本语法 语法格式： (parameters) -&gt; expression 或者 (parameters) -&gt; { statements; } 格式理解： (对应函数式接口的参数列表) -&gt; { 对应函数式接口的实现方法 } 简单范例： 1234public void simpleLambda1() { Runnable runnable = () -&gt; System.out.println(&quot;hello world&quot;); new Thread(runnable).start();} 123456public void simpleLambda2() { new Thread(() -&gt; { System.out.println(&quot;hello world&quot;); }).start();}} 函数式接口 Runnable 的定义如下： 123456789101112131415@FunctionalInterfacepublic interface Runnable { /** * When an object implementing interface &lt;code&gt;Runnable&lt;/code&gt; is used * to create a thread, starting the thread causes the object's * &lt;code&gt;run&lt;/code&gt; method to be called in that separately executing * thread. * &lt;p&gt; * The general contract of the method &lt;code&gt;run&lt;/code&gt; is that it may * take any action whatsoever. * * @see java.lang.Thread#run() */ public abstract void run();} 可以看到 Runnable 添加了 @FunctionalInterface 注解，这个注解用于表示该接口会设计成一个函数式接口。表明 Runnable 接口可用作函数接口。 该注解会强制 javac 检查一个接口是否符合函数接口的标准。如果该注解添加给一个枚举类型、类或另一个注解，或者接口包含不止一个抽象方法，javac 就会报错。 重构代码时，使用它能很容易发现问题。 函数接口是只有一个抽象方法的接口，用作 Lambda 表达式的类型。任一 Lambda 表达式都有且只有一个函数式接口与之对应，从这个角度来看，也可以说是 该函数式接口的实例化。 引用值 java 8 之前 使用匿名内部类，也许遇到过这样的情况：需要引用它所在方法里的变量。这时，需要将变量声明为 final，Java 8 虽然放松了这一限制，可以 引用非 final 变量，但是该变量在既成事实上必须是 final。 Lambda 表达式中引用既成事实上的 final 变量 123456public void simpleLambda3() { String ss = &quot;world&quot;; new Thread(() -&gt; { System.out.println(&quot;hello &quot; + ss); }).start();} 如果试图给ss赋值 1234567public void simpleLambda4() { String ss = &quot;world&quot;; new Thread(() -&gt; { System.out.println(&quot;hello &quot; + ss); }).start(); ss = &quot;小明&quot;;} 编译器报错，提示：“Variable used in lambda expression should be final or effectively final”。 不同形式的 Lambda 表达式 1234567891011121. Runnable noArguments = () -&gt; System.out.println(&quot;Hello World&quot;);2. Runnable multiStatement = () -&gt; { System.out.print(&quot;Hello&quot;); System.out.println(&quot; World&quot;); };3. ActionListener oneArgument = event -&gt; System.out.println(&quot;button clicked&quot;);4. BinaryOperator&lt;Long&gt; addExplicit = (Long x, Long y) -&gt; x + y;5. BinaryOperator&lt;Long&gt; add = (x, y) -&gt; x + y; 说明： 所示的 Lambda 表达式不包含参数，使用空括号 () 表示没有参数。该 Lambda 表达式实现了 Runnable 接口，该接口也只有一个 run 方法，没有参数， 且返回类型为 void。 所示的 Lambda 表达式不包含参数，主体是一段代码块，使用大括号（{}）将代码块括起来。 只有一行代码的 Lambda 表达式也可使用大括号，用以明确 Lambda 表达式从何处开始、到哪里结束。 所示的 Lambda 表达式包含且只包含一个参数，可省略参数的括号。 所示的 Lambda 表达式包含多个参数，并显式的声明了参数类型。这行代码并不是将两个数字相加，而是创建了一个函数，用来计算两个数字相加的结果。 所示的 Lambda 表达式包含多个参数，和 4 不同的是它参数类型是编译器推导出来的","link":"/2020/06/01/Java8%E4%B9%8BLambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"title":"Java send mail","text":"使用 Java 发送电子邮件主要有两个方法，一个是使用 sun 公司的 mail 包，另一个是使用 SpringBoot。SpringBoot 的封装更简单。 这里简单介绍使用 SpringBoot 发送邮件的方法和使用时的注意点。 SMTP/SMTPS 协议 SMTP（Simple Mail Transfer Protocal）称为简单邮件传输协议。SMTP 是一个请求/响应协议，它监听25号端口，用于接收用户的 Mail 请求，并与远端 Mail 服务器建立 SMTP 连接。 SMTPS（SMTP-over-SSL）为 SMTP 协议基于 SSL 安全协议之上的一种变种协议。它继承了 SSL 安全协议的非对称加密的高度安全可靠性，可防止邮件泄露。 如今绝大多数邮件服务器都使用 SMTP/SMTPS 协议。 引入依赖库 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt; &lt;version&gt;${spring-boot.version}&lt;/version&gt;&lt;/dependency&gt; 继续查看 spring-boot-starter-mail 的依赖树可以发现，它是对 sun 公司的 mail 包进行了封装。 参数配置 在 application.yml 里进行邮件发送的相关参数配置，示例如下： 123456789101112131415161718spring: mail: host: mail.163.com #发送邮件服务器 username: xxx@163.com #发送邮件的邮箱地址 password: xxx from: xXx@163.com # 发送邮件的地址，和上面username一致 to: sss@163.com default-encoding: utf-8 protocol: smtps port: 465 properties: mail: smtp: auth: true starttls: enable: true required: false test-connection: false 上面的配置有几个需要注意的点： password 是客户端授权码，有客户端授权码填授权码，没有的话，填发送邮箱的密码。 protocol 需要填写。 邮件服务器采用 SMTPS 发送协议，protocol 的值要写成 smtps；如果采用 SMTP，则要写成 smtp。 port 的键是 spring.mail.port 有些文章将 port 的键写成了 spring.mail.properties.mail.smtp.port，这可能是 SpringBoot 的版本原因，注意区分。 封装接口实现发送 封装 Service 实现邮件发送，一个简单的示例如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Servicepublic class MailService { @Value(&quot;${spring.mail.username}&quot;) private String from; @Value(&quot;${spring.mail.to:}&quot;) private String to; @Autowired private JavaMailSender javaMailSender; /** * 发送文本邮件 * * @param to 收件人 * @param subject 主题 * @param content 内容 */ public void sendTextMail(String to, String subject, String content) { SimpleMailMessage message = new SimpleMailMessage(); // 发送对象 if (null != to &amp;&amp; to.length() &gt; 0) { if (to.indexOf(&quot;,&quot;) &gt; 0) { message.setTo(to.split(&quot;,&quot;)); } else { message.setTo(to); } } else { message.setTo(from); } // 邮件主题 message.setSubject(subject); // 邮件内容 message.setText(content); // 邮件的发起者 message.setFrom(from); try { javaMailSender.send(message); } catch (org.springframework.mail.MailSendException e) { log.error(&quot;error:&quot; + from, e); } }} 注入 JavaMailSender，JavaMailSender 实现了邮件发送。上例是发送文本邮件，可发送多个人。","link":"/2020/12/23/Java-send-mail/"},{"title":"Java中的Random类","text":"前言 Java中生成随机数的方式有很多，Math.Random()、Random、ThreadLocalRandom、SecureRandom类等。不同的类和方法适用的产生随机数的场景也不一样。 伪随机数是用确定性的算法计算出来自[0,1]均匀分布的随机数序列。并不真正的随机，但具有类似于随机数的统计特征，如均匀性、独立性等。在计算伪随机数时，若 使用的初值（种子）不变，那么伪随机数的数序也不变。 Math.Random() Math.Random()函数能够返回带正号的double值，该值大于等于0.0且小于1.0，即取值范围是[0.0,1.0)的左闭右开区间，返回值是一个伪随机选择的数，在该 范围内（近似）均匀分布。例子如下： 12345public static void mathRandom() { System.out.println(&quot;Math.random()=&quot; + Math.random()); int num = (int) (Math.random() * 3); System.out.println(&quot;num=&quot; + num);} 输出 12Math.random()=0.8890842451831729num=2 Random类 Random类有两种构造方法： Random() 使用系统计时器的当前值作为随机种子来构建Random对象 Random(long seed) 使用指定 long 种子来构建Random对象 创建一个Random对象的时候可以给定任意一个合法的种子数，种子数只是随机算法的起源数字，和生成的随机数的区间没有任何关系。 123Random rand =new Random(10);int i;i=rand.nextInt(100); 初始种子为10，它对产生的随机数的范围并没有起作用,rand.nextInt(100);中的100是随机数的上限,产生的随机数为0-100的整数,不包括100。 对于种子相同的Random对象，生成的随机数序列是一样的 例子： 123456789101112Random ran1 = new Random(8);System.out.println(&quot;使用种子为8的Random对象生成[0,10)内随机整数序列: &quot;);for (int i = 0; i &lt; 10; i++) { System.out.print(ran1.nextInt(10) + &quot; &quot;);}System.out.println();Random ran2 = new Random(8);System.out.println(&quot;使用另一个种子为8的Random对象生成[0,10)内随机整数序列: &quot;);for (int i = 0; i &lt; 10; i++) { System.out.print(ran2.nextInt(10) + &quot; &quot;);} 输出： 1234使用种子为8的Random对象生成[0,10)内随机整数序列:4 6 0 1 2 8 1 1 3 0使用另一个种子为8的Random对象生成[0,10)内随机整数序列:4 6 0 1 2 8 1 1 3 0 相同的Random对象，多次调用nextInt生成的随机数序列会不同 例子： 12345678910Random ran2 = new Random(8);System.out.println(&quot;使用另一个种子为8的Random对象生成[0,10)内随机整数序列: &quot;);for (int i = 0; i &lt; 10; i++) { System.out.print(ran2.nextInt(10) + &quot; &quot;);}System.out.println();System.out.println(&quot;使用同一个种子为8的Random对象生成[0,10)内随机整数序列: &quot;);for (int i = 0; i &lt; 10; i++) { System.out.print(ran2.nextInt(10) + &quot; &quot;);} 输出： 1234使用另一个种子为8的Random对象生成[0,10)内随机整数序列:4 6 0 1 2 8 1 1 3 0使用同一个种子为8的Random对象生成[0,10)内随机整数序列:4 0 2 7 8 8 2 9 3 5 不同的Random对象，采用默认构造函数，生成的随机数序列不一样 例子： 12345678910111213Random r3 = new Random();System.out.println();System.out.println(&quot;使用种子缺省是当前系统时间的Random对象生成[0,10)内随机整数序列&quot;);for (int i = 0; i &lt; 10; i++) { System.out.print(r3.nextInt(10) + &quot; &quot;);}Random r4 = new Random();System.out.println();System.out.println(&quot;使用种子缺省是当前系统时间的Random对象生成[0,10)内随机整数序列&quot;);for (int i = 0; i &lt; 10; i++) { System.out.print(r4.nextInt(10) + &quot; &quot;);} 输出： 1234使用种子缺省是当前系统时间的Random对象生成[0,10)内随机整数序列7 6 6 1 1 5 4 3 3 5使用种子缺省是当前系统时间的Random对象生成[0,10)内随机整数序列2 6 2 3 1 1 5 2 8 1 “Random” objects should be reused Sonar 的代码审查提出“For better efficiency and randomness, create a single Random, then store, and reuse it.”，主要原因是： 创建Random是有代价的，如下面的例子： 1234public static int getRandom(int bound) { Random ran = new Random(); return ran.nextInt(bound);} getRandom每次调用都会构造一个新的Random对象，效率不高。 随机性不够好，如下面例子： 1234public static int getRandom(int bound) { Random ran = new Random(8); return ran.nextInt(bound);} 这样多次调用getRandom可能得到的值都是一样的，随机性不够好。其实这也是一种随机，只是均匀性，对立性不够了。 但是如果不需要频繁的获取平均数，将Random对象存储起来，可能会造成Random对象不能被gc，也会造成浪费。 线程安全性 Random是一个线程安全类，理论上可以通过它同时在多个线程中获得互不相同的随机数，但是它会因为多线程竞争同一个seed而造成性能下降，所以建议在多线程的 情况下采用ThreadLocalRandom来产生随机数。 Random在执行nextInt时，会调用next函数，代码如下： 123456789protected int next(int bits) { long oldseed, nextseed; AtomicLong seed = this.seed; do { oldseed = seed.get(); nextseed = (oldseed * multiplier + addend) &amp; mask; } while (!seed.compareAndSet(oldseed, nextseed)); return (int)(nextseed &gt;&gt;&gt; (48 - bits));} 由于seed的类型是AtomicLong，在计算nextseed时是原子操作，所以没有线程安全性的问题。 SecureRandom Random类只要种子一样，产生的随机数也一样： 因为种子确定，随机数算法也确定，因此输出就是确定的。 SecureRandom提供加密的强随机数生成器 (RNG)，要求种子必须是不可预知的，可产生非确定性输出 SecureRandom许多实现都是伪随机数生成器 (PRNG) 形式，这意味着它们将使用确定的算法根据实际的随机种子生成伪随机序列 SecureRandom和Random都是如果种子一样，产生的随机数也一样： 因为种子确定，随机数算法也确定，因此输出是确定的。 SecureRandom类收集了一些随机事件，比如鼠标点击，键盘点击等等，SecureRandom 使用这些随机事件作为种子。这意味着，种子是不可预测的，而不像Random默认使用系统当前时间的毫秒数作为种子，有规律可寻。 不当用法： 1234byte[] salt = new byte[128];SecureRandom secureRandom = new SecureRandom();secureRandom.setSeed(System.currentTimeMillis()); //使用系统时间作为种子secureRandom.nextBytes(salt); 例子中指定了当前系统时间作为种子，替代系统默认随机源。如果同一毫秒连续调用，则得到的随机数则是相同的。 系统默认的随机源取决于$JAVA_HOME/jre/lib/security/java.security配置中的securerandom.source属性。例如jdk1.8中该配置为： 1securerandom.source=file:/dev/random SecureRandom内置两种随机数算法，NativePRNG和SHA1PRNG，看实例化的方法了。默认来说会使用NativePRNG算法生成随机数。 一般来说尽量避免指定任何随机生成器，只需调用空参数构造函数：new SecureRandom()，让系统选择最好的随机数生成器。","link":"/2020/04/29/Java%E4%B8%AD%E7%9A%84Random%E7%B1%BB/"},{"title":"Java8流使用简介","text":"Stream API支持的许多操作。这些操作能让你快速完成复杂的数据查询，如筛选、切片、映射、查找、匹配和归约。 筛选和切片 Streams 接口支持 filter 方法。该操作会接受一个谓词（一个返回 boolean 的函数）作为参数，并返回一个包括所有符合谓词的元素的流。 流还支持一个叫作 distinct 的方法，它会返回一个元素各异（根据流所生成元素的 hashCode 和 equals 方法实现）的流。 看下面一个例子，筛选出列表中所有的偶数，并确保没有重复： 12345List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 1, 3, 3, 2, 4, 6);numbers.stream() .filter(i -&gt; i % 2 == 0) .distinct() .forEach(System.out::println); 输出： 123246 跳过元素 流还支持skip(n)方法，返回一个扔掉了前n个元素的流。如果流中元素不足n个，则返回一个空流。请注意，limit(n)和skip(n)是互补的。 如下例，跳过超过90分的头1个学生，并剩下的学生中的头3个的姓名： 1234567891011121314List&lt;String&gt; goodStudentNames = students.stream() .filter(s -&gt; { System.out.println(&quot;filter-&gt; &quot; + s.getName()); return s.getScore() &gt; GOOD; }) .skip(1) .map(s -&gt; { System.out.println(&quot;map-&gt; &quot; + s.getName()); return s.getName(); }) .limit(3) .collect(toList());System.out.println(goodStudentNames); 映射 Stream API 通过 map 和 flatMap 方法从某些对象中选择信息。 map方法，它会接受一个函数作为参数。这个函数会被应用到每个元素上，并将其映射成一个新的元素（使用映射一词，是因为它和转换类似，但其中的细微差别在于 它是“创建一个新版本”而不是去“修改”）。 给定一个单词列表，返回另一个列表，显示每个单词中有几个字母。可以像下例，给 map 传递一个方法引用 String::length 来解决这个问题。 12345List&lt;String&gt; words = Arrays.asList(&quot;Java 8&quot;, &quot;Lambdas&quot;, &quot;In&quot;, &quot;Action&quot;);List&lt;Integer&gt; wordLengths = words.stream() .map(String::length) .collect(toList());System.out.println(wordLengths); 输出： 1[6, 7, 2, 6] 扁平化 对于一张单词表，如何返回一张列表，列出里面各不相同的字符呢？例如，给定单词列表[“Hello”,“World”]，你想要返回列表[“H”,“e”,“l”, “o”,“W”,“r”,“d”]。 可以用 flatMap 来解决这个问题。程序如下： 123456789List&lt;String&gt; words = Arrays.asList(&quot;Hello&quot;, &quot;World&quot;);List&lt;String&gt; uniqueCharacters = words.stream() .map(w -&gt; w.split(&quot;&quot;)) .flatMap(Arrays::stream) .distinct() .collect(Collectors.toList());System.out.println(uniqueCharacters); Arrays.stream()的方法可以接受一个数组并产生一个流，如下例： 12String[] arrayOfWords = {&quot;Goodbye&quot;, &quot;Tom&quot;};Stream&lt;String&gt; streamOfwords = Arrays.stream(arrayOfWords); map(w -&gt; w.split(“”)) 得到 Stream&lt;String[]&gt;。 flatMap(Arrays::stream) 得到 Stream&lt;String&gt;，使用 flatMap 方法的效果是，各个数组并不是分别映射成一个流，而是映射成流的内容。所 有使用map(Arrays::stream)时生成的单个流都被合并起来，即扁平化为一个流。 如果是map map(Arrays::stream) 得到 Stream&lt;Stream&lt;String&gt;&gt; 再看一个问题：给定两个数字列表，如何返回所有的数对呢？例如，给定列表[1, 2, 3]和列表[3, 4]，应该返回 [(1, 3), (1, 4), (2, 3), (2, 4), (3, 3), (3, 4)]。为简单起见，用有两个元素的数组来代表数对。 12345678List&lt;Integer&gt; numbers1 = Arrays.asList(1, 2, 3);List&lt;Integer&gt; numbers2 = Arrays.asList(3, 4);List&lt;int[]&gt; pairs = numbers1.stream() .flatMap( i -&gt; numbers2.stream().map(j -&gt; new int[]{i, j}) ) .collect(toList()); 当 i = 1 时 i -&gt; numbers2.stream().map(j -&gt; new int[]{i, j}) 得到 Stream&lt;Int[]&gt; (1,3), Stream&lt;Int[]&gt; (1,4) 这一对流。 依次循环，会得到 3 对流，flatMap 对这 3 对流扁平化，新成包含 6 个数组的一个流。 扩展上面的问题，只返回总和能被3整除的数对呢？可利用 filter 来实现: 12345678910List&lt;Integer&gt; numbers1 = Arrays.asList(1, 2, 3);List&lt;Integer&gt; numbers2 = Arrays.asList(3, 4);List&lt;int[]&gt; pairs = numbers1.stream() .flatMap( i -&gt; numbers2.stream() .filter(j -&gt; (i + j) % 3 == 0) .map(j -&gt; new int[]{i, j}) ) .collect(toList()); 输出： 1[(2,4)(3,3)] 查找和匹配 Stream API通过allMatch、anyMatch、noneMatch、findFirst和findAny方法，来判断数据集中的某些元素是否匹配一个给定的属性。 归约 将流中所有元素反复结合起来，得到一个值，比如一个Integer。这样的查询可以被归类为归约操作（将流归约成一个值）。 元素求和 对流中所有的元素求和： 12List&lt;Integer&gt; numbers = Arrays.asList(4, 5, 3, 9);int sum = numbers.stream().reduce(0, (a, b) -&gt; a + b); reduce接受两个参数： 初始值，这里是0 BinaryOperator来将两个元素结合起来产生一个新值，这里我们用的是lambda (a, b) -&gt; a + b。 首先，0作为Lambda（a）的第一个参数，从流中获得4作为第二个参数（b）。0 + 4得到4，它成了新的累积值。 然后再用累积值和流中下一个元素5调用Lambda，产生新的累积值9。接下来，再用累积值和下一个元素3调用Lambda，得到12。 最后，用12和流中最后一个元素9调用Lambda，得到最终结果21。 在Java 8中，Integer类现在有了一个静态的 sum 方法来对两个数求和，用不着反复用Lambda写同一段代码了： 1int sum = numbers.stream().reduce(0, Integer::sum); 如果没有初始值，reduce 还有一个重载的变体，它不接受初始值，但是会返回一个Optional对象： 1Optional&lt;Integer&gt; sum = numbers.stream().reduce((a, b) -&gt; (a + b)); 最大值、最小值 我们利用刚刚学到的 reduce 来计算流中最大或最小的元素是否可能，该怎样做呢？ 正如前面描述的，reduce接受两个参数： 一个初始值 一个 Lambda 来把两个流元素结合起来并产生一个新值 那么我们需要一个给定两个元素能够返回最大值和最小值的 Lambda 表达式。 123Optional&lt;Integer&gt; min = numbers.stream().reduce((x, y) -&gt; x &gt; y ? y : x);Optional&lt;Integer&gt; max = numbers.stream().reduce(Integer::max);System.out.println(&quot;min: &quot; + min.orElse(0) + &quot; max: &quot; + max.orElse(0)); 输出： 1min: 3 max: 9 构建 值创建 使用静态方法 Stream.of，通过显式值创建一个流。它可以接受任意数量的参数。例如，以下代码直接使用 Stream.of 创建了一个字符串流。 然后将字符串转换为大写，再一个个打印出来： 12Stream&lt;String&gt; stream = Stream.of(&quot;Java 8 &quot;, &quot;Lambdas &quot;, &quot;In &quot;, &quot;Action&quot;);stream.map(String::toUpperCase).forEach(System.out::println); 数组创建 使用静态方法Arrays.stream从数组创建一个流。它接受一个数组作为参数。例如，将一个原始类型int的数组转换成一个IntStream，如下所示： 12int[] numbers = {2, 3, 5, 7, 10, 14};int sum = Arrays.stream(numbers).sum(); 文件生成流 Java中用于处理文件等I/O操作的NIO API 已更新，以便利用Stream API。java.nio.file.Files 中的很多静态方法都会返回一个流。 如下例： 12345678910long uniqueWords = 0;try(Stream&lt;String&gt; lines = Files.lines(Paths.get(&quot;data.txt&quot;), Charset.defaultCharset())){uniqueWords = lines.flatMap(line -&gt; Arrays.stream(line.split(&quot; &quot;))) .distinct() .count();} catch(IOException e){} 使用Files.lines得到一个流，其中的每个元素都是给定文件中的一行。然后对line调用split方法将行拆分成单词。应该注意的是，该如何使用flatMap产生一个扁 平的单词流，而不是给每一行生成一个单词流。最后，把distinct和count方法链接起来，数数流中有多少各不相同的单词。 函数生成流 Stream API提供了两个静态方法来从函数生成流：Stream.iterate和Stream.generate。这两个操作可以创建所谓的无限流。 一般来说，应该使用limit(n)来对这种流加以限制。 迭代 123Stream.iterate(0, n -&gt; n + 2) .limit(10) .forEach(System.out::println); iterate 方法接受一个初始值（在这里是0），还有一个依次应用在每个产生的新值上的Lambda。这里，我们使用n -&gt; n + 2，返回的是前一个元 素加上 2。因此，iterate 方法生成了一个所有正偶数的流：流的第一个元素是初始值 0。然后加上 2 来生成新的值 2，再加上 2 来得到新的值 4，以此类推。 这种iterate操作基本上是顺序的，因为结果取决于前一次应用。此操作将生成一个无限流——这个流没有结尾，因为值是按需计算的，可以永远计算下去。 我们说这个流是无界的。使用limit方法来显式限制流的大小。这里只选择了前10个偶数。然后可以调用forEach终端操作来消费流，并分别打印每个元素。 生成 generate 方法也可让你按需生成一个无限流。但 generate 不是依次对每个新生成的值应用函数的。它接受一个 Supplier 类型的 Lambda 提供新的值。 我们先来看一个简单的用法： 123Stream.generate(Math::random) .limit(5) .forEach(System.out::println); 这段代码将生成一个流，其中有五个0到1之间的随机双精度数。","link":"/2020/06/12/Java8%E6%B5%81%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/"},{"title":"Java单例模式","text":"软件设计模式（Design pattern），又称设计模式，是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。 单例模式（Singleton Pattern）的定义是：保证一个类仅有一个实例，并提供一个访问它的全局访问点。单例模式是一种常用的模式，有一些对象我们往往 只需要一个，比如线程池、全局缓存。单例模式一般被认为是最简单、最易理解的设计模式，也因为它的简洁易懂，是项目中最常用、最易被识别出来的模式。但 单例模式要用好、用对并不是一件简单的事。 单例模式的问题 单例模式可以有多种实现方法，需要根据情况作出正确的选择 单例模式极易被滥用 如果某个工程中出现了太多单例，就应该重新审视一下设计。 单例模式的争议 单例既负责实例化类并提供全局访问，又实现了特定的业务逻辑，一定程度上违背了“单一职责原则”，是反模式的。 单例模式将全局状态（global state）引入了应用，全局状态会引入状态不确定性（state indeterminism），导致微妙的副作用，很容易就会破坏了 单元测试的有效性。 单例导致了类之间的强耦合，扩展性差，违反了面向对象编程的理念。 单例封装了自己实例的创建，不适用于继承和多态，同时创建时一般也不传入参数等，难以用一个模拟对象来进行测试。 Java单例模式的实现 懒加载，线程不安全 1234567891011public class Singleton { private static Singleton instance; private Singleton (){} public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; }} 懒加载，线程安全 12345678910public class Singleton { private static Singleton instance; private Singleton (){} public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; }} 这种写法能够在多线程中很好的工作，而且看起来它也具备很好的lazy loading，但由于加上了同步机制导致效率降低。 静态常量 1234567public class Singleton { private final static Singleton instance = new Singleton(); private Singleton(){} public static Singleton getInstance(){ return instance; }} 这种方式基于 classloder 机制，在类加载的时候就完成实例化，Java类加载器初始化静态资源过是线程安全的，避免了线程同步问题。但在类装载的时候就 完成实例化，如果从始至终从未使用过这个实例，则会造成内存的浪费。 静态内部类 123456789public class Singleton { private static class SingletonHolder { private static final Singleton instance = new Singleton(); } private Singleton (){} public static final Singleton getInstance() { return SingletonHolder.instance; }} 同样利用了 classloder 的机制来保证初始化 instance 时只有一个线程，它和“静态常量”方式不同：“静态常量”方式是只要 Singleton 类被装载了，那么 instance 就会被实例化（没有达到 lazy loading 效果，可能造成资源浪费），而“静态内部类”方式是 Singleton 类被装载了，instance 不一定被初始化。 因为 SingletonHolder 类没有被主动使用，只有显示通过调用 getInstance 方法时，才会显示装载 SingletonHolder 类，从而实例化 instance。 双重校验锁 例子： 1234567891011121314public class Singleton { private static Singleton instance; private Singleton (){} public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; }} 第一次校验：由于单例模式只需要创建一次实例，如果后面再次调用 getInstance 方法时，则直接返回之前创建的实例，因此大部分时间不需要执行同步方法里面 的代码，大大提高了性能。如果不加第一次校验的话，那跟上面的“静态常量”方式没什么区别，每次都要去竞争锁。 第二次校验：如果没有第二次校验，假设线程t1执行了第一次校验后，判断为null，这时t2也获取了CPU执行权，也执行了第一次校验，判断也为null。 接下来t2获得锁，创建实例。这时t1又获得CPU执行权，由于之前已经进行了第一次校验，结果为null（不会再次判断），获得锁后，直接创建实例。结果就会导致 创建多个实例。所以需要在同步代码里面进行第二次校验，如果实例为空，则进行创建。 但这个例子是错误的，问题的原因在于JVM指令重排优化。 instance = new Singleton(); 这行，当某个线程执行这行语句时，构造函数的调用似乎应该在 instance 得到赋值之前发生，但是在java虚拟机内部， 却不是这样的，完全有可能先new出来一个空的未调用过构造函数的 instance 对象，然后再将其赋值给 instance 引用，然后再调用构造函数， 对 instance 对象当中的元素进行初始化。若紧接着另外一个线程来调用 getInstance，取到的就是状态不正确的对象，程序可能就会出错。 枚举 12345public enum Singleton { INSTANCE; public void something() { }} 枚举的方式实现单例，可以保证线程安全。 破除单例模式的方法 克隆 如果类继承了 Cloneable 接口，并且实现了 clone 方法，尽管这个类的构造函数是私有的，还是可以创建一个对象。 12345678910111213141516171819202122232425262728293031323334public class Singleton implements Cloneable { private static Singleton instance; private Singleton (){} private String id = &quot;999&quot;; public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } @Override public Object clone() throws CloneNotSupportedException { return super.clone(); }}public static void singleton(){ com.shrill.singleton.Singleton s = com.shrill.singleton.Singleton.getInstance(); com.shrill.singleton.Singleton s1 = null; try { s1 = (com.shrill.singleton.Singleton) s.clone(); System.out.println(&quot;s hashCode:&quot; + s.hashCode()); System.out.println(&quot;s1 hashCode:&quot; + s1.hashCode()); } catch (CloneNotSupportedException e) { e.printStackTrace(); }}// 输出s hashCode:1618212626s1 hashCode:1129670968 hash 值不一样，所以克隆成功了，生成了一个新对象。单例模式被成功破坏！ 序列化 假设你的单例模式，实现了 Serializable 接口，那模式可能会被破坏。如下例： 123456789101112131415161718192021222324252627282930313233343536373839404142public class SerSingleton implements Serializable { private static SerSingleton instance; private SerSingleton() { } private String id = &quot;999&quot;; public static synchronized SerSingleton getInstance() { if (instance == null) { instance = new SerSingleton(); } return instance; }}public static void serSingleton() { SerSingleton s1 = null; SerSingleton s = SerSingleton.getInstance(); try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;z:\\\\a.bin&quot;))) { oos.writeObject(s); oos.flush(); } catch (IOException e) { e.printStackTrace(); } try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(&quot;z:\\\\a.bin&quot;))) { s1 = (SerSingleton) ois.readObject(); if (null != s1) { System.out.println(&quot;s hashCode:&quot; + s.hashCode()); System.out.println(&quot;s1 hashCode:&quot; + s1.hashCode()); } } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); }}// 输出s hashCode:1729199940s1 hashCode:127618319 解决办法，实现 readResolve 方法，返回 instance。 123private Object readResolve() { return instance;} 反射 通过反射可以执行对象的非公开方法，那么就可以执行对象的私有构造方法，从而构造出一个新的对象，单例模式又被破坏了。解决方法： 12345private SerSingleton() { if (null != instance) { throw new RuntimeException(); }} 再说枚举单例模式 前面提到枚举的方式实现单例，可以保证线程安全，它是怎样做到呢？ 定义枚举时使用enum和class一样，是Java中的一个关键字。就像class对应用一个Class类一样，enum也对应有一个Enum类。通过将定义好的枚举反编译就能 发现，枚举在经过javac的编译之后，会被转换成形如public final class T extends Enum的定义。而且，枚举中的各个枚举项是通过static来定义的。 如： 12345678910111213public enum SingletonEnum implements Serializable, Cloneable { INSTANCE; private String id = &quot;999&quot;; SingletonEnum() { System.out.println(&quot;SingletonEnum create&quot;); } public int getHashCode() { return this.hashCode(); }} 反编译结果： 1234567public final class com.shrill.singleton.SingletonEnum extends java.lang.Enum&lt;com.shrill.singleton.SingletonEnum&gt; implements java.io.Serializable, java.lang.Cloneable { public static final com.shrill.singleton.SingletonEnum INSTANCE; public static com.shrill.singleton.SingletonEnum[] values(); public static com.shrill.singleton.SingletonEnum valueOf(java.lang.String); public int getHashCode(); static {};} 可以看到 INSTANCE 变量加上静态属性，而Java类加载器初始化静态资源过是线程安全的，所以，创建一个enum类型是线程安全的。 由于枚举没有 clone 方法，即是实现了 Cloneable 也没有 clone 方法可被调用，所以通过 enum 实现的单例模式不会被 clone 破坏。 对于序列化，枚举的特殊之处又在哪里？普通的Java类的反序列化过程中，会通过反射调用类的默认构造函数来初始化对象。即使单例中构造函数是私有的，也会被 反射给破坏掉。由于反序列化后的对象是重新 new 出来的，所以这就破坏了单例。Java 序列化枚举对象仅仅是将枚举对象的name属性输出到结果中，反序列化的 时候则是通过java.lang.Enum的valueOf方法来根据名字查找枚举对象。同时，编译器是不允许任何对枚举这种序列化机制的定制，因此禁用了 writeObject、readObject等方法。所以通过 enum 实现的单例模式不会被 序列化 破坏。valueOf方法： 12345678910public static &lt;T extends Enum&lt;T&gt;&gt; T valueOf(Class&lt;T&gt; enumType, String name) { T result = enumType.enumConstantDirectory().get(name); if (result != null) return result; if (name == null) throw new NullPointerException(&quot;Name is null&quot;); throw new IllegalArgumentException( &quot;No enum constant &quot; + enumType.getCanonicalName() + &quot;.&quot; + name);} 首先调用 enumType 这个 Class 对象的 enumConstantDirectory 方法返回的 map 中获取名字为name的枚举对象，如果不存在就会抛出异常。 跟进enumConstantDirectory方法,发现到最后会以反射的方式调用 enumType 这个类型的 values() 静态方法，也就是上面我们看到的编译器为我们创建的那个方法，然后用返回结果填充enumType这个Class对象中的enumConstantDirectory属性。 例子： 1234567891011121314151617181920212223242526public static void singletonEnum() { SingletonEnum s = SingletonEnum.INSTANCE; try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;z:\\\\b.bin&quot;))) { oos.writeObject(s); oos.flush(); } catch (IOException e) { e.printStackTrace(); } SingletonEnum s1 = null; try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(&quot;z:\\\\b.bin&quot;))) { s1 = (SingletonEnum) ois.readObject(); if (null != s1) { System.out.println(&quot;s hashCode:&quot; + s.hashCode()); System.out.println(&quot;s1 hashCode:&quot; + s1.hashCode()); } } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); }}//输出:SingletonEnum creates hashCode:1020391880s1 hashCode:1020391880s equals s1 true 说明序列化后是同一个对象。 java 语言规范禁止对枚举类型的反射实例化。所以反射也对枚举实现的单例不能起破坏作用。例子： 1234567891011121314151617181920212223public static void singletonEnum1() { SingletonEnum s = SingletonEnum.INSTANCE; System.out.println(&quot;s hashCode:&quot; + s.getHashCode()); Class&lt;?&gt; clz = SingletonEnum.class; try { Constructor&lt;?&gt; c = clz.getConstructor(); Object rcvr = c.newInstance(); Method method = rcvr.getClass().getMethod(&quot;getHashCode&quot;); method.invoke(rcvr); } catch (IllegalArgumentException | NoSuchMethodException | SecurityException | IllegalAccessException | InvocationTargetException | InstantiationException e) { e.printStackTrace(); }}输出：SingletonEnum creates hashCode:1618212626java.lang.NoSuchMethodException: com.shrill.singleton.SingletonEnum.&lt;init&gt;() at java.lang.Class.getConstructor0(Class.java:3082) at java.lang.Class.getConstructor(Class.java:1825) at com.shrill.SingletonApp.singletonEnum(SingletonApp.java:62) 反射会抛出异常。","link":"/2020/05/14/Java%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"title":"Java的NIO说明","text":"Java的NIO说明 Java NIO，是Java SE 1.4版以后，针对网络传输效能优化的新功能。在Java 7时再推出NIO 2，针对档案存取的效能优化。 有些资料将 NIO 称之为 Non-block I/O，即非阻塞I/O，这个说法不是很正确。NIO 应该称为 New I/O，参考JSR 51: New I/O APIs for the JavaTM Platform。因为NIO支持阻塞和非阻塞这两种模式，所以将NIO称之为 Non-block I/O 就不准确了。 NIO.2是“APIs for filesystem access, scalable asynchronous I/O operations, socket-channel binding and configuration, and multicast datagrams. ”，上述描述抄自JSR 203，简单来说就是一些 API 用于文件系统访问、可伸缩的异步 i/o 操作、套接字通道绑定和配置以及多播数据报。 由于NIO.2具有异步能力，所以NIO.2又被称为AIO(Asynchronous I/O)。 名词解释 阻塞和非阻塞 阻塞调用在调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用在不能立刻得到结果之前，该调用不会阻塞当前线程。 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。 同步和异步 同步，就是在发出一个“调用”时，在没有得到结果之前，该“调用”就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由“调用者”主动等待结果。 异步则是相反，“调用”在发出之后，这个调用就直接返回了，没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在“调用”发出后，“被调用者”通过状态、通知来通知调用者，或回调处理这个调用。 同步和异步关注的是获取结果的方式。[1] 从上面的解释来看，虽然阻塞/非阻塞和同步/异步是两组关注点不同的概念，还是存在一些疑惑： 阻塞和同步的区别是什么？ 阻塞调用会一直block住对应的进程（线程）直到操作完成，数据从kernel中拷贝到用户内存后，用户进程（线程）解除blocking。同步可以是阻塞的也可以是非阻塞的。 非阻塞和异步的区别是什么？ 非阻赛是在kernel还在准备数据的情况下会立刻返回，用户进程（线程）可通过轮询kernel等方式，直到数据准备好再拷贝数据到用户内存来获取数据，这种方式就是同步非阻塞。异步就是用户进程将整个操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据，kernel已经将数据拷贝至用户内存。 NIO与AIO 通过上面的解释我们知道了NIA与AIO的区别，由于AIO具备异步的特性，它不需要采用轮询的方式来获取数据，只需等待通知就可以获取数据，摒除了空等，相对NIO来说性能有所提高。 Java NIO direct buffer的优势 以下是官方文档给出的说明： Given a direct byte buffer, the Java virtual machine will make a best effort to perform native I/O operations directly upon it. That is, it will attempt to avoid copying the buffer’s content to (or from) an intermediate buffer before (or after) each invocation of one of the underlying operating system’s native I/O operations. 大致意思是：给定一个直接字节缓冲区，Java虚拟机将尽最大努力直接对它执行本地 I/O 操作。也就是说，它将尝试避免在每次调用某个底层操作系统的本地 I/O操作之前（或之后）将缓冲区内容复制到中间缓冲区（或从中间缓冲区复制到缓冲区）。 优势很明显，减少一次拷贝过程。 NIO 操作的时候，那么是不是有这么一个中间缓冲区呢？ Java NIO 在读写到相应的 Channel 的时候，会先将 Java Heap 的 buffer 内容拷贝至直接内存—— Direct Memory。这样的话，采用 DirectByteBuffer 的性能肯定强于使用 HeapByteBuffer，它省去了临时buffer的拷贝开销，这也是为什么各个NIO框架大多使用DirectByteBuffer的原因。 绝大部分Channel类都是通过sun.nio.ch.IOUtil，这个工具类和外界进行通讯的，如FileChannel/SocketChannel等等，查看sun.nio.ch.IOUtil#read和sun.nio.ch.IOUtil#write代码可以证明这点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static int read(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { if (var1.isReadOnly()) { throw new IllegalArgumentException(&quot;Read-only buffer&quot;); } else if (var1 instanceof DirectBuffer) { return readIntoNativeBuffer(var0, var1, var2, var4); } else { ByteBuffer var5 = Util.getTemporaryDirectBuffer(var1.remaining()); int var7; try { int var6 = readIntoNativeBuffer(var0, var5, var2, var4); var5.flip(); if (var6 &gt; 0) { var1.put(var5); } var7 = var6; } finally { Util.offerFirstTemporaryDirectBuffer(var5); } return var7; }}static int write(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { if (var1 instanceof DirectBuffer) { return writeFromNativeBuffer(var0, var1, var2, var4); } else { int var5 = var1.position(); int var6 = var1.limit(); assert var5 &lt;= var6; int var7 = var5 &lt;= var6 ? var6 - var5 : 0; ByteBuffer var8 = Util.getTemporaryDirectBuffer(var7); int var10; try { var8.put(var1); var8.flip(); var1.position(var5); int var9 = writeFromNativeBuffer(var0, var8, var2, var4); if (var9 &gt; 0) { var1.position(var5 + var9); } var10 = var9; } finally { Util.offerFirstTemporaryDirectBuffer(var8); } return var10; }} 从上面的代码可以看出，如果 var1 是 Directbuffer 就直接拷贝（或写入），否则创建一个临时 Directbuffer，将 var1 写入这个临时 Directbuffer，然后再拷贝（或写入）。 Java为什么在执行网络IO或者文件IO时，一定要通过堆外内存呢？ HeapByteBuffer 内存是分配在堆上的，直接由 Java 虚拟机负责垃圾收集，DirectByteBuffer 是通过 JNI 在 Java 虚拟机外的内存中分配了一块内存（所以即使在运行时通过 -Xmx 指定了 Java 虚拟机的最大堆内存，还是可能实例化超出该大小的 Direct ByteBuffer），DirectByteBuffer 是用户空间的，它的创建是使用了 malloc 申请的内存，该内存块并不直接由 Java 虚拟机负责垃圾收集，但是在 Direct ByteBuffer 包装类被回收时，会通过 Java Reference 机制来释放该内存块。 当把一个地址通过JNI传递给底层的C库的时候，有一个基本的要求，就是这个地址上的内容不能失效。然而，在GC管理下的对象是会在Java堆中移动的。也就是说，有可能把一个地址传给底层的write，但是这段内存却因为GC整理内存而失效了。所以必须要把待发送的数据放到一个GC管不着的地方。这就是调用native方法之前，数据一定要在堆外内存的原因。DirectBuffer 没有省内存拷贝，但是使用HeapBuffer却需要多一次拷贝，所以相对来说Directbuffer要快。 此外，Directbuffer 的 GC 压力更小。虽然 GC 仍然管理着 DirectBuffer 的回收，但它是使用 PhantomReference 来达到的，在平常的 Young GC 或者 mark and compact 的时候却不会在内存里搬动。如果IO的数量比较大，比如在网络发送很大的文件，那么 GC 的压力下降就会很明显。[2] 后记 I/O模型中只有同步阻塞、同步非阻塞和异步，没有异步非阻塞。 所有的系统I/O都分为两个阶段：等待就绪和操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。 下图是几种常见I/O模型的对比： 下图是阻塞/非阻塞和同步/异步的小结： 参考 怎样理解阻塞非阻塞与同步异步的区别？ Java NIO direct buffer的优势在哪儿？","link":"/2018/03/12/Java%E7%9A%84NIO%E8%AF%B4%E6%98%8E/"},{"title":"Navicat地址","text":"Navicat下载地址 国内Navicat网址是：http://www.navicat.com.cn 网站只有中文版下载。 国外Navicat网址是：http://www.navicat.com 国外 Navicat Premium下载地址： http://download.navicat.com/download/navicat9_premium_cs.exe http://download.navicat.com/download/navicat100_premium_cs.exe http://download.navicat.com/download/navicat110_premium_cs_x86.exe http://download.navicat.com/download/navicat110_premium_cs_x64.exe http://download.navicat.com/download/navicat112_premium_cs_x86.exe http://download.navicat.com/download/navicat112_premium_cs_x64.exe http://download.navicat.com/download/navicat120_premium_cs_x86.exe http://download.navicat.com/download/navicat120_premium_cs_x64.exe 需要下载英文版，将地址中的cs改成en即可。文件名字带有版本号规则，如11.0, 11.2, 12.0等。 以上地址下载的都是对应版本的最新版。","link":"/2018/02/07/Navicat%E5%9C%B0%E5%9D%80/"},{"title":"Kafka日志管理","text":"Kafka 启动后，会产生很多日志，包括程序运行日志和消息日志，存在把磁盘撑爆的风险，所以为了 Kafka 能够正常的运行，对它进行日志管理是必要的一环。 Kafka运行日志 修改kafka-run-class.sh Kafka 运行时日志默认输出到 $KAFKA_HOME/logs 目录下，需要将日志输出到指定分区(应该选择一个磁盘空间比较大的分区)。 比如 /data/kafka/logs 目录下。 修改脚本 $KAFKA_HOME/bin/kafka-run-class.sh。 $KAFKA_HOME 为 Kafka 的安装路径。 打开 kafka-run-class.sh，定位到LOG_DIR 1234# Log directory to useif [ &quot;x$LOG_DIR&quot; = &quot;x&quot; ]; then LOG_DIR=&quot;$base_dir/logs&quot;fi 增加一行，修改为 12345LOG_DIR=/data/kafka/logs# Log directory to useif [ &quot;x$LOG_DIR&quot; = &quot;x&quot; ]; then LOG_DIR=&quot;$base_dir/logs&quot;fi log4j.properties Kafka 采用 log4j 进行日志信息输送控制，Kafka 日志管理的配置文件为 log4j.properties，位于$KAFKA_HOME/config/log4j.properties。 在生产环境下，建议把日志级别改为 error 级。 Kafka 的 log4j.properties 中采用 DailyRollingFileAppender 按天进行日志备份，不支持只保留最近 n 天的数据，时间一久导致日志文件很多， 并且一天的文件有可能比较大，所以可以把 DailyRollingFileAppender 改成 RollingFileAppender，限制日志文件的大小和备份的个数。 但如果要求必须按天保存的话，也就只能 DailyRollingFileAppender 进行日志备份了，这时就要注意定期进行日志的清理，避免大量的日志撑爆磁盘。 下面是一个修改日志级别，并采用 RollingFileAppender，进行日志备份的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384log4j.rootLogger=ERROR, defaultlog4j.appender.default=org.apache.log4j.RollingFileAppenderlog4j.appender.default.File=${kafka.logs.dir}/default.loglog4j.appender.default.MaxBackupIndex = 10log4j.appender.default.MaxFileSize = 100MBlog4j.appender.default.layout=org.apache.log4j.PatternLayoutlog4j.appender.default.layout.ConversionPattern=[%d] %p %m (%c)%nlog4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppenderlog4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.loglog4j.appender.kafkaAppender.MaxBackupIndex = 10log4j.appender.kafkaAppender.MaxFileSize = 100MBlog4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%nlog4j.appender.stateChangeAppender=org.apache.log4j.RollingFileAppenderlog4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.loglog4j.appender.stateChangeAppender.MaxBackupIndex = 10log4j.appender.stateChangeAppender.MaxFileSize = 100MBlog4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%nlog4j.appender.requestAppender=org.apache.log4j.RollingFileAppenderlog4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.loglog4j.appender.requestAppender.MaxBackupIndex = 10log4j.appender.requestAppender.MaxFileSize = 100MBlog4j.appender.requestAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%nlog4j.appender.cleanerAppender=org.apache.log4j.RollingFileAppenderlog4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.loglog4j.appender.cleanerAppender.MaxBackupIndex = 10log4j.appender.cleanerAppender.MaxFileSize = 100MBlog4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%nlog4j.appender.controllerAppender=org.apache.log4j.RollingFileAppenderlog4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.loglog4j.appender.controllerAppender.MaxBackupIndex = 10log4j.appender.controllerAppender.MaxFileSize = 100MBlog4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%nlog4j.appender.authorizerAppender=org.apache.log4j.RollingFileAppenderlog4j.appender.authorizerAppender.File=${kafka.logs.dir}/kafka-authorizer.loglog4j.appender.authorizerAppender.MaxBackupIndex = 10log4j.appender.authorizerAppender.MaxFileSize = 100MBlog4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n# Change the line below to adjust ZK client logginglog4j.logger.org.apache.zookeeper=WARN# Change the two lines below to adjust the general broker logging level (output to server.log and stdout)log4j.logger.kafka=ERRORlog4j.logger.org.apache.kafka=ERROR# Change to DEBUG or TRACE to enable request logginglog4j.logger.kafka.request.logger=ERROR, requestAppenderlog4j.additivity.kafka.request.logger=false# Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output# related to the handling of requests#log4j.logger.kafka.network.Processor=TRACE, requestAppender#log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender#log4j.additivity.kafka.server.KafkaApis=falselog4j.logger.kafka.network.RequestChannel$=WARN, requestAppenderlog4j.additivity.kafka.network.RequestChannel$=falselog4j.logger.kafka.controller=ERROR, controllerAppenderlog4j.additivity.kafka.controller=falselog4j.logger.kafka.log.LogCleaner=ERROR, cleanerAppenderlog4j.additivity.kafka.log.LogCleaner=falselog4j.logger.state.change.logger=ERROR, stateChangeAppenderlog4j.additivity.state.change.logger=false 注意： 上述配置中的文件大小、备份日志文件个数和日志级别需要根据环境和要求进行调整 Kafka的数据 Kafka 的数据有时也会称为日志或消息，请不要与运行日志混淆。 在 $KAFKA_HOME/config/server.properties 中配置了 log.dirs 值，表示 Kafka 数据的存放目录，而非 Kafka 的运行日志目录。 日志/消息清理（delete） Kafka 消息日志的清理逻辑是启动线程定期扫描日志文件，将符合清理规则的消息日志文件删除。 Kafka 默认的清理策略是基于文件修改时间戳的清理策略，默认会保留 7 天的消息日志量，基于消息日志总量大小的清理规则不生效。 在 $KAFKA_HOME/config/server.properties 中 log.retention.hours 配置了该值： 12# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=168 单位是小时，刚好 7 天。 因此，Kafka 数据的存放目录也一定要考虑磁盘空间是否能够满足保存7天的消息日志量，避免出现磁盘空间不够的情况。 Kafka 还可以基于消息日志大小进行清理，该策略会依次检查每个日志中的日志分段是否超出指定的大小（retentionSize），对超出指定大小的日志分段采取 删除策略。retentionSize 可通过参数 log.retention.bytes 来配置（在 $KAFKA_HOME/config/server.properties 中），单位为字节， 默认值为 -1，表示无穷大。该参数配置的是Log中所有日志文件的总大小，并非单个日志分段的大小。Kafka 的默认配置是基于上述的时间段清理，该参数是注释 状态。 123# A size-based retention policy for logs. Segments are pruned from the log unless the remaining# segments drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824 单个日志分段文件的大小限制可通过 log.segment.bytes 来限制，默认为1073741824，即1GB。 运行建议 建议在 Kafka 运行期间 对 Kafka 运行日志和消息进行监控，总结分析出每天的日志量的大小，合理规划磁盘空间和时间， 定期对 Kafka 的运行日志和消息进行备份和清理。","link":"/2020/07/22/Kafka%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/"},{"title":"Java之happens-before","text":"从一个例子开始 1234567891011121314public class MemModle { private int a = 0; private int b = 0; public void method1() { int m2 = a; b = 1; } public void method2() { int m1 = b; a = 2; }} 在单线程的情况下，如果先执行 method1 再执行 method2，最终 m1，m2 为 1， 0；如果先执行 method2 再执行 method1，最终 m1，m2 为 0， 2。 在多线程的情况下，假设这两个方法分别在不同的线程执行，如果 Java 虚拟机在执行了任一方法的第一条赋值语句后就切换线程，那么 m1，m2 的最终结果可能就是 0，0 了。除了上面三种情况外，m1，m2 的结果还有一种情况出现：1，2。 造成这种看似不可能的结果的原因有 3 个： 即时编译器的重排序 处理器的乱序执行 内存系统的重排序 后面两种原因涉及到具体的体系架构，这里不做探讨。 即时编译器的重排序 即时编译器(和处理器)需要保证程序能够遵守 as-if-serial 属性。as-if-serial 语义是：不管怎么重排序（编译器和处理器为了提高并行度）， （单线程）程序的执行结果不能被改变。编译器、runtime 和处理器都必须遵守 as-if-serial 语义。 通俗的讲，在单线程的情况下，要给程序一个顺序执行的假象，即经过重排序的执行结果要和顺序执行的结果一致。 此外，如果两个操作之间存在数据依赖，那么即时编译器(和处理器)不能调整它们的执行顺序，否则会造成程序语义的变化。 Java 内存模型与 happens-before Java 5 引入了明确定义的 Java 内存模型，其中最为重要的一个概念就是 happens-before 关系。happens-before 关系是用来描述两个操作的内存可见性的。 它定义了内存的可见性原则。如果一个操作 happens-before 另一个操作， 那么第一个操作的结果对第二个结果可见。 规则 下面是Java内存模型中的八条可保证 happen—before 的规则，它们无需任何同步器协助就已经存在，可以在编码中直接使用。 如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序性保障，虚拟机可以对它们进行随机地重排序。 单线程规则 在一个单独的线程中，执行结果与按照程序代码的执行顺序的结果是一致的。 锁定规则 一个 解锁 操作 happen—before 之后对同一个锁的 加锁 操作。 volatile 变量规则 对一个 volatile 变量的写操作 happen—before 之后对该变量的读操作。 线程启动规则 Thread 对象的 start() happen—before 该线程的第一个操作。 线程结束规则 线程的最后一个操作 happen—before 它的终止事件（即其它线程通过Thread.isAlive()或者Thread.join() 判断该线程是否终止）。 中断规则 线程对其它线程的中断操作 happen—before 被中断线程所收到的中断时事件（被中断线程的 InterruptedException 异常或者Thread.interrupted 调用）。 终结器规则 构造器中的最后一个操作 happen—before 析构器的第一个操作。 传递性规则 happens-before 关系具备传递性，如果操作 A happens-before 操作 B， 而操作 B happens-before 操作 C，那么 操作 A happens-before 操作 C。 在开头的例子中，程序没有定义任何 happens-before 关系，仅拥有默认线程内 happens-before 关系，也就是 m2 的赋值操作 happens-before b 的 赋值操作，m1的赋值操作 happens-before a 的赋值操作。拥有 happens-before 关系的两对赋值操作之间没有数据依赖，因此即时编译器（处理器）都可能 对其进行重排序。如下： 123456thread1 thread2 | | m2=a m1=b b=1 a=2 | | | | 重排后 123456thread1 thread2 | | b=1 | | m1=b | a=2 m2=a | 只要将 b 的赋值操作排在 m2 的赋值操作之前，那么就可以 1，2 的结果。 那么怎么解决这个问题呢？将 a 或者 b 设置为 volatile 字段。 比如将 b 设置为 volatile 字段。根据 volatile 字段的 happens-before 规则，我们知道 b 的写（赋值）操作 happens-before m1 的赋值操作。 在没有标记 volatile 的时候，同一线程中，m2=a和b=1存在happens before关系，但因为没有数据依赖可以重排列。一旦标记了volatile，即时编译器和CPU需要考虑到多线程happens-before关系，因此不能自由地重排序。 123456thread1 thread2 | | m2=a | b=1 | | m1=b | a=2 也就意味着，当对 a 进行赋值的时候，对 m2 的赋值操作已经完成。因此，在 b 为 volatile 字段的情况下，不会出现 m1，m2 出现 1， 2 的情况。 内存模型的实现 Java 内存模型是通过内存屏障（Memory Barrier）来禁止指令重排序的。 对于即时编译器来说，它会根据 happens-before 规则，向正在编译的目标方法中插入相应的读读、读写、写读和写写内存屏障。 这些内存屏障会限制即时编译器的重排序操作。以 volatile 字段访问为例，所插入的内存屏障将不允许 volatile 字段 写操作 之前的内存访问被重排序至其之后； 也将不允许 volatile 字段 读操作 之后的内存访问被重排序至其之前。 然后，即时编译器将根据具体的底层体系架构，将这些内存屏障替换成具体的 CPU 指令。以我们日常接触的 X86_64 架构来说，读读、读写以及写写内存屏障是 空操作（no-op），只有写读内存屏障会被替换成具体指令。 锁，final 字段 锁 锁操作同样具备 happens-before 关系。解锁操作 happens-before 之后对同一把锁的加锁操作。实际上，在解锁时，Java 虚拟机同样需要强制 刷新缓存，使得当前线程所修改的内存对其它线程可见。需要注意的是，锁操作的 happens-before 规则的关键字是同一把锁。也就意味着，如果编译器能够 证明某把锁仅被同一线程持有，那么它可以移除相应的加锁解锁操作。 因此也就不再强制刷新缓存。举个例子，即时编译后的 synchronized (new Object()) {}，可能等同于空操作，而不会强制刷新缓存。 final 字段 final 实例字段则涉及新建对象的发布问题。当一个对象包含 final 实例字段时，我们希望其他线程只能看到已初始化的 final 实例字段。 因此，即时编译器会在 final 字段的写操作后插入一个写写屏障（StoreStore屏障），以防某些优化将新建对象的发布（即将实例对象写入一个共享引用中）重排序至 final 字段的 写操作之前。在 X86_64 平台上，写写屏障（StoreStore屏障）是空操作。 具体来说对于final域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 123456789101112131415161718192021public class MemModle1 { int i; //普通变量 final int j; //final变量 static MemModle1 obj; public MemModle1() { i = 1; //写普通域 j = 2; //写final域 } public static void writer() { //写线程A执行 obj = new MemModle1(); } public static void reader() { //读线程B执行 MemModle1 object = obj; //读对象引用 int a = object.i; //读普通域 int b = object.j; //读final域 }} 引用 极客时间 -《Java内存模型》","link":"/2020/05/26/Java%E4%B9%8Bhappens-before/"},{"title":"Redis应用场景","text":"Redis应用场景 Redis介绍 Redis是一个使用ANSI C编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从2015年6月开始，Redis的开发由Redis Labs赞助，根据月度排行网站DB-Engines.com的数据显示，Redis是最流行的键值对存储数据库。 Redis的外围由一个键、值映射的字典构成。与其他非关系型数据库主要不同在于：Redis中值的类型不仅限于字符串，还支持如下抽象数据类型： 字符串列表 无序不重复的字符串集合 有序不重复的字符串集合 键、值都为字符串的哈希表 值的类型决定了值本身支持的操作。Redis支持不同无序、有序的列表，无序、有序的集合间的交集、并集等高级服务器端原子操作。 Redis应用 会话缓存 最常用的一种使用Redis的情景是会话缓存（session cache）。用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化、Redis提供的数据类型丰富。例如：缓存用户的登录信息，由于用户登录信息有时比较复杂，如果采用json格式存储，使用起来会简单很多。 列出最新的项目列表 下面这个语句常用来显示最新项目，随着数据多了，查询毫无疑问会越来越慢。 1SELECT * FROM foo WHERE … ORDER BY time DESC LIMIT 10; 应用中，“列出最新的回复”之类的查询非常普遍。如果数据库进行了分库分表的话，查询会变得复杂而且性能低下。类似的问题就可以用Redis来解决。比如说，我们的一个应用想要列出用户贴出的最新20条评论。我们假设数据库中的每条评论都有一个唯一的ID字段。我们可以使用分页来制作主页和评论页，使用Redis的模板，每次新评论发表时，我们会将它的ID添加到一个Redis列表： LPUSH latest.comments 我们将列表裁剪为指定长度，因此Redis只需要保存最新的5000条评论： LTRIM latest.comments 0 5000 LPUSH LPUSH key value [value …] 将一个或多个值 value 插入到列表 key 的表头 LTRIM LTRIM key start stop 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 当我们需要获取最新评论的项目范围时，调用一个函数来完成(使用伪代码)： 1234567public List&lt;Comment&gt; getLatestComments(start, num_items) { id_list = redis.lrange(&quot;latest.comments&quot;,start,start+num_items – 1); if (id_list.length &lt; num_items) { id_list = SQL_DB(&quot;SELECT … ORDER BY time LIMIT …&quot;); } return id_list;} 最新ID常驻在Redis中，并且一直是在更新的。但是我们做了限制不能超过5000个ID，因此我们的获取ID函数会一直询问Redis。只有在start/count参数超出了这个范围的时候，才需要去访问数据库。当然根据需求也可以不访问数据库。 排名 还有一个很普遍的需求是按得分排名，在按得分排序以及实时更新这些几乎每秒钟都需要更新的功能上数据库的性能不够理想。典型的比如那些在线游戏的排行榜，根据得分你通常想要： 列出前100名高分选手 列出某用户当前的全球排名 这些操作对于Redis来说很简单，即使你有几百万个用户，每分钟都会有几百万个新的得分。 模式是这样的，每次获得新得分时，我们用这样的代码： ZADD userscore 得到前100名高分用户很简单： ZREVRANGE userscore 0 99 用户的全球排名也相似，只需要： ZRANK userscore ZADD ZADD key score member [[score member] [score member] …] 将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 ZREVRANGE ZREVRANGE key start stop [WITHSCORES] 返回有序集 key 中，指定区间内的成员。其中成员的位置按 score 值递减(从大到小)来排列. ZRANK ZRANK key member 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。 延时任务 经常有类似的需求: 订单生成后10分钟,如果用户不付款就关闭订单 用户超时2天未收货，修改为自动收货 针对于类似这样的任务,一般我们是用定时任务来处理的。订单是存储在mysql的一个表里,表里会有各种状态和创建时间。利用quartz来设定一个定时任务,我们暂时设置为每5分钟扫描一次。扫描的条件为未付款并且当前时间大于创建时间超过15分钟.然后我们再去逐一的操作每一条数据。这个方案简单易用, 但扫表会增加程序负荷、任务执行不够准时。 还可以采用延时的方式来处理这样的问题，例如利用jdk自带的delayQueue。delayQueue的有点是：效率高,任务触发时间延迟低，不需要扫表，不会对数据库造成压力。但是delayQueue不支持分布式。 Redis有2种思路实现延时任务： ZREMRANGEBYSCORE ZREMRANGEBYSCORE key min max 移除有序集 key 中，所有 score 值介于 min 和 max 之间的成员。 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为被移除成员的数量。 有序集合 使用 sorted Sets 的自动排序, key 为任务id，score 为任务计划执行时的时间戳，任务在 ZADD 加入 sets 后就已经按时间排序，然后采用 delayQueue 的思路每隔1s(或者其他间隔)用 ZRANGEBYSCORE 取出小于当前时间的的任务id 然后再去执行任务。 由于有序集合中只有任务id，所以还需要一个哈希表来存储任务，有序集合和哈希表通过任务id关联起来。由于先从集合中取出到期的任务id，再从哈希表中取出任务，这是两次操作，可能出现不满足事务的情况发生。处理方法是采用 Redis 事务机制来实现事务，或者利用 Redis 执行 Lua 脚本是原子性的来采用Lua脚本封装这两个操作。 键过期通知 Reids 2.8 后有一种“键空间”通知的机制 Keyspace Notifications，允许客户端去订阅一些key的事件，其中就有 key过期的事件，我们可以把 key 名称设置为 task 的 id 等标识(这种方式 value 的值无法取到，所以只用 key 来识别任务)，expire 设置为计划要执行的时间，然后设置一个客户端来订阅消息过期事件，然后处理 task。因为开启键空间通知功能需要消耗一些 CPU ，所以在默认配置下，该功能处于关闭状态。可以通过修改 redis.conf 文件，或者直接使用 CONFIGSET 命令来开启或关闭键空间通知功能。配置文件修改方式如下： 1notify-keyspace-events Ex // 打开此配置，其中Ex表示键事件通知里面的key过期事件，每当有过期键被删除时，会发送通知 notify-keyspace-events 选项的参数为空字符串时，功能关闭。 当参数不是空字符串时，功能开启。 Redis 使用以下两种方式删除过期的键： 当一个键被访问时，如果键已经过期，那么该键将被删除。 底层系统会在后台渐进地查找并删除那些过期的键，从而处理那些已经过期、但是还没被访问到的键。当过期键被程序发现、并且将键从数据库中删除时，Redis 会产生一个 expired 通知。Redis 并不保证生存时间（TTL）变为 0 的键会立即被删除：如果程序没有访问这个过期键，或者带有生存时间的键非常多的话，那么在键的生存时间变为0 ，直到键真正被删除这中间，可能会有一段比较显著的时间间隔。 那么通知产生的时间会有一段间隔，如果不能接受这个间隔，可采用有序集合的方式来实现延时任务。 计数 Redis 是一个很好的计数器，它有 INCRBY 等命令。虽然可以用数据库做计数器，来获取统计或显示新信息，但数据库太慢了。使用 Redis 就不需要再担心了。有了原子递增(atomic increment)，你可以放心的加上各种计数，用GETSET重置，或者是让它们过期。例如这样操作： 1INCR user 60 计算出最近用户在页面间停顿超过或不超过60秒的页面浏览量。 INCRBY INCRBY key increment 将 key 所储存的值加上增量 increment。如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCRBY 命令。如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。本操作的值限制在 64 位(bit)有符号数字表示之内。 指定时间内的特定项目 比如想要知道某些特定的注册用户或IP地址，他们到底有多少访问了某篇文章。在获得一次新的页面浏览时只需要这样做： 1SADD page:day0: 当然用unix时间替换day0，比如time()-(time()%3600*24)等等。想知道特定用户的数量吗?只需要使用 1SCARD page:day0: 计算某个特定用户是否访问了这个页面： 1SISMEMBER page:day0: SADD SADD key member [member …] 将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略。 SCARD SCARD key 返回集合 key 的基数(集合中元素的数量)。 SISMEMBER SISMEMBER key member 判断 member 元素是否集合 key 的成员。 分布式锁 利用 SETNX 可实现分布式锁。 SETNX SETNX key value 将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。 时间复杂度：O(1) 返回值：设置成功，返回 1 。设置失败，返回 0 。 设置成功会返回1，可表示拿到了锁，设置返回 0 表示没拿到锁，继续等待。释放锁可调用 DEL 删除 key。考虑到锁未释放而程序宕机，该锁将不会被释放的情况，可以给锁设置一个过期时间，过期后该锁会被 Redis 删除。","link":"/2018/03/08/Redis%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"},{"title":"String常量池和String.intern","text":"String常量池和String.intern 前言 字符串在实际中使用非常频繁，如果为每个字符串常量都生成一个String对象，明显会造成内存的浪费，针对这一问题，JVM实现一个字符串常量池的概念，提供了如下实现： 相同的字符串常量，在常量池只有一份副本； 通过双引号声明的字符串，直接保存在常量池中； 如果是String对象，可以通过String.intern方法，把字符串常量保存到常量池中； 提示： Jave不仅有字符串常量池，Java中基本类型的包装类的大部分都实现了常量池技术，这些类是Byte,Short,Integer,Long,Character,Boolean,另外两种浮点数类型（Double 、Float）的包装类则没有实现。另外Byte,Short,Integer,Long,Character这5种整型的包装类也只是在对应值小于等于127时才可使用常量池，即JVM不负责创建和管理大于127的这些类的对象。 intern方法 举个例子： 1234567public static void testIntern() { String s1 = new StringBuilder(&quot;Intern&quot;).append(&quot;StringTest&quot;).toString(); System.out.println(s1.intern() == s1); String s2 = new StringBuilder(&quot;CP&quot;).append(&quot;936&quot;).toString(); System.out.println(s2.intern() == s2);} jdk1.7的结果: true false 为什么不是: true true 原因 intern的实现 123456789101112131415161718192021222324/** * Returns a canonical representation for the string object. * &lt;p&gt; * A pool of strings, initially empty, is maintained privately by the * class {@code String}. * &lt;p&gt; * When the intern method is invoked, if the pool already contains a * string equal to this {@code String} object as determined by * the {@link #equals(Object)} method, then the string from the pool is * returned. Otherwise, this {@code String} object is added to the * pool and a reference to this {@code String} object is returned. * &lt;p&gt; * It follows that for any two strings {@code s} and {@code t}, * {@code s.intern() == t.intern()} is {@code true} * if and only if {@code s.equals(t)} is {@code true}. * &lt;p&gt; * All literal strings and string-valued constant expressions are * interned. String literals are defined in section 3.10.5 of the * &lt;cite&gt;The Java&amp;trade; Language Specification&lt;/cite&gt;. * * @return a string that has the same contents as this string, but is * guaranteed to be from a pool of unique strings. */public native String intern(); 从上面的代码（反编译jdk8）看出String.intern在java中是native方法，注释翻译如下： 执行intern方法时，如果常量池中存在和String对象相同的字符串，则返回常量池中对应字符串的引用； 如果常量池中不存在对应的字符串，则添加该字符串到常量池中，并返回字符串引用。 字符串常量池 如果是jdk6，上面的代码执行结果会是：false false。jdk7之前常量池的内存在永久代进行分配，永久代和Java堆的内存是物理隔离的，执行intern方法时，如果常量池不存在该字符串，虚拟机会在常量池中复制该字符，再返回并返回引用。jdk6的常量池没有上面例子的两个字符串，所以返回的都是false。 为什么jdk7返回ture和false呢？原因是jdk7把字符串常量池移到了堆中。 第一个字符串在常量池中不存在，就要字符串添加到常量池中，由于常量池在堆上，添加的时候只是加上该字符串的引用，没有拷贝的过程，然后再返回该字符串的引用，这样intern返回的引用和s1是同一个引用，所以打印出true； 第二个打印出false，只有一个原因，常量池中有那个字符串。调试时发现确实在字符串常量池中发现这该字符串，如下图： 字符串常量池大小 1、jdk6是1009 2、jdk7默认是60013，可以通过-XX:StringTableSize = 10009设置StringTable大小， 通过-XX:+PrintFlagsFinal打印虚拟机的Global flags参数，可以获得当前StringTable的大小。 3、jdk8移除了PermGen，常量池移动了堆上，大小受程序内存的限制。","link":"/2018/02/09/String%E5%B8%B8%E9%87%8F%E6%B1%A0%E5%92%8CString-intern/"},{"title":"Oracle转MySql","text":"最近产品要支持MySQl数据库，从Oracle转MySql，记录过程，用以备忘。 手动转换 手动方式，就是操作步骤会比较繁琐一些。 对 Table 的结构和数据： 使用 SQL Developer 把 oracle 的 table 的schema 和 Data（.sql 和 .xls） 导出 使用 MySQL 的 WorkBench 创建 Table 和导入数据。 由于语法和数据类型会稍微有一些不同，需要做一些调整。 对于 View 来说， 特别是复杂的有子查询的 Oracle View 说，要导入到 MySQL 不是那么容易了，基本上都需要重写。 使用工具 Navicat 试了好几个工具，只有 Navicat 最方便，不仅能导入表结构，还能导入注释。视图和存储过程要手动导入。 创建 MySql 数据库。 启动 Navicat，选择工具菜单下的数据传输子菜单。 在数据传输对话窗体的常规标签页选择源和目标。这里源选择 Oracle 数据库，目标选择 MySQL 数据库。在选项标签页，找到遇到错误继续，根据情况设置是否勾选。 点击下一步，在接下来的数据库对象页签选择表（Oracle转MySQl，只能选择表）。 点击开始等待执行结果。 手动导入存储过程和视图。由于语法不一样，sql语句需要改写。 MySQL5转MySQL8 MySQL5转MySQL8相对比较简单，手动转或者用Navicat转都不复杂，Navicat 还可以导入存储过程和视图。只是 mysql8.0.1 之后的默认 COLLATE 为utf8mb4_0900_ai_ci，如果在转换的时候没注意的话，可能出现 java.sql.SQLException: Illegal mix of collations (utf8mb4_general_ci,IMPLICIT) and (utf8mb4_0900_ai_ci,IMPLICIT) for *** 样的错误。解决方案如下： MySQL8建库时，编码选 utf8mb4，COLLATE选 utf8mb4_0900_ai_ci。utf8mb4_unicode_ci和utf8mb4_general_ci对于中文和英文来说，其实没什么太大区别。对于我们开发的国内使用的系统来说，随便选哪个都行。utf8mb4_0900_ai_ci大体上就是unicode的进一步细分，0900指代unicode比较算法的编号（ Unicode Collation Algorithm version），ai表示accent insensitive（发音无关），例如e, è, é, ê 和 ë是一视同仁的。所以选 utf8mb4_0900_ai_ci 没什么问题。 采用Navicat导入时，在“数据传输”的选项里，注意不要勾选包含字符集。 linux 下 MySQL 重启 由于是从源码包安装的Mysql，所以系统中是没有红帽常用的servcie mysqld restart这个脚本，如果没有自建脚本，只好手工重启。采用 Killall mysql，可能会损害数据库。安全重启的方法如下： 12$mysql_dir/bin/mysqladmin -u root -p shutdown$mysql_dir/bin/./mysqld_safe &amp; mysqladmin和mysqld_safe位于Mysql安装目录的bin目录下。 mysql5.7在 windows 平台也有 mysqladmin 指令，测试可以该指令来停止 mysql 服务。","link":"/2019/09/10/Oracle%E8%BD%ACMySql/"},{"title":"RestTemplate重试","text":"springframework 提供了 RestTemplate 作为 web 客户端，它提供了多种便捷访问远程 Http 服务的方法,能够大大提高客户端的编写效率。 RestTemplate 默认依赖 JDK 的 HTTP 连接工具，也可以通过 setRequestFactory 属性切换到不同的 HTTP 源，比如 Apache HttpComponents、Netty 和OkHttp。 使用 RestTemplate 调用服务的时候，因为网络波动造成的对方服务异常或者对方服务降级后又好了，我们可能会写个循环对 RestTemplate 的调用进行重试，但这样实现起来麻烦，代码也可能变得冗长。其实 RestTemplate 为我们提供了重试机制。 HttpRequestRetryHandler Apache 的 httpclient 的 HttpRequestRetryHandler 接口为我们提供了一种重试机制。大致代码如下： 123HttpClientBuilder clientBuilder = HttpClients.custom();HttpRequestRetryHandler retryHandler = new DefaultHttpRequestRetryHandler(retryTimes, false); clientBuilder.setDefaultRequestConfig(config).setRetryHandler(retryHandler) 这里采用的是 DefaultHttpRequestRetryHandler 实现了 HttpRequestRetryHandler 接口。 ServiceUnavailableRetryStrategy Apache 的 httpclient 的 ServiceUnavailableRetryStrategy 接口为我们提供了另外一种重试机制。大致代码如下： 123456789101112131415ServiceUnavailableRetryStrategy retryStrategy = new ServiceUnavailableRetryStrategy() { @Override public boolean retryRequest(HttpResponse response, int executionCount, HttpContext httpContext) { int statusCode = response.getStatusLine().getStatusCode(); return executionCount &lt; retryTimes &amp;&amp; statusCode == HttpStatus.SC_SERVICE_UNAVAILABLE; } @Override public long getRetryInterval() { return retryInterval; }};clientBuilder.setDefaultRequestConfig(config).setRetryHandler(retryHandler) .setServiceUnavailableRetryStrategy(retryStrategy); 从上面的代码可以看出，HttpRequestRetryHandler 和 ServiceUnavailableRetryStrategy 这两种机制可以合起来使用。 区别 这两种重试机制的区别是什么呢？从名字来看，HttpRequestRetryHandler 是请求时重试，ServiceUnavailableRetryStrategy 是针对服务端的响应状态的重试。实现上也是如此： DefaultHttpRequestRetryHandler 主要实现的是请求的异常进行重试： InterruptedIOException UnknownHostException ConnectException SSLException ServiceUnavailableRetryStrategy 的实现是请求成功了，但是 http 状态码可能不是 2xx，这种情况下的重试机制。如上例就是服务器返回 SC_SERVICE_UNAVAILABLE （503） 重试。 通过上面的了解，我们采用 RestTemplate 作为 http 的客户端，在实现重试时，就不用在每次调用的地方写循环来进行重试了。封装一个 创建 RestTemplate 的工具类，组合上面两个接口，就可以方便灵活的实现重试了。","link":"/2021/11/23/RestTemplate%E9%87%8D%E8%AF%95/"},{"title":"docker镜像shell脚本备份","text":"docker镜像shell脚本备份 get docker tag shell脚本 12345678910111213POM_POSITION=mt-parent/pom.xmlversionNum=$(awk -v RS=&quot;&lt;/*docker.image.tag&gt;&quot; 'NR==2{print}' $POM_POSITION)if [ ${#versionNum} -gt 15 ];then exit 0elsecurrentTime=$(date +%Y%m%d%H%M%S000)commiteId=$(git rev-parse --short HEAD)wholeTag=${versionNum}_${currentTime}_${commiteId}echo $wholeTagsed -i &quot;s/&lt;docker.image.tag&gt;$versionNum/&lt;docker.image.tag&gt;$wholeTag/g&quot; $POM_POSITIONversionNum2=$(awk -v RS=&quot;&lt;/*docker.image.tag&gt;&quot; 'NR==2{print}' $POM_POSITION)echo $versionNum2fi 构建镜像脚本 123POM_POSITION=mt-parent/pom.xmlwholeTag=$(awk -v RS=&quot;&lt;/*docker.image.tag&gt;&quot; 'NR==2{print}' $POM_POSITION)docker rmi 54.223.110.70/java/mt-user:$wholeTag","link":"/2018/08/20/docker%E9%95%9C%E5%83%8Fshell%E8%84%9A%E6%9C%AC%E5%A4%87%E4%BB%BD/"},{"title":"aop实现日志入库@AfterThrowing调用正常，数据未提交","text":"最近开发使用AOP方式实现系统审计日志，主要功能为方法开始、结束、异常时记录日志并存入数据库。测试时发现系统抛出异常时，@AfterThrowing 异常处理切入点可正常调用，但该切入点记录的数据未能正常提交数据库。查找原因，发现有两种： Spring事务是通过AOP实现的，记录日志的AOP与事务AOP存在执行顺序问题，如果事务AOP先执行了，@AfterThrowing 中数据则无法提交。 @AfterThrowing 处理方法和抛出异常的方法在同一个事务中，@AfterThrowing 处理完成后，抛出异常的方法因为有异常，导致事务回滚，所以记录数据未能提交数据库。 解决方案 解决办法为 @Aspect 切面类实现org.springframework.core.orderd接口或使用@Order注解,推荐使用注解方式。然后将 Order 的值设置得很小。值越小，优先级越高。 第2种情况得解决，在AOP的逻辑中启用一个新的事务。 测试发现，通过在 @AfterThrowing 方法上加上注解 @Transactional，并指定传播行为是REQUIRES_NEW依然不行。因为 @Transactional 也是声明式事务，本身就是AOP实现的，在AOP的代码中不起作用。就只能使用 spring 的编程式事务了，需要引入 TransactionTemplate。实现如下： 123456789101112131415@Autowiredprivate TransactionTemplate transactionTemplate;@AfterThrowing(pointcut = &quot;pointcut()&quot;, throwing = &quot;e&quot;)public void doAfterThrowing(JoinPoint joinPoint, Throwable e) throws Throwable { //声明式事务在切面中不起作用，需使用编程式事务 //设置传播行为：总是新启一个事务，如果存在原事务，就挂起原事务 transactionTemplate.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); transactionTemplate.execute(TransactionCallback&lt;T&gt;() { @Override public T doInTransaction(TransactionStatus status) { // 数据库操作 } });} 值得注意的是，transactionTemplate 不一定可以注入，程序启动时可能会报：“No qualifying bean of type TransactionTemplate”这样的错误，那么就用 PlatformTransactionManager 构造一个transactionTemplate。 12345@Autowiredprivate PlatformTransactionManager transactionManager;...transactionTemplate = new TransactionTemplate(transactionManager);","link":"/2023/04/13/aop%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E5%85%A5%E5%BA%93-AfterThrowing%E8%B0%83%E7%94%A8%E6%AD%A3%E5%B8%B8%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%9C%AA%E6%8F%90%E4%BA%A4/"},{"title":"git:Authentication failed for 又不弹出用户名和密码windows解决方法","text":"fatal: Authentication failed for又不弹出用户名和密码 最近用tortoisegit更新或者提交代码，提示fatal: Authentication failed for，但是又不能够弹出输入账号密码框。 解决方法 搞了半天在命令行下： 1git config --system --unset credential.helper 然后就终于可以重新填写用户名和密码进行提交了。 但是这样一来，用户名和密码不能保存，每次更新和提交都提示输入密码，很麻烦。继续研究… 安装Git-Credential-Manager-for-Windows 执行命令： 1git config --global credential.helper manager 这下好了，在Users下生成了.gitconfig 文件，里面的内容如下： 12[credential] helper = manager windows下另外一种可能方案 在控制面板的用户帐户\\凭据管理器里面有个windows 凭证，删除出错的账户凭证，可能就可以解决问题了。","link":"/2018/09/06/git-Authentication-failed-for-%E5%8F%88%E4%B8%8D%E5%BC%B9%E5%87%BA%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E5%AF%86%E7%A0%81windows%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"},{"title":"govendor使用","text":"govendors使用 govendor 是golang依赖包管理工具之一。 环境变量 GOROOT: go的安装路径,官方包路径根据这个设置自动匹配 GOPATH: 工作路径，主要包含三个目录: bin、pkg、src go中是没有项目这个概念的，只有包。可执行包只是特殊的一种，类似我们常说的项目 GOPATH 可以设置多个，不管是可执行包，还是非可执行包，通通都应该在某个$GOPATH/src 下，比如可以把你的可执行(项目)包，安放在某个 $GOPATH/src 下，例如 $GOPATH/src/app/example.com，这样本地包的import就变成: 1import &quot;app/example.com/subpackage&quot; 这样使用 GOPATH 的好处： 可以使用 go install 你的子包，有利于 go build 的时间，如果子包较大，那就更明显了。 gocode 的自动完成可以用了。 gocode windows安装 go get -u -ldflags -H=windowsgui github.com/nsf/gocode 安装 安装指令 go get -u github.com/kardianos/govendor 设置 包含vendor文件夹的代码路径应该在GoPath路径下： 1234567$GOPATH/src/ example.com/ main.go vendor/ github.com/ spf13/viper/ gizak/termui/ 如上面的例子，example.com为工程文件夹，main.go 和 vendor 在此目录下，为同一级。 配置文件 vendor.json 配置文件 vendor.json 中的 rootPath 是指 $GOPATH/src 下的路径，例如： 1&quot;rootPath&quot;: &quot;services/example.com&quot; 路径是： 123$GOPATH/src/ services/example.com 使用 参考 govendor 文档。","link":"/2018/04/02/govendor%E4%BD%BF%E7%94%A8/"},{"title":"Qt5.9.8 &amp; vs2019 静态编译","text":"Qt5.9.8 &amp; vs2019 静态编译 简单介绍Qt 5.9.8使用VS2019静态编译的过程以及可能遇到的问题的解决方法。首先是依赖，与运行不同，Qt在编译的时候会有一些其它依赖。请安装好它们并且添加至环境变量。 Perl version 5.12 or later，下载 选用的是strawberry perl，有Portable edition版可选。 python3 除了上述依赖之外，Qt还有一些可选的外部库依赖，一些库在Qt源码中已经提供，我们只需要在配置的时候指定一下就好，一些并没有提供，需要自己下载别人编译好的或者需要自己编译。主要有两项，分别是ICU和OpenSSL。 ICU 关于ICU在Qt中用于国际化的处理，如不想编译ICU也可跳过这一章节。编译ICU需要cygwin或者MSYS，采用MSYS没有编译成功，本文记录采用cygwin编译过程。 源码下载 icu项目地址 安装cygwin，至少安装以下几个工具: make dos2unix binutils 编译工程 打开命令行，设置环境变量，将 cygwin 的 bin 目录路经加入环境变量，执行命令 set PATH=%PATH%;D:\\cygwin\\bin 配置VC编译环境，执行命令 call &quot;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat&quot; 进入ICU根目录的 source 文件夹，转换文件，执行命令 123dos2unix *dos2unix -f configure 如果需要编成 MT 的，打开 runConfigureICU 文件，将 Cygwin/MSVC 配置中的 MD 改成 MT 配置编译选项，执行命令 bash runConfigureICU Cygwin/MSVC -prefix=/cygdrive/z/icu --enable-static --disable-shared -prefix: 设置安装目录，注意，以 /cygdrive 开头 –enable-debug --disable-release：编Debug版本，什么都不加，默认为release –enable-static --disable-shared：静态编译 lib -–disable-static -–enable-shared：动态编译 dll 静态编译，执行命令 make ，注意用 cygwin 的 make 编译完成后，安装icu，执行命令 make install 命令执行完毕后，icu的库文件就会复制到之前 -prefix 参数指定的目录中 清理临时文件，make.exe clean 编译debug版，添加 --enable-debug --disable-release 选项无效，可修改runConfigureICU 中 12debug=0release=1 为 12debug=1release=0 单独编译出debug版。 OpenSSL Qt5.9.8 要求OpenSSL的版本是 &gt;= 1.0.0 并且 &lt; 1.1.0。编译OpenSSL需要 Perl。编译OpenSSL可添加添加zlib支持。 OpenSSL和zlib下载 openssl官网 zlib下载 zlib编译 vs2019开发人员命令工具，选择 x64 Native Tools Command Prompt for VS 2019 zlib源码根目录下进入到 contrib\\masmx86 下 执行如下命令 bld_ml64.bat 启动 vs2019，打开 \\contrib\\vstudio\\vc14\\zlibvc.sln，升级工程 选择64位编译模式进行编译 添加zlib支持 打开openssl\\crypto\\comp\\c_zlib.c，在# include &lt;zlib.h&gt;上方添加#define ZLIB_WINAPI 编译 打开命令行，设置perl环境 call portableshell.bat /SETENV 设置vs2019编译环境 call &quot;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat&quot; 设置OpenSSL编译选项 call perl Configure VC-WIN64A no-asm no-shared --prefix=%openssl% zlib --with-zlib-include=d:\\ops\\zlib-1.2.11 --with-zlib-lib=d:\\ops\\zlib-1.2.11\\x64\\zlibstat.lib 输入下面两个命令之一配置编译选项，debug-VC-WIN32表示32位调试模式，VC-WIN64A表示64位模式，no-asm表示不用汇编，no-shared表示编译静态库，–prefix=表示最后生成的目录，zlib表示静态依赖zlib，–with-zlib-include=表示zlib的头文件目录，–with-zlib-lib=表示zlib的静态库文件路径 cd openssl源码路径开始编译 编译安装1.1.1c版 12nmake -f Makefilenmake -f Makefile install 编译安装1.0.2s版 call ms\\do_win64a nmake -f ms\\nt.mak nmake -f ms\\nt.mak install 编译Qt Qt 5.9.8源码下载，下载 qt-everywhere-opensource-src-5.9.8.zip。 打开命令行，设置perl环境 call portableshell.bat /SETENV 设置python环境，set PATH=%PATH%;d:\\ops\\python3 设置ICU环境变量 1234set icu=d:\\ops\\icuset PATH=%PATH%;%icu%\\binset ICU_INCLUDE=%icu%\\includeset ICU_LIB=%icu%\\lib 设置OpenSSL环境 1234set OPENSSL=d:\\ops\\openssl-1.0.2sset PATH=%PATH%;%OPENSSL%\\binset OPENSSL_INCLUDE=%OPENSSL%\\includeset OPENSSL_LIB=%OPENSSL%\\lib 进入Qt源码，创建 build 路径，并进入 12cd 5.9.8mkdir qt-build &amp;&amp; cd qt-build 配置静态编译选项 修改 qtbase\\mkspecs\\common\\msvc-desktop.conf，再找到下面几行 123QMAKE_CFLAGS_RELEASE = $$QMAKE_CFLAGS_OPTIMIZE -MDQMAKE_CFLAGS_RELEASE_WITH_DEBUGINFO += $$QMAKE_CFLAGS_OPTIMIZE -MD -ZiQMAKE_CFLAGS_DEBUG = -Zi -MDd 修改成 123QMAKE_CFLAGS_RELEASE = $$QMAKE_CFLAGS_OPTIMIZE -MTQMAKE_CFLAGS_RELEASE_WITH_DEBUGINFO += $$QMAKE_CFLAGS_OPTIMIZE -MT -ZiQMAKE_CFLAGS_DEBUG = -Zi -MTd 就是把MD的编译选项改成MT。 如果是vs2017及以上版本，修改 qtbase\\mkspecs\\common\\msvc-version.conf，找下面几行 123# https://developercommunity.visualstudio.com/content/problem/139261/msvc-incorrectly-defines-cplusplus.html# QMAKE_CXXFLAGS_CXX14 = -std:c++14# QMAKE_CXXFLAGS_CXX1Z = -std:c++latest 修改成 123# https://developercommunity.visualstudio.com/content/problem/139261/msvc-incorrectly-defines-cplusplus.htmlQMAKE_CXXFLAGS_CXX14 = -std:c++14QMAKE_CXXFLAGS_CXX1Z = -std:c++17 另一处 1234greaterThan(QMAKE_MSC_VER, 1910) { # No compat spec past MSVC 2017 COMPAT_MKSPEC =} 修改成 12345greaterThan(QMAKE_MSC_VER, 1910) { # No compat spec past MSVC 2017 DEFINES += _ENABLE_EXTENDED_ALIGNED_STORAGE COMPAT_MKSPEC =} 设置qt配置选项 1call ..\\configure.bat -mp -release -static -static-runtime -confirm-license -platform win32-msvc -opensource -prefix %QT_INSTALL% -icu -I %ICU_INCLUDE% -L %ICU_LIB% ICU_LIBS=&quot;sicudt.lib sicuuc.lib sicuin.lib&quot; -openssl -I %OPENSSL_INCLUDE% -L %OPENSSL_LIB% OPENSSL_LIBS=&quot;libeay32.lib ssleay32.lib&quot; -opengl desktop -qt-zlib -nomake examples -nomake tests -skip qt3d -skip qtdatavis3d -skip qtgamepad -skip qtserialport -skip qtspeech -skip qtwayland -skip qtwebengine -skip qtwebview -skip qtremoteobjects 具体配置详情如下： 1234567891011121314151617-confirm-license -opensource-release 版本-static 静态-platform win32-msvc 版本-static-runtime-mp 多线程编译-silent 不显示编译时多余的大量信息-opengl desktop 选择desktop而不是dynamic，避免qcustomplot无法使用opengl-qt-sqlite 三个选项[system/qt/no]选择源码自带-qt-pcre 三个选项[system/qt/no]选择源码自带-qt-zlib 三个选项[system/qt/no]选择源码自带-qt-freetype 三个选项[system/qt/no]选择源码自带-qt-harfbuzz 三个选项[system/qt/no]选择源码自带-qt-libpng 三个选项[system/qt/no]选择源码自带-qt-libjpeg 三个选项[system/qt/no]选择源码自带-nomake examples 不编译例子-nomake tests 不编译测试 移除了 webengine、qtwebview等组件，qtremoteobjects配置出错，也移除了。 编译并安装 12nmake &gt; z:\\temp\\nmake.out.txt 2&gt;&amp;1nmake install &gt; z:\\temp\\nmake_install.out.txt 2&gt;&amp;1 安装路径为上一步配置的 -prefix 路径。 参考 Compiling-ICU-with-MSVC","link":"/2019/08/24/Qt5-9-8-vs2019-%E9%9D%99%E6%80%81%E7%BC%96%E8%AF%91/"},{"title":"jedis报错:No reachable node in cluster","text":"jedis报错:No reachable node in cluster 服务器掉电重启后，java项目报错了： 1234567891011121314org.springframework.data.redis.RedisConnectionFailureException: No reachable node in cluster; nested exception is redis.clients.jedis.exceptions.JedisNoReachableClusterNodeException: No reachable node in cluster at org.springframework.data.redis.connection.jedis.JedisExceptionConverter.convert(JedisExceptionConverter.java:67) at org.springframework.data.redis.connection.jedis.JedisExceptionConverter.convert(JedisExceptionConverter.java:41) at org.springframework.data.redis.PassThroughExceptionTranslationStrategy.translate(PassThroughExceptionTranslationStrategy.java:37) at org.springframework.data.redis.connection.jedis.JedisClusterConnection.convertJedisAccessException(JedisClusterConnection.java:3696) at org.springframework.data.redis.connection.jedis.JedisClusterConnection.get(JedisClusterConnection.java:546) at org.springframework.data.redis.connection.DefaultStringRedisConnection.get(DefaultStringRedisConnection.java:284) at org.springframework.data.redis.core.DefaultValueOperations$1.inRedis(DefaultValueOperations.java:46) at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:54) at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:204) at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:166) at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:88) at org.springframework.data.redis.core.DefaultValueOperations.get(DefaultValueOperations.java:43) at com.jhqc.pxsj.msa.pub.redis.RedisForStringServiceImpl.get(RedisForStringServiceImpl.java:29) 应是redis cluster出问题了，对解决方法做个记录备忘。 重启redis集群 检测redis 节点 1./redis-trib.rb check 192.168.31.233:7000 但是出现如下错误信息： 1234567&gt;&gt;&gt; Check for open slots...[WARNING] Node 192.168.31.233:7000 has slots in importing state (19816).[WARNING] Node 192.168.31.233:7001 has slots in migrating state (19816).[WARNING] The following slots are open: 19816 尝试用：redis-trib.rb fix 192.168.31.233:7000 提示：[ERR] Calling MIGRATE: ERR Syntax error, try CLIENT (LIST | KILL | GETNAME | SETNAME | PAUSE | REPLY) 清理 slot 使用命令：cluster setslot 19816 stable 清理id为19816有问题的slot。 note：服务器cluster命令没有找到，通过RedisDesktopManager连接上每个节点，执行上面的命令。 重启redis cluster 重启java服务 没重启java服务，客户端同样报错，重启后正常。","link":"/2018/08/23/jedis%E6%8A%A5%E9%94%99-No-reachable-node-in-cluster/"},{"title":"Qt5.12.5 &amp; vs2019 静态编译","text":"Qt5.12.5 &amp; vs2019 静态编译 vs2019 编译 Qt5.12.5 的过程和之前记录的《Qt5.9.8 &amp; vs2019 静态编译》大致相同。这里记录一下不同的地方。 需要LLVM 编译 Qt5.12.5 需要 llvm, 下载llvm安装，注意在执行配置Qt之前，在设置环境变量 LLVM_INSTALL_DIR。 1set LLVM_INSTALL_DIR=d:\\ops\\LLVM-8.0.1-win64 要求openssl版本大于等于1.1 编译安装 openssl-1.1.1c 即可。","link":"/2019/09/12/Qt5-12-5-vs2019-%E9%9D%99%E6%80%81%E7%BC%96%E8%AF%91/"},{"title":"hexo安装和配置","text":"hexo安装和配置 hexo简介 Hexo是一个快速、简洁且高效的博客框架，支持Markdown格式，有众多优秀插件和主题。 官网： http://hexo.io github: https://github.com/hexojs/hexo 安装 先安装node.js，安装完成后再执行 1npm install -g hexo 初始化 新建一个文件夹（名字如：hexo，可以随便取），由于这个文件夹将来就作为你存放代码的地方，所以最好不要随便放。进入那个目录 1hexo init hexo会自动下载一些文件到这个目录，包括node_modules。 1hexo g 在public目录生成相关html文件。 1hexo s 开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容。 修改主题 默认主题不好看，改用next主题，hexo-theme-next。 首先安装主题 12cd hexogit clone https://github.com/theme-next/hexo-theme-next themes/next 主题都安装到themes目录中。 然后修改hexo目录中_config.yml文件，将其中的theme: landscape改为theme: next，然后重新执行hexo g来重新生成。 传到github 如果github pages服务都配置好了，发布上传很容易，一句hexo d就搞定。 其次，配置_config.yml中有关deploy的部分： 1234deploy: type: git repo: https://github.com/gspark/gspark.github.io.git branch: master 直接执行hexo d的话一般会报如下错误： 1Deployer not found: github 或者 1Deployer not found: git 原因是还需要安装一个插件： 1npm install hexo-deployer-git --save 原始md文件管理 hexo生成的静态页面上传github后，原始的md文件也需要版本管理： 对自己的xxx.github.io仓库打一个分支，如hexo，再把hexo目录下对应的代码文件和配置文件上传到这个分支即可。以后再分支上进行md文件的增、删、改、查和对配置文件的修改。 配置 资源文件配置 通过将 config.yml 文件中的 post_asset_folder 选项设为 true 来打开。 12_config.yml post_asset_folder: true 当资源文件管理功能打开后，Hexo将会在你每一次通过 bat hexo new [layout] &lt;title&gt; 命令创建新文章时自动创建一个文件夹。这个资源文件夹将会有与这个 markdown 文件一样的名字。将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过相对路径来引用它们，这样你就得到了一个更简单而且方便得多的工作流。 图片路径 hexo博客图片的问题在于，markdown文章使用的图片路径和hexo博客发布时的图片路径不一致。 可使用CodeFalling/hexo-asset-image插件来解决。 在hexo的目录下执行 1npm install https://github.com/CodeFalling/hexo-asset-image --save 只要使用 1![](本地图片测试/logo.jpg) 就可以插入图片。其中[]里面不写文字则没有图片标题。","link":"/2018/02/24/hexo%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"},{"title":"hystrix在spring中使用和配置","text":"hystrix在spring中使用和配置 - 微服务 这篇文章是整理的一些Hystrix的经验。 Hystrix启用 在应用主类中使用 @EnableCircuitBreaker 或 @EnableHystrix 注解开启Hystrix的使用。 Hystrix依赖隔离 Hystrix使用命令模式HystrixCommand(Command)包装依赖调用逻辑，每个命令在单独线程中/信号授权下执行。 可配置依赖调用超时时间,超时时间一般设为比99.5%平均时间略高即可.当调用超时时，直接返回或执行fallback逻辑。 为每个依赖提供一个小的线程池（或信号），如果线程池已满调用将被立即拒绝，默认不采用排队.加速失败判定时间。 依赖调用结果分:成功，失败（抛出异常），超时，线程拒绝，短路。 请求失败(异常，拒绝，超时，短路)时执行fallback(降级)逻辑。 提供熔断器组件,可以自动运行或手动调用,停止当前依赖一段时间(10秒)，熔断器默认错误率阈值为50%,超过将自动运行。 Hystrix流程 流程说明： 1:每次调用创建一个新的HystrixCommand,把依赖调用封装在run()方法中。 2:执行execute()/queue做同步或异步调用。 3:判断熔断器(circuit-breaker)是否打开,如果打开跳到步骤8,进行降级策略,如果关闭进入步骤。 4:判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8,否则继续后续步骤。 5:调用HystrixCommand的run方法.运行依赖逻辑。 5a:依赖逻辑调用超时,进入步骤8。 6:判断逻辑是否调用成功。 6a:返回成功调用结果。 6b:调用出错，进入步骤8。 7:计算熔断器状态,所有的运行状态(成功, 失败, 拒绝,超时)上报给熔断器，用于统计从而判断熔断器状态。 8:getFallback()降级逻辑.以下四种情况将触发getFallback调用： (1):run()方法抛出非HystrixBadRequestException异常 (2):run()方法调用超时 (3):熔断器开启拦截调用 (4):线程池/队列/信号量是否跑满 8a:没有实现getFallback的Command将直接抛出异常。 8b:fallback降级逻辑调用成功直接返回。 8c:降级逻辑调用失败抛出异常。 9:返回执行成功结果。 Hystrix服务降级 在为具体执行逻辑的函数上增加 @HystrixCommand 注解来指定服务降级方法,例如: 12345678@HystrixCommand(fallbackMethod = &quot;fallback&quot;)public String consumer() { return restTemplate.getForObject(&quot;http://eureka-client/dc&quot;, String.class);}public String fallback() { return &quot;fallback&quot;;} 当consumer出现异常的时候，服务请求会通过HystrixCommand注解中指定的降级逻辑进行执行，因此该请求的结果返回了fallback。 Hystrix隔离 Hystrix隔离方式采用线程/信号的方式,通过隔离限制依赖的并发量和阻塞扩散。 其实，我们在定义服务降级的时候，已经自动的实现了依赖隔离。 Hystrix熔断器 Circuit Breaker 每个熔断器默认维护10个bucket,每秒一个bucket,每个bucket记录成功,失败,超时,拒绝的状态，默认错误超过50%且10秒内超过20个请求进行中断拦截. 例1： 12345678910111213141516@HystrixCommand(groupKey = &quot;productStockOpLog&quot;, commandKey = &quot;addProductStockOpLog&quot;, fallbackMethod = &quot;addProductStockOpLogFallback&quot;, commandProperties = { @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;400&quot;),//指定多久超时，单位毫秒。超时进fallback @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;),//判断熔断的最少请求数，默认是10；只有在一个统计窗口内处理的请求数量达到这个阈值，才会进行熔断与否的判断 @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;10&quot;),//判断熔断的阈值，默认值50，表示在一个统计窗口内有50%的请求处理失败，会触发熔断 })public void addProductStockOpLog(Long sku_id, Object old_value, Object new_value) throws Exception { if (new_value != null &amp;&amp; !new_value.equals(old_value)) { doAddOpLog(null, null, sku_id, null, ProductOpType.PRODUCT_STOCK, old_value != null ? String.valueOf(old_value) : null, String.valueOf(new_value), 0, &quot;C端&quot;, null); }}public void addProductStockOpLogFallback(Long sku_id, Object old_value, Object new_value) throws Exception { LOGGER.warn(&quot;发送商品库存变更消息失败,进入Fallback,skuId:{},oldValue:{},newValue:{}&quot;, sku_id, old_value, new_value);} 例2： 1234567891011121314@HystrixCommand(groupKey=&quot;UserGroup&quot;, commandKey = &quot;GetUserByIdCommand&quot;，commandProperties = { @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;100&quot;),//指定多久超时，单位毫秒。超时进fallback @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;),//判断熔断的最少请求数，默认是10；只有在一个统计窗口内处理的请求数量达到这个阈值，才会进行熔断与否的判断 @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;10&quot;),//判断熔断的阈值，默认值50，表示在一个统计窗口内有50%的请求处理失败，会触发熔断},threadPoolProperties = { @HystrixProperty(name = &quot;coreSize&quot;, value = &quot;30&quot;), @HystrixProperty(name = &quot;maxQueueSize&quot;, value = &quot;101&quot;), @HystrixProperty(name = &quot;keepAliveTimeMinutes&quot;, value = &quot;2&quot;), @HystrixProperty(name = &quot;queueSizeRejectionThreshold&quot;, value = &quot;15&quot;), @HystrixProperty(name = &quot;metrics.rollingStats.numBuckets&quot;, value = &quot;12&quot;), @HystrixProperty(name = &quot;metrics.rollingStats.timeInMilliseconds&quot;, value = &quot;1440&quot;)}) 说明： hystrix函数必须为public，fallback函数可以为private。两者需要返回值和参数相同。 参数配置： 参数说明 值 备注 groupKey productStockOpLog group标识，一个group使用一个线程池 commandKey addProductStockOpLog command标识 fallbackMethod addProductStockOpLogFallback fallback方法，两者需要返回值和参数相同 超时时间设置 400ms 执行策略，在THREAD模式下，达到超时时间，可以中断 For most circuits, you should try to set their timeout values close to the 99.5th percentile of a normal healthy system so they will cut off bad requests and not let them take up system resources or affect user behavi 统计窗口（10s）内最少请求数 10 熔断策略 熔断多少秒后去尝试请求 5s 熔断策略，默认值 熔断阀值 10% 熔断策略：一个统计窗口内有10%的请求处理失败，会触发熔断 线程池coreSize 10 默认值（推荐值） 线程池maxQueueSize -1 即线程池队列为SynchronousQueue 配置参数说明 分类 参数 作用 默认值 备注 基本参数 groupKey 表示所属的group，一个group共用线程池 getClass().getSimpleName(); 基本参数 commandKey 当前执行方法名 Execution （ 控制HystrixCommand.run()的执行策略） execution.isolation.strategy 隔离策略，有THREAD和SEMAPHORE THREAD 当前执行方法名 Execution execution.isolation.thread.timeoutInMilliseconds 超时时间 1000ms 默认值：1000 在THREAD模式下，达到超时时间，可以中断 在SEMAPHORE模式下，会等待执行完成后，再去判断是否超时 设置标准： 有retry，99meantime+avg meantime 没有retry，99.5meantime Execution execution.timeout.enabled 是否打开超时 true Execution execution.isolation.thread.interruptOnTimeout 是否打开超时线程中断 true THREAD模式有效 Execution execution.isolation.semaphore.maxConcurrentRequests 信号量最大并发度 10 SEMAPHORE模式有效 Fallback （ 设置当fallback降级发生时的策略） fallback.isolation.semaphore.maxConcurrentRequests fallback最大并发度 10 Fallback fallback.enabled fallback是否可用 true Circuit Breaker （配置熔断的策略） circuitBreaker.enabled 是否开启熔断 true Circuit Breaker circuitBreaker.requestVolumeThreshold 一个统计窗口内熔断触发的最小个数/10s 20 Circuit Breaker circuitBreaker.sleepWindowInMilliseconds 熔断多少秒后去尝试请求 5000ms Circuit Breaker circuitBreaker.errorThresholdPercentage 失败率达到多少百分比后熔断 50 主要根据依赖重要性进行调整 Circuit Breaker circuitBreaker.forceOpen 是否强制开启熔断 Circuit Breaker circuitBreaker.forceClosed 是否强制关闭熔断 如果是强依赖，应该设置为true Metrics （设置关于HystrixCommand执行需要的统计信息） metrics.rollingStats.timeInMilliseconds 设置统计滚动窗口的长度，以毫秒为单位。用于监控和熔断器 10000 滚动窗口被分隔成桶(bucket)，并且进行滚动。 例如这个属性设置10s(10000)，一个桶是1s Metrics metrics.rollingStats.numBuckets 设置统计窗口的桶数量 10 metrics.rollingStats.timeInMilliseconds必须能被这个值整除 Metrics metrics.rollingPercentile.enabled 设置执行时间是否被跟踪，并且计算各个百分比，50%,90%等的时间 true Metrics metrics.rollingPercentile.timeInMilliseconds 设置执行时间在滚动窗口中保留时间，用来计算百分比 60000ms Metrics metrics.rollingPercentile.numBuckets 设置rollingPercentile窗口的桶数量 6 metrics.rollingPercentile.timeInMilliseconds必须能被这个值整除 Metrics metrics.rollingPercentile.bucketSize metrics.rollingPercentile.bucketSize 100 如果设置为100，但是有500次求情，则只会计算最近的100次 Metrics metrics.healthSnapshot.intervalInMilliseconds 采样时间间隔 500 Request Context ( 设置HystrixCommand使用的HystrixRequestContext相关的属性) requestCache.enabled 设置是否缓存请求，request-scope内缓存 true Request Context requestLog.enabled 设置HystrixCommand执行和事件是否打印到HystrixRequestLog中 ThreadPool Properties(配置HystrixCommand使用的线程池的属性) coreSize 设置线程池的core size,这是最大的并发执行数量 10 设置标准：coreSize = requests per second at peak when healthy × 99th percentile latency in seconds + some breathing room 大多数情况下默认的10个线程都是值得建议的 ThreadPool Properties maxQueueSize 最大队列长度。设置BlockingQueue的最大长度 -1 默认值：-1 如果使用正数，队列将从SynchronousQueue改为LinkedBlockingQueue ThreadPool Properties queueSizeRejectionThreshold 设置拒绝请求的临界值 5 此属性不适用于maxQueueSize = - 1时 设置设个值的原因是maxQueueSize值运行时不能改变，我们可以通过修改这个变量动态修改允许排队的长度 ThreadPool Properties keepAliveTimeMinutes 设置keep-live时间 1分钟 这个一般用不到因为默认corePoolSize和maxPoolSize是一样的 Hystrix官方文档 https://github.com/Netflix/Hystrix/wiki","link":"/2018/02/05/hystrix%E5%9C%A8spring%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%92%8C%E9%85%8D%E7%BD%AE/"},{"title":"elasticsearch安装","text":"elasticsearch安装 安装 从 elastic 的官网 elastic.co/downloads/elasticsearch 获取最新版本的 Elasticsearch。 当你准备在生产环境安装 Elasticsearch 时，你可以在 官网下载地址 找 到 Debian 或者 RPM 包，除此之外，你也可以使用官方支持的 Puppet module 或者 Chef cookbook。 当你解压好了归档文件之后，Elasticsearch 已经准备好运行了。按照下面的操作，在前台(foregroud)启动 Elasticsearch： 12cd elasticsearch-&lt;version&gt;./bin/elasticsearch 如果你想把 Elasticsearch 作为一个守护进程在后台运行，那么可以在后面添加参数 -d 。 如果你是在 Windows 上面运行 Elasticseach，你应该运行 bin\\elasticsearch.bat 而不是 bin\\elasticsearch 。 测试 Elasticsearch 是否启动成功，可以打开另一个终端，执行以下操作： 1curl 'http://localhost:9200/?pretty' 配置 jdk elasticsearch 的7.3.0需要jdk11，安装包自带了，如果环境中JAVA_HOME不是jdk11的话，修改bin\\elasticsearch-env脚本，如下： 1234567891011# now set the path to java##if [ ! -z &quot;$JAVA_HOME&quot; ]; then## JAVA=&quot;$JAVA_HOME/bin/java&quot;##else if [ &quot;$(uname -s)&quot; = &quot;Darwin&quot; ]; then # OSX has a different structure JAVA=&quot;$ES_HOME/jdk/Contents/Home/bin/java&quot; else JAVA=&quot;$ES_HOME/jdk/bin/java&quot; fi##fi 将java路径设置成elasticsearch带的jdk路径。 远程地址 默认情况下，Elasticsearch 只允许本机访问，如果需要远程访问，可以修改 Elasticsearch 安装目录中的config/elasticsearch.yml文件，去掉network.host的注释，将它的值改成0.0.0.0，让任何人都可以访问，然后重新启动 Elasticsearch 。 停止 获取PID 12$ jps | grep Elasticsearch14542 Elasticsearch 或者 1234$ ./bin/elasticsearch -p /tmp/elasticsearch-pid -d$ cat /tmp/elasticsearch-pid &amp;&amp; echo15516kill -SIGTERM 15516 启动问题 org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root 不能用root身份登录 解决办法： 123groupadd studentuseradd es -g student -p 123chown -R es:student elasticsearch-7.0.0 or 123adduser espasswd essudo chown -R es elasticsearch-&lt;version&gt;/ max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535] 问题翻译过来就是：elasticsearch用户拥有的可创建文件描述的权限太低，至少需要65536； 解决办法： 12345切换到root用户修改#vim /etc/security/limits.conf在最后面追加下面内容*** hard nofile 65536*** soft nofile 65536 *** 是启动ES的用户 退出用户重新登录，使配置生效 重新 ulimit -Hn 查看硬限制 会发现数值有4096改成65535 max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 问题翻译过来就是：elasticsearch用户拥有的内存权限太小，至少需要262144； 解决办法： 123456切换到root用户,执行命令：#sysctl -w vm.max_map_count=262144查看结果：#sysctl -a|grep vm.max_map_count显示：vm.max_map_count = 262144 上述方法修改之后，如果重启虚拟机将失效，所以： 解决办法： 在 /etc/sysctl.conf 文件最后添加一行 vm.max_map_count=262144 即可永久修改 the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured 默认的发现设置不适合生产使用;至少有一个[发现]。seed_hosts,发现。seed_providers,集群。必须配置initial_master_nodes] 这时候继续编辑elasticsearch.yml文件 将 #cluster.initial_master_nodes: [“node-1”, “node-2”] 修改为 cluster.initial_master_nodes: [“node-1”]，记得保存。","link":"/2019/08/08/elasticsearch%E5%AE%89%E8%A3%85/"},{"title":"kafka启动","text":"启动kafka 启动前需要先查看ZooKeeper是否已经安装启动 前台启动kafka 1./bin/kafka-server-start.sh ./config/server.properties 后台启动kafka 123nohup ./bin/kafka-server-start.sh config/server.properties &amp;或者./bin/kafka-server-start.sh -daemon config/server.properties 检查kafka是否启动 123jps -l | grep kafka输出：16318 kafka.Kafka 停止kafka 1./bin/kafka-server-stop.sh 停止ZooKeeper 1./bin/zookeeper-server-stop.sh","link":"/2018/08/21/kafka%E5%90%AF%E5%8A%A8/"},{"title":"Kafka入门","text":"Kafka入门 Kafka最初是由LinkedIn（领英）开发，并随后于2011年初开源，并于2012年10月23日由Apache Incubator孵化出站。Kafka由Scala和Java编写。该项目的目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个“按照分布式事务日志架构的大规模发布/订阅消息队列”，这使它作为企业级基础设施来处理流式数据非常有价值。此外，Kafka可以通过Kafka Connect连接到外部系统（用于数据输入/输出），并提供了Kafka Streams（一个Java流式处理库）。 Apache的Kafka™是一个分布式流平台(a distributed streaming platform) 它可以让你发布和订阅记录流。在这方面，它类似于一个消息队列或企业消息系统。 它可以让你持久化收到的记录流，从而具有容错能力。 它可以让你处理收到的记录流。 何为消息系统 消息系统负责将数据从一个应用程序传输到另一个应用程序，因此应用程序可以专注于数据，但不用关心消息如何共享。 分布式消息传递基于可靠消息队列的概念。 消息在客户端应用程序和消息传递系统之间异步排队。 有两种类型的消息模式可用：一种是点对点，另一种是发布-订阅（pub-sub）消息系统。 大多数消息模式遵循pub-sub。 点对点消息系统 在点对点消息系统中，消息被保留在队列中。 一个或多个消费者可以消费队列中的消息，但是特定消息最多只能由一个消费者消费。 一旦消费者读取队列中的消息，它就从该队列中消失。 该系统的典型示例是订单处理系统，其中每个订单将由一个订单处理器处理，但多个订单处理器也可以同时工作。 发布-订阅消息系统 在发布-订阅系统中，消息被保留在一个主题中。与点对点系统不同，消费者可以订阅一个或多个主题并使用该主题中的所有消息。在发布-订阅系统中，消息生产者称为发布者，消息使用者称为订阅者。 Kafka通常用于下面的使用场景 监控 Kafka通常用于监控数据的操作。 这涉及聚合来自分布式应用程序的统计信息，以产生集中化的操作数据。 日志聚合方案 kafka可以用来收集跨组织的多个服务的日志，并将这些日志转为统一的格式供消费者使用。 流式处理 流行的实时计算框架，如Storm和Spark流式读取topic中的数据进行变换、处理，然后将结果写入新的，用户或应用需要使用的topic中。Kafka强大的持久性功能在流式处理上下文中也是非常有用的。 解耦 在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你可以做出独立的扩展或者是修改两边的处理过程，只要确保它们遵守同样的接口约束。 Kafka的架构 在上图中一个topic被配置为拥有3个分区，分区1包含两个偏移因子0和1。分区2包含4个偏移因子0， 1， 2 和 3。分区3包含1个偏移因子0。 分区副本的id和该副本所在的broker的id一致。 假设，如果一个topic配置的副本因子为3，则kafka会针对该topic的每个分区创建3个独立的副本并将这些副本尽量均匀分散到集群的每个节点上。为了在集群的节点间进行负载，每一个broker都会保存一个或多个这样的分区。多个producer和consumer可以同时发布或获取消息。 相关术语 组件 描述 topics 隶属于特定分类的消息流称为topic。数据保存在topic中 partitions topics会被切分为分区。针对每一个主题，kafka最少保持一个分区。每一个这样的分区以顺序不可变的方式保存消息。一个分区有一个或多个大小相同的segment文件组成。Topics拥有多个分区，因此可以保存大量的数据 Partition offset 每个分区中的消息拥有一个唯一的序列id，被称为offset Replicas of partition 分区副本仅仅是分区的备份，不会对副本分区进行读写操作，只是用来防止数据丢失 Brokers 1. Brokers 是维护发布消息的系统。每个broker针对每个topic可能包含0个或多个该topic的分区。假设，一个topic拥有N个分区，并且集群拥有N个broker，则每个broker会负责一个分区2. 假设，一个topic拥有N个分区，并且集群拥有N+M个broker，则前N个broker每个处理一个分区，剩余的M个broker则不会处理任何分区3。 假设，一个topic拥有N个分区，并且集群拥有M个broker（M &lt; N），则这些分区会在所有的broker中进行均匀分配。每个broker可能会处理一个或多个分区。这种场景不推荐使用，因为会导致热点问题和负载不均衡问题 Kafka Cluster 由多个broker组成的kafka被称为kafka集群。一个kafka集群在不停机扩展。集群负载所有消息的持久化和副本处理 Producers Producers 是向一个或多个Kafka中topic发布消息的发布者。Producers 将消息发送到 Kafka 的 brokers中。任意时刻 producer 发布到broker中的消息都会被追加到某个分区的最后一个segment文件的最后。Producer 也可以选择消息发送到指定的分区 Consumers Consumers 从broker读取数据。Consumers 订阅一个或多个 topic，并通过pull方式从broker拉取订阅的数据 Leader Leader是负责某个分区数据读写操作的节点。每个分区都有一个leader Follower 跟随leader操作的节点被称为follower。如果leader节点不可用，则会从所有的fellower中挑选一个作为新的leader节点。一个follower节点作为leader节点一个普通的消费者，拉取leader数据并更新自己的数据存储 kafka集群模型 下面的表格描述了在上图中提到的每个组件的详细信息。 组件 描述 Broker Kafka集群通常使用多个Broker来实现集群的负载均衡。 Kafka brokers 是无状态的，因为它们使用 ZooKeeper 来保持它们的集群信息。 单个Kafka Broker 每秒可以处理数十万的读写请求，即使保存了TB级的数据也不会影响性能。Kafka broker leader 的选举是通过Zookeeper实现的 ZooKeeper ZooKeeper是用来管理和协调Kafka broker 的。ZooKeeper 服务主要用来通知 producer 和 consumer 关于任何新加入Kafka集群或某个Kafka Broker宕机退出集群的消息。 根据收到Zookeeper的关于Broker的存在或失败的消息通知，然后生产者和消费者采取决定，并开始与其它Broker协调它们的任务 Producers producer将数据推送给Broker。 当新Broker启动时，所有生产者搜索它并自动发送消息到该新Broker。 Kafka Producer不等待来自Broker的确认，并以Broker可以处理的速度发送消息 Consumers 由于 Kafka brokers 是无状态的， 因此需要Consumer来维护根据partition offset已经消费的消息数量信息。 如果 consumer 确认了一个指定消息的offset，那也就意味着 consumer 已经消费了该offset之前的所有消息。Consumer可以向Broker异步发起一个拉取消息的请求来缓存待消费的消息。consumers 也可以通过提供一个指定的offset值来回溯或跳过Partition中的消息。Consumer 消费消息的offset值是保存在ZooKeeper中的 Kafka工作流程 Kafka是由分裂为一个或多个partition的topic的集合。 Kafka中的partition可以认为是消息的线性排序序列，其中每个消息由它们的索引（称为offset）来标识。 Kafka集群中的所有数据是每个partition数据分区的并集。 新写入的消息写在分区的末尾，消息由消费者顺序读取。通过将消息复制到不同的Broker来提供持久性。Kafka以快速，可靠，持久，容错和零停机的方式提供基于pub-sub和队列模型的消息系统。 在这两种情况下，生产者只需将消息发送到topic，消费者可以根据自己的需要选择任何一种类型的消息传递系统。 Pub-Sub 消息模型工作流程 生产者定期向topic发送消息。 Kafka broker 根据配置将topic的消息存储到指定的partition上。Kafka确保所有的消息均匀分布在topic的所有partition上。如果producer发送了两条消息，并且该topic有两个partition，则每个partition会有一条消息。 Consumer 订阅指定的topic。 一旦消费者订阅了topic，Kafka将向消费者提供topic的当前offset，并且还将offset保存在Zookeeper中。 消费者将定期请求Kafka（如100 Ms）新消息。 消费者将收到消息并进行处理。 一旦消息被处理，消费者将向Kafka broker发送确认。 一旦Kafka收到确认，它将offset更改为新值，并在Zookeeper中更新它。 由于offset在Zookeeper中被维护，消费者可以正确地读取下一条消息，即使服务器宕机后重启。 以上流程将重复，直到消费者停止请求。 消费者可以随时回退/跳转到某个topic的期望offset处，并读取所有后续消息。 队列消息模型工作流程 &amp; Consumer Group 在基于队列的消息系统中，取代单个消费者的是订阅了相同topic的一群拥有相同Group ID的消费者集群。简单来说，订阅具有相同“组ID”的主题的消费者被认为是单个组，并且消息在它们之间共享。 生产者定期向topic发送消息。 Kafka broker 根据配置将topic的消息存储到指定的partition上。 单个consumer以名为Group-1的Group ID 订阅名为Topic-01的topic。 Kafka 会以和Pub-Sub消息模型相同的方式和consumer进行交互直到新的消费者以同样的Group ID加入到消费者分组中。 一旦新的消费者加入后，Kafka将操作切换到共享模式，将所有topic的消息在两个消费者间进行均衡消费。这种共享行为直到加入的消费者结点数目达到该topic的分区数。 一旦消费者的数目大于topic的分区数，则新的消费者不会收到任何消息直到已经存在的消费者取消订阅。出现这种情况是因为Kafka中的每个消费者将被分配至少一个分区，并且一旦所有分区被分配给现有消费者，新消费者将必须等待。 该功能被称为 “Consumer Group”。以同样的方式，Kafka将以非常简单和高效的方式提供这两种系统功能。 kafka 消息的语义 消息系统系统一般有以下的语义： At most once：消息可能丢失，但不会重复投递 At least once：消息不会丢失，但可能会重复投递 Exactly once：消息不丢失、不重复，会且只会被分发一次（真正想要的） Producer 发送消息以后，有一个commit的概念，如果commit成功，则意味着消息不会丢失，但是Producer有可能提交成功后，没有收到commit的消息。这有可能造成 at least once 语义。 从 Consumer 角度来看，我们知道 Offset 是由 Consumer 自己维护。所以何时更新 Offset 就决定了 Consumer 的语义。如果收到消息后更新 Offset，如果 Consumer crash，那新的 Cunsumer再次重启消费，就会造成 At most once 语义（消息会丢，但不重复）。 如果 Consumser 消费完成后，再更新 Offset。如果 Consumer crash，别的 Consumer 重新用这个 Offser 拉取消息，这个时候就会造成 at least once 的语义（消息不丢，但多次被处理）。 所以结论：默认Kafka提供at-least-once语义的消息分发，允许用户通过在处理消息之前保存位置信息的方式来提供at-most-once语义。如果我们可以实现消费是幂等的，这个时候就可以认为整个系统是Exactly once的了。 kafka中的partition和offset 说到分区，就要说kafka对消息的存储.在官方文档中。 首先，kafka是通过log(日志)来记录消息发布的.每当产生一个消息，kafka会记录到本地的log文件中，这个log和我们平时的log有一定的区别.这里可以参考一下The Log。 分区partition kafka是为分布式环境设计的，因此如果日志文件，其实也可以理解成消息数据库，放在同一个地方，那么必然会带来可用性的下降，一挂全挂，如果全量拷贝到所有的机器上，那么数据又存在过多的冗余，而且由于每台机器的磁盘大小是有限的，所以即使有再多的机器，可处理的消息还是被磁盘所限制，无法超越当前磁盘大小，因此有了partition的概念。 kafka对消息进行一定的计算，通过hash来进行分区。这样，就把一份log文件分成了多份。如上面的分区读写日志图，分成多份以后，在单台broker上，比如快速上手中，如果新建topic的时候，我们选择了–replication-factor 1 --partitions 2，那么在log目录里，我们会看到test-0目录和test-1目录.就是两个分区了。 偏移offset 分区就是一个有序的，不可变的消息队列。新来的commit log持续往后面加数据。这些消息被分配了一个下标(或者偏移)，就是offset，用来定位这一条消息。 消费者消费到了哪条消息，是保持在消费者这一端的。消息者也可以控制，消费者可以在本地保存最后消息的offset，并间歇性的向zookeeper注册offset，也可以重置offset。 partition存储的时候，又分成了多个segment(段)，然后通过一个index，索引，来标识第几段。这里先可以去看一下本地log目录的分区文件夹。例如，test-0，这个分区里面，会有一个index文件和一个log文件： 对于某个指定的分区，假设每5个消息，作为一个段大小，当产生了10条消息的情况想，目前有会得到： 0.index (表示这里index是对0-4做的索引) 5.index (表示这里index是对5-9做的索引) 10.index (表示这里index是对10-15做的索引，目前还没满) 和 0.log 5.log 10.log ，当消费者需要读取offset=8的时候，首先kafka对index文件列表进行二分查找，可以算出，应该是在5。index对应的log文件中，然后对对应的5.log文件，进行顺序查找： 5-&gt;6-&gt;7-&gt;8，直到顺序找到8就好了。 参考 kafka入门教程 kafka中的partition和offset","link":"/2018/03/22/kafka%E5%85%A5%E9%97%A8/"},{"title":"maven利用插件antrun使用if条件标签","text":"利用 maven 对项目打包的时候，有时候需要根据条件选择不同的文件或输出到不同路径，因此希望 maven 能够提供 if 标签，maven 的 antrun 插件可以提供 if 标签。 antrun 简介 通过 antrun 插件可以在 maven 中运行 ant task，可以在 POM 文件中嵌入 ant 脚本。具体参考 antrun。 使用 antrun 可像如下格式使用 antrun 1234567891011121314151617181920212223242526272829&lt;project&gt; [...] &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt; &lt;!-- a lifecycle phase --&gt; &lt;/phase&gt; &lt;configuration&gt; &lt;target&gt; &lt;!-- 在这里添加 ant task， 所有能在 ant 的 build.xml 的&lt;target&gt;标签里的都可以出现这里 --&gt; &lt;/target&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; [...]&lt;/project&gt; 使用 if 标签 要使用 if 标签，需要引入 Ant-Contrib，Ant-Contrib 是 ant 的一个任务集。配置例子如下： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;project&gt; [...] &lt;build&gt; [...] &lt;plugins&gt; [...] &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; [...] &lt;configuration&gt; &lt;target&gt; &lt;!-- 下面这句很重要，ant 会加载 antcontrib.properties 中定义的标签，其中就有 if 标签--&gt; &lt;taskdef resource=&quot;net/sf/antcontrib/antcontrib.properties&quot; classpathref=&quot;maven.plugin.classpath&quot;/&gt; [...] &lt;/target&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;ant-contrib&lt;/groupId&gt; &lt;artifactId&gt;ant-contrib&lt;/artifactId&gt; &lt;version&gt;1.0b3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;ant&lt;/groupId&gt; &lt;artifactId&gt;ant&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 注意引入了 ant-contrib 这个倚赖包。 if 标签的例子 下面是一个根据条件选择不同的文件的例子： 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;target&gt; &lt;taskdef resource=&quot;net/sf/antcontrib/antcontrib.properties&quot; classpathref=&quot;maven.plugin.classpath&quot;/&gt; &lt;if&gt; &lt;equals arg1=&quot;${profileActive}&quot; arg2=&quot;local&quot;/&gt; &lt;then&gt; &lt;copy todir=&quot;${project.build.directory}/classes&quot; overwrite=&quot;true&quot;&gt;&lt;!--执行复制操作,todir的值是将要复制文件到的地方,overwrite是否重写--&gt; &lt;fileset dir=&quot;src/main/resources&quot;&gt;&lt;!--${project.build.directory}值是你的target目录--&gt; &lt;include name=&quot;logback-spring-${profileActive}.xml&quot;/&gt; &lt;/fileset&gt; &lt;/copy&gt; &lt;/then&gt; &lt;else&gt; &lt;copy todir=&quot;${project.build.directory}/classes&quot; overwrite=&quot;true&quot;&gt;&lt;!--执行复制操作,todir的值是将要复制文件到的地方,overwrite是否重写--&gt; &lt;fileset dir=&quot;src/main/resources&quot;&gt;&lt;!--${project.build.directory}值是你的target目录--&gt; &lt;include name=&quot;logback-spring.xml&quot;/&gt; &lt;/fileset&gt; &lt;/copy&gt; &lt;/else&gt; &lt;/if&gt; &lt;/target&gt;&lt;/configuration&gt; 其中 &lt;equals arg1=&quot;${profileActive}&quot; arg2=&quot;local&quot;/&gt; 的意思是： arg1 的值等于${profileActive}, arg2 的值等于local，判断 arg1 是否等于 arg2。 结合if标签，就是如果 arg1 等于 arg2，就执行 then 标签里面的任务，否则，就执行else 标签中的任务。 if 标签的其它例子 Ant-Contrib 的 if 标签还有一些其它用法，例如 elseif 标签，例如： 123456789101112131415161718&lt;if&gt; &lt;equals arg1=&quot;${foo}&quot; arg2=&quot;bar&quot; /&gt; &lt;then&gt; &lt;echo message=&quot;The value of property foo is 'bar'&quot; /&gt; &lt;/then&gt; &lt;elseif&gt; &lt;equals arg1=&quot;${foo}&quot; arg2=&quot;foo&quot; /&gt; &lt;then&gt; &lt;echo message=&quot;The value of property foo is 'foo'&quot; /&gt; &lt;/then&gt; &lt;/elseif&gt; &lt;else&gt; &lt;echo message=&quot;The value of property foo is not 'foo' or 'bar'&quot; /&gt; &lt;/else&gt;&lt;/if&gt; 具体可以参看文档。","link":"/2020/08/12/maven%E5%88%A9%E7%94%A8%E6%8F%92%E4%BB%B6antrun%E4%BD%BF%E7%94%A8if%E6%9D%A1%E4%BB%B6%E6%A0%87%E7%AD%BE/"},{"title":"oracle列表中最大表达式数为1000，mybaits解决方案","text":"问题 在使用 mybatis 查询的时候报错：ORA-01795 列表中的最大表达式数为1000。同样的 sql 语句在 mysql 数据库没有问题，只是 oracle 数据库报此错误。这个报错信息也非常明确了，就是where in的后面带的参数太多了，超过了1000个。 解决方案 因为超过了1000条，那么就让in后面的参数不超过1000。将查询参数按1000进行分组，分组可以采用 guava 的 Lists.Partition 进行分组，得到List&lt;List&lt;T&gt;&gt;。 方案1：循环请求查询 查询 sql 语句不变，循环执行分组后的列表，再合并查询结果。但这种方案需要多次查询数据库，效率不高。 方案2：采用 or 差分 in 把in拆成多个in，用or来连接，因为or两边如果都是索引的话，索引是不会失效。以 user 表为例： 123select id from user where id in(?,?,?,?.....)--- 拆分后select id from user where (id in(?,?,?,?.....) or id in(?,?,?,?.....) maper接口修改 1List&lt;LONG&gt; selectUsers(List&lt;List&lt;String&gt;&gt; idss) mapper xml 修改 123456789&lt;select id=&quot;selectUsers&quot;&gt; select id from user where &lt;foreach collection=&quot;list&quot; item=&quot;ids&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;or&quot; &gt; id in &lt;foreach collection=&quot;ids&quot; item=&quot;idx&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot; &gt; #{idx} &lt;/foreach&gt; &lt;/foreach&gt;&lt;/select&gt; 代码调整，在查询之前先对参数进行分组，然后调用 selectUsers 方法。 最后 推荐后一种方案，但最好执行一下执行计划，确定能够触发索引。","link":"/2023/02/03/oracle%E5%88%97%E8%A1%A8%E4%B8%AD%E6%9C%80%E5%A4%A7%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%95%B0%E4%B8%BA1000%EF%BC%8Cmybaits%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"java循环优化","text":"在许多应用程序中，循环都扮演着非常重要的角色。为了提升循环的运行效率，研发编译器的工程师提出了不少面向循环的编译优化方式，如循环无关代码外提，循环 展开等。 今天，我们便来了解一下，Java 虚拟机中的即时编译器都应用了哪些面向循环的编译优化。 循环外提 所谓的循环无关代码（Loop-invariant Code），指的是循环中值不变的表达式。如果能够在不改变程序语义的情况下，将这些循环无关代码提出循环之外，那么 程序便可以避免重复执行这些表达式，从而达到性能提升的效果。 例子： 1234567public int loop1(int x, int y, int[] ary) { int sum = 0; for (int i = 0; i &lt; ary.length; i++) { sum += x * y + ary[i]; } return sum;} 对应字节码 1234567891011121314151617181920212223242526272829public int loop1(int, int, int[]); descriptor: (II[I)I flags: ACC_PUBLIC Code: stack=4, locals=6, args_size=4 0: iconst_0 1: istore 4 3: iconst_0 4: istore 5 // 循环开始 6: iload 5 8: aload_3 9: arraylength // ary.length 10: if_icmpge 32 // i &lt; ary.length 13: iload 4 15: iload_1 16: iload_2 17: imul // x*y 18: aload_3 19: iload 5 21: iaload // ary[i] 22: iadd 23: iadd 24: istore 4 26: iinc 5, 1 29: goto 6 // 循环结束 32: iload 4 34: ireturn 在上面这段代码中，循环体中的表达式x*y，以及循环判断条件中的ary.length均属于循环不变代码。 前者是一个整数乘法运算，而后者则是内存访问操作，读取数组对象ary的长度。（数组的长度存放于数组对象的对象头中，可通过 arraylength 指令来访问。） 理想情况下，上面这段代码经过循环无关代码外提之后，等同于下面这一手工优化版本。 123456789public int loop1Opt(int x, int y, int[] ary) { int sum = 0; int t0 = x * y; int t1 = ary.length; for (int i = 0; i &lt; t1; i++) { sum += t0 + ary[i]; } return sum;} 对应字节码 123456789101112131415161718192021222324252627282930313233public int loop1Opt(int, int, int[]); descriptor: (II[I)I flags: ACC_PUBLIC Code: stack=4, locals=8, args_size=4 0: iconst_0 1: istore 4 3: iload_1 4: iload_2 5: imul 6: istore 5 8: aload_3 9: arraylength 10: istore 6 12: iconst_0 13: istore 7 // 循环开始 15: iload 7 17: iload 6 19: if_icmpge 40 22: iload 4 24: iload 5 26: aload_3 27: iload 7 29: iaload // ary[i] 30: iadd 31: iadd 32: istore 4 34: iinc 7, 1 37: goto 15 // 循环结束 40: iload 4 42: ireturn 我们可以看到，无论是乘法运算x*y，还是内存访问ary.length，现在都在循环之前完成。原本循环中需要执行这两个表达式的地方，现在直接使用循环之前这两个表达式的执行结果。 java的JIT实现了循环无关代码的外提。 即时编译器JIT还外提了 int 数组加载指令iaload所暗含的 null 检测（null check）以及下标范围检测（range check）。 如果将iaload指令想象成一个接收数组对象以及下标作为参数，并且返回对应数组元素的方法，那么它的伪代码大致如下所示： 1234567891011public int iaload(int[] aryRef, int index) { if (aryRef == null) { // null 检测 throw new NullPointerException(); } if (index &lt; 0 || index &gt;= aryRef.length) { // 下标范围检测 throw new ArrayIndexOutOfBoundsException(); } return aryRef[index];} loop1 方法中的 null 检测属于循环无关代码。这是因为它始终检测作为输入参数的 int 数组是否为 null，而这与第几次循环无关。 为了更好地阐述具体的优化，修改了原来的例子，并将iaload展开，形成如下所示的代码： 123456789101112131415public int loop1(int[] a) { int sum = 0; for (int i = 0; i &lt; a.length; i++) { if (a == null) { // null check throw new NullPointerException(); } if (i &lt; 0 || i &gt;= a.length) { // range check throw new ArrayIndexOutOfBoundsException(); } sum += a[i]; } return sum;} 在上面的代码段中，null 检测涉及了控制流依赖，无法完成外提。 在 HotSpot VM的C2编译器 中，null 检测的外提是通过额外的编译优化，也就是循环预测（Loop Prediction，对应虚拟机参数-XX:+UseLoopPredicate）来实现的。该优化的实际做法是在循环之前插入同样的检测代码，并在命中的时候进行去优化。这样一来，循环中的检测代码便会被归纳并消除掉。 12345678910111213141516171819public int loop2(int[] a) { int sum = 0; if (a == null) { deoptimize(); // never returns } for (int i = 0; i &lt; a.length; i++) { if (a == null) { // now evluate to false throw new NullPointerException(); } if (i &lt; 0 || i &gt;= a.length) { // range check throw new ArrayIndexOutOfBoundsException(); } sum += a[i]; } return sum;} 循环展开 另外一项非常重要的循环优化是循环展开（Loop Unrolling）。它指的是在循环体中重复多次循环迭代，并减少循环次数的编译优化。 1234567public int loop3(int[] ary) { int sum = 0; for (int i = 0; i &lt; 64; i++) { sum += (i % 2 == 0) ? ary[i] : -ary[i]; } return sum;} 上面的代码经过一次循环展开之后将形成下面的代码： 123456789public int loop3Opt(int[] ary) { int sum = 0; for (int i = 0; i &lt; 64; i += 2) { // 注意这里的步数是 2 sum += (i % 2 == 0) ? ary[i] : -ary[i]; sum += ((i + 1) % 2 == 0) ? ary[i + 1] : -ary[i + 1]; } return sum;} 循环展开的缺点显而易见：它可能会增加代码的冗余度，导致所生成机器码的长度大幅上涨。 不过，随着循环体的增大，优化机会也会不断增加。一旦循环展开能够触发进一步的优化，总体的代码复杂度也将降低。比如前面的例子经过循环展开之后便可以进一步优化为如下所示的代码： 12345678public int loop3Opt1(int[] ary) { int sum = 0; for (int i = 0; i &lt; 64; i += 2) { sum += ary[i]; sum += -ary[i + 1]; } return sum;} 其他循环优化 除了循环无关代码外提以及循环展开之外，即时编译器还有两个比较重要的循环优化技术：循环判断外提（loop unswitching）以及循环剥离（loop peeling）。 循环判断外提 指的是将循环中的 if 语句外提至循环之前，并且在该 if 语句的两个分支中分别放置一份循环代码。 123456789public int loop4(int[] ary) { int sum = 0; for (int i = 0; i &lt; ary.length; i++) { if (ary.length &gt; 4) { sum += ary[i]; } } return sum;} 上面这段代码经过循环判断外提之后，将变成下面这段代码： 12345678910111213 public int loop4Opt(int[] ary) { int sum = 0; if (ary.length &gt; 4) { for (int i = 0; i &lt; ary.length; i++) { sum += ary[i]; } } else { for (int i = 0; i &lt; ary.length; i++) { } } return sum; }} 进一步优化 123456789public int loop4Opt1(int[] ary) { int sum = 0; if (ary.length &gt; 4) { for (int i = 0; i &lt; ary.length; i++) { sum += ary[i]; } } return sum;} 循环判断外提与循环无关检测外提所针对的代码模式比较类似，都是循环中的 if 语句。不同的是，后者在检查失败时会抛出异常，中止当前的正常执行路径；而前者所针对的是更加常见的情况，即通过 if 语句的不同分支执行不同的代码逻辑。 循环剥离 指的是将循环的前几个迭代或者后几个迭代剥离出循环的优化方式。一般来说，循环的前几个迭代或者后几个迭代都包含特殊处理。通过将这几个特殊的迭代剥离出去，可以使原本的循环体的规律性更加明显，从而触发进一步的优化。 123456789public int loop5(int[] ary) { int j = 0; int sum = 0; for (int i = 0; i &lt; ary.length; i++) { sum += ary[j]; j = i; } return sum;} 上面这段代码剥离了第一个迭代后，将变成下面这段代码： 12345678910public int loop5Opt(int[] ary) { int sum = 0; if (null != ary &amp;&amp; 0 &lt; ary.length) { sum += ary[0]; for (int i = 1; i &lt; ary.length; i++) { sum += ary[i - 1]; } } return sum;} 转自 极客时间 -《循环优化》","link":"/2020/04/23/java%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"},{"title":"mybatis if test判断错误处理","text":"mybatis 中常用到标签进行if判断，但有时不注意写法，就会出现判断失效的情况。 单个字符 12&lt;if test=&quot;take == '0'&quot;&gt;&lt;/if&gt; 这里不会进行判断。mybatis 是用 OGNL 表达式来解析的，在 OGNL 的表达式中，'0’会被解析成字符，java是强类型的，char 和 一个string 会导致不等，所以比较的结果只会是false。不会报错。可用双引号处理： 12&lt;if test='take == &quot;0&quot;'&gt;&lt;/if&gt; 或者类型转换： 12&lt;if test=&quot;take == '0'.toString()&quot;&gt;&lt;/if&gt; == 错写为 = 12&lt;if test=&quot;take = null&quot;&gt;&lt;/if&gt; 这里不会进行判断。原因同上，而且 take 会赋值为 null。","link":"/2022/10/27/mybatis-if-test%E5%88%A4%E6%96%AD%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"title":"springboot中application.yml和bootstrap.yml的区别","text":"spring boot 默认支持 properties(.properties)和 YAML(.yml .yaml ) 两种格式的配置文件。 yml 和 properties 文件都属于配置文件，它们的功能一样。 在 spring boot 框架中 bootstrap.yml 和 application.yml 都可以用来配置参数，甚至这两个文件可以同时出现。 配置写到 bootstrap.yml和写到 application.yml 有什么区别 加载顺序不一样 bootstrap.yml 先加载 application.yml 后加载。 从技术上来讲，bootstrap.yml 由父 Spring ApplicationContext 加载。父 ApplicationContext 在使用 application.yml 之前被加载。 应用场景不一样 bootstrap.yml 典型的应用场景 stackoverflow 中有个高票（301）的回答： I have just asked the Spring Cloud guys and thought I &gt;should share the info I have here. bootstrap.yml is loaded before application.yml. It is typically used for the following: when using Spring Cloud Config Server, you should specify spring.application.name and spring.cloud.config.server.git.uri inside bootstrap.yml some encryption/decryption information Technically, bootstrap.yml is loaded by a parent Spring ApplicationContext. That parent ApplicationContext is loaded before the one that uses application.yml. 大致意思如下： 当使用 Spring Cloud Config Server 配置中心时，需要在 bootstrap.yml 配置文件中指定 spring.application.name 和 spring.cloud.config.server.git.uri，添加连接到配置中心的配置属性来加载外部配置中心的配置信息。 一些加密/解密信息 因为当使用 Spring Cloud 的时候，配置信息一般是从 config server 加载的，为了取得配置信息（比如密码等），需要一些提早的或引导配置。因此，把 config server 信息放在 bootstrap.yml，用来加载真正需要的配置信息。config server 可能做了安全认证，所以访问所需的加解密信息也需要配置在 bootstrap.yml 里。 属性覆盖 不接配置中心的情况下，启动的时候 spring boot 默认会加载 bootstrap.yml 以及 bootstrap-{profile}。{profile}在 bootstrap.yml中 spring.profiles.active 指定。 加载顺序是： bootstrap.yml &gt; bootstrap-{profile}.yml &gt; application.yml &gt;application-{profile}.yml 如果这4个配置文件中存在相同的属性，那么后加载的属性值会覆盖掉前加载的属性值。 需要注意的是，有些文章说 bootstrap 不会被本地配置覆盖，如果这个说法是指 bootstrap 配置属性不会被 application 覆盖，那是错误的。 如果 spring.profiles.active 配置多个 profile，最后面的 profile 才会生效。例如： 12345spring: application: name: foo profiles: active: dev,mysql mysql profile才有效 在接配置中心的情况下，如果有 application.yml，它的属性值会被从配置中心中的同名属性值覆盖。 思考 在没有配置中心的情况下，是选择使用 bootstrap.yml 还是 application.yml，或者两者都用？ 根据 bootstrap.yml 典型的应用场景，在没有配置中心的情况下，使用 bootstrap.yml 的意义不大，即使有加解密信息，将它们放到 application.yml 也是可以的。 建议在没有配置中心的情况下，去掉 bootstrap.yml 只使用 application.yml，减少配置文件，配置集中以减少出错的几率。","link":"/2020/08/06/springboot%E4%B8%ADapplication-yml%E5%92%8Cbootstrap-yml%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"onclick无效的一个原因","text":"写 html 的时候发现 onclick 的事件不生效，另外再写一个 onclick 事件又能够成功。花了好久查不到原因。代码如下： 12345678&lt;div id=&quot;choosefile&quot; onclick=&quot;choosefile();&quot;&gt; &lt;div style=&quot;text-align:center&quot;&gt;open md file&lt;/div&gt; &lt;input type=&quot;file&quot; id=&quot;file0&quot; style=&quot;display: none&quot; onchange=&quot;selectedFile();&quot;&gt;&lt;/div&gt;&lt;input type=&quot;button&quot; id=&quot;save0&quot; value=&quot;保存&quot; οnclick=&quot;alert('hello world')&quot; /&gt;&lt;input type=&quot;button&quot; id=&quot;md0&quot; value=&quot;导出文档&quot; οnclick=&quot;exportFile()&quot; /&gt; 后面两个 button 的click事件不生效。 正常来说，字符 o 对应的 ascii 码值应为 111，但是上面代码中后面两个 button click 事件的“οnclick”的 ο 的 ascii码值为 959 ，看起来与普通的字符 o 是一样的，但是浏览器只能识别 111，导致程序错误。 将 ο 改成 o 问题就解决了。 ascii码值为 959的 ο 是什么呢？是希腊小写字母 omicron。","link":"/2021/06/04/onclick%E6%97%A0%E6%95%88%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8E%9F%E5%9B%A0/"},{"title":"std::numeric_limits&lt;int&gt;::max() error C2589: &#39;(&#39; : illegal token on right side of &#39;::&#39; 处理","text":"const unsigned int maxUnitSize = unitCodes.count(maxUnit) &gt; 0 ? unitCodes.at(maxUnit) : std::numeric_limits::max(); 编译的时候报错： 123warning C4003: not enough actual parameters for macro 'max'error C2589: '(' : illegal token on right side of '::'error C2059: syntax error : '::'· 原因是STL的numeric_limits::max()和VC min/max 宏冲突问题。 解决方法是通过括号“（）”来避免预编译器报错。修改为: const unsigned int maxUnitSize = unitCodes.count(maxUnit) &gt; 0 ? unitCodes.at(maxUnit) : (std::numeric_limits::max)();","link":"/2022/10/21/std-numeric-limits-int-max-error-C2589-illegal-token-on-right-side-of-%E5%A4%84%E7%90%86/"},{"title":"win10编译ffmpeg7","text":"win10环境下编译ffmpeg需要msys2环境，建议在msys2的环境中采用MSVC进行编译，可直接编译出.lib文件和.dll动态库。预先安装msys2和visual studio环境。 再安装ffmpeg7相关的库，步骤如下： 安装和编译必要的依赖项以及x264、x265和sdl 1pacman -S diffutils make pkg-config yasm note: 列出孤立的包（-t不再被依赖的&quot;作为依赖项安装的包&quot;） 1pacman -Qqdt 注意：这些通常是可以删除的。(sudo pacman -Qqdt | sudo pacman -Rs -) 先从开始菜单找到Visual Studio Command line，选择“x64 Native Tools Command Prompt for VS”，然后在命令行执行： 1call msys2_shell.cmd -mintty -mingw64 -no-start -here -use-full-path note: 执行msys2目录中的msys2_shell.cmd批处理，注意加上 -use-full-path 使 msys2 的环境变量继承当前CMD的窗口的环境变量。 然后在打开的mintty中编译ffmpeg。 编译 x264 编译当前最新x264时需要用到nasm 12345pacman -S nasmgit clone --depth=1 https://code.videolan.org/videolan/x264.git./configure --prefix=/d/ops/ffmpeg --enable-sharedmake -j 8 &amp;&amp; make installmv /d/ops/ffmpeg/lib/libx264.dll.lib /d/ops/ffmpeg/lib/libx264.lib 编译 x265 下载源码 12git clone --depth=1 https://bitbucket.org/multicoreware/x265_git.gitcd x265_git/build/msys-cl 进入 x265_git/build/msys-cl 目录，修改make-Makefiles.sh为： 1234567891011121314151617INSTALL_DIR=&quot;/d/ops/ffmpeg&quot;if cl; then echo else echo &quot;please launch msys from 'visual studio command prompt'&quot; exit 1ficmake -G &quot;NMake Makefiles&quot; -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR -DCMAKE_CXX_FLAGS=&quot;-DWIN32 -D_WINDOWS -W4 -GR -EHsc&quot; -DCMAKE_C_FLAGS=&quot;-DWIN32 -D_WINDOWS -W4&quot; ../../sourceif [ -e Makefile ]then nmakefinmake install 添加了如下三行： 123INSTALL_DIR=&quot;/d/ops/ffmpeg&quot;-DCMAKE_INSTALL_PREFIX=$INSTALL_DIRnmake install 运行make-Makefiles.sh进行编译和安装 1./make-Makefiles.sh 将 /d/ffmpeg/lib/libx265.lib， 改名位x265.lib。编译ffmpeg时查找的库名为x265.lib。 安装 sdl 因为要生产 ffplay，而 ffplay 需要 sdl，所以要下载 sdl2 并安装，地址: https://github.com/libsdl-org/SDL/releases/latest 下载 VC 版，解压。在 lib 目录里面添加 pkgconfig 文件夹，在里面创建 sdl2.pc 文件，内容如下: 123456789101112prefix=/d/ops/sdl2exec_prefix=${prefix}libdir=${prefix}/lib/x64includedir=${prefix}/includeName: sdl2Description: Simple DirectMedia Layer is a cross-platform multimedia library designed to provide low level access to audio, keyboard, mouse, joystick, 3D hardware via OpenGL, and 2D video framebuffer.Version: 2.30.8Requires:Conflicts:Libs: -L${libdir} -lSDL2main -lSDL2Cflags: -I${includedir} 配置 ffmpeg 进入 ffmpeg 目录将如下代码保存为.sh文件并执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748X264_INSTALL=&quot;/d/ops/ffmpeg&quot;X265_INSTALL=&quot;/d/ops/ffmpeg&quot;SDL_INSTALL=&quot;/d/ops/sdl2&quot;X264_INCLUDE=$X264_INSTALL/includeX264_LIB=$X264_INSTALL/libX264_BIN=$X264_INSTALL/binX265_INCLUDE=$X265_INSTALL/includeX265_LIB=$X265_INSTALL/libX265_BIN=$X265_INSTALL/binSDL_INCLUDE=$SDL_INSTALL/includeSDL_LIB=$SDL_INSTALL/lib/x64SDL_BIN=$SDL_INSTALL/lib/x64export PATH=$X264_BIN:$PATHexport PATH=$X265_BIN:$PATHexport PATH=$SDL_BIN:$PATH## export INCLUDE=$INCLUDE:$SDL_INCLUDE## export LIB=$LIB:$SDL_LIBexport PKG_CONFIG_PATH=$X264_LIB/pkgconfig:$PKG_CONFIG_PATHexport PKG_CONFIG_PATH=$X265_LIB/pkgconfig:$PKG_CONFIG_PATHexport PKG_CONFIG_PATH=$SDL_INSTALL/lib/pkgconfig:$PKG_CONFIG_PATHecho $PKG_CONFIG_PATHecho $LIBINSTALL_DIR=&quot;/d/ops/ffmpeg&quot;OPTIONS=&quot;--toolchain=msvc --enable-yasm --enable-asm --enable-shared --disable-programs --enable-swresample --enable-swscale --enable-gpl --enable-libx264 --enable-libx265 --enable-ffmpeg --enable-sdl2 --enable-ffplay &quot;CC=cl ./configure $OPTIONS --prefix=$INSTALL_DIR/ note: 不要export LIB，否则会链接VC的lib出错。 12make -j 8 &amp;&amp; make installmake clean 如果编译出错，报fftools/opt_common.c(206): warning C4129: “l：: 不可识别的字符转义序列，修改config.h文件编码为utf-8，继续编译。 或者把 config.h 中的中文改成英文。","link":"/2024/10/28/win10%E7%BC%96%E8%AF%91ffmpeg7/"},{"title":"pacman常用命令备忘","text":"Pacman 是 Arch Linux 的包管理器。msys2 也采用 Pacman做包管理。 修改镜像 修改 etc\\pacman.d\\mirrorlist.mingw32 等文件，将 12Server = http://mirrors.ustc.edu.cn/msys2/mingw/x86_64/Server = https://mirrors.tuna.tsinghua.edu.cn/msys2/mingw/x86_64/ 移到 Server = https://repo.msys2.org/mingw/x86_64/ 之前。 更新系统 可使用如下命令更新: 1pacman -Syu 如果你已经使用 pacman -Sy 将本地的包数据库与远程的仓库进行了同步，也可以只执行：pacman -Su。 在 pacman 5.0.1.6403 以后的版本执行： 1pacman -Syuu 更新已安装包： 1pacman -Suu 安装包 pacman -S + 包名：例如，执行 pacman -S firefox 将安装 Firefox。你也可以同时安装多个包， 只需以空格分隔包名即可。 pacman -Sy + 包名：与上面命令不同的是，该命令将在同步包数据库后再执行安装。 pacman -Sv + 包名：在显示一些操作信息后执行安装。 pacman -U + 本地包名，其扩展名为 pkg.tar.gz。 pacman -U + http://www.example.com/repo/example.pkg.tar.xz，安装一个远程包（不在 pacman 配置的源里面） 删除包 pacman -R + 包名：该命令将只删除包，保留其全部已经安装的依赖关系 pacman -Rs + 包名：在删除包的同时，删除其所有没有被其他已安装软件包使用的依赖关系 pacman -Rsc + 包名：在删除包的同时，删除所有依赖这个软件包的程序 pacman -Rd + 包名：在删除包时不检查依赖。 搜索包 pacman -Ss + 关键字：在仓库中搜索含关键字的包。 pacman -Qs + 关键字： 搜索已安装的包。 pacman -Qi + 包名：查看有关包的详尽信息。 pacman -Ql + 包名：列出该包的文件。 其它 pacman -Sw + 包名：只下载包，不安装。 pacman -Sc 清理未安装的包文件，包文件位于 /var/cache/pacman/pkg/ 目录。 pacman -Scc 清理所有的缓存文件。 pacman -Qqdt 列出可以清理的包。","link":"/2021/05/25/pacman%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98/"},{"title":"visual studio C++工程.vs文件夹过大","text":"visual studio 的 c++ 工程因为 IntelliSense 会在 .vs 文件夹里面生产磁盘缓存，缓存文件有时候会变得很大，导致 .vs 文件夹非常巨大。解决方法如下： 修改缓存文件位置 Tools | Options | Text Editor | C/C++ | Advanced 标签页，找到 Browsing Database Fallback，将 的 Always Use Fallback Location 和 Do Not Warn If Fallback Location Used 设为 true。然后设置 Fallback Location 缓存文件的路径。 设置 IntelliSense 的自动预编译标头为禁用 Tools | Options | Text Editor | C/C++ | Advanced 标签页，找到 IntelliSense，设置 Disable Automatic Precompiled Header 为 true。这样会减小 IntelliSense 的操作速度，但可以减小缓存文件的大小。","link":"/2023/03/14/visual-studio-C++%E5%B7%A5%E7%A8%8B.vs%E6%96%87%E4%BB%B6%E5%A4%B9%E8%BF%87%E5%A4%A7/"},{"title":"使用Java8的Optional的思考","text":"Optional 是 Java 8 引入的一个工具类，也是 Java 8 的新特性之一。在 Stream API 中很多地方也都使用到了 Optional。 空指针异常（NullPointerExceptions）是 Java 最常见的异常之一。程序员不得不在代码中写很多 null 的检查逻辑，让代码看起来非常臃肿；由于其属于运行时异常，非常难以预判的。 为了预防空指针异常，Google 的 Guava 项目率先引入 Optional 类，通过使用检查空值的方式来防止代码污染，受到 Guava 项目的启发，Java 8 中引入了 Optional 类。那在使用 Optional 上有那些值得思考和注意的地方呢。 isPresent() 和 get() 的思考 虽然官方文档说明中对 Optional 的描述是：If a value is present, isPresent() will return true and get() will return the value。我们就调用 isPresent() 和 get() 来避免 NullPointException 好了，但如果只是简单的认为它可以解决 NPE 的问题, 于是代码就会是这样： 123456Optional&lt;Student&gt; student = ......if (student.isPresent()) { return student.get().getName();} else { return &quot;&quot;;} 那么这种写法这与我们之前写成的： 1234567Student student = .....if (student != null) { return student.getName();} else { return &quot;&quot;;} 本质上是没有区别的。 虽然说使用 ifPresent(Consumer&lt;? super T&gt; consumer) 来替代 isPresent() 要好一些，但是 Consumer 的 accept 函数的返回值是 void 类型，接受单个输入参数且不返回结果的操作，有使用上的限制。 所以，如果我们在使用 Optional 时，如果有需要调用 isPresent() 和 get() 的地方，那就该重新审视一下，是否真的有必要使用 Optional。 不要将 Optional 作为函数参数 把 Optional 类型用作函数参数在 IntelliJ IDEA 中是强力不推荐的。这该怎么理解呢？Optional 对象是一个容器对象，它包含的对象是否是空，是不确定的。 函数作为被调用者，它根据传入的参数进行逻辑运算，那么传给它的参数应该是明确的。 参数不为 Optional 的例子： 1234567891011public void func1() { func2(student);}public int func2(Student student) { if (student == null) { return 0; } student.setId(getId()); return insertStudent(name);} 参数为 Optional 的例子： 123456789public void func1() { func2(Optional.ofNullable(student));}public int func2(Optional&lt;Student&gt; studentOpt) { Student student = studentOpt.orElse(new Student()); student.setId(getId()); return insertStudent(student);} 如上两个例子，如果 Optional 为参数的话，按照后一个例子的写法，func2 的逻辑就改变了。 如果要逻辑一致的话，就又会使调用到 isPresent() 和 get() 方法，这样就不如不使用 Optional 更为直接。 不要将 Optional 作为字段类型 不要将 Optional 作为字段类型有两个原因： Optional 包含的对象是不确定的 不确定的数据作为字段值没有什么意义，应该审视是否有更好的设计。 Optional 不能支持序列化 Optional 作为字段类型,如果对象需要被序列化，将会出现异常Exception in thread &quot;main&quot; java.io.NotSerializableException。 Optional 使用举例 除了上述几个场景外，在需要处理 null 的地方，采用 Optional 还是比较好的选择。Optional 虽不能完全杜绝 NPE，但是它能相对优雅的预防 NPE。 比如我们可以这样做： 12Optional&lt;Student&gt; studentOpt = Optional.ofNullable(student);String name = studentOpt.orElse(&quot;&quot;); Optional.ofNullable(obj)：以一种宽容的方式来构造一个 Optional 实例。传 null 进到就得到 Optional.empty(), 非 null 就调用 Optional.of(obj)。Optional.of(obj) 是可能抛出 NPE的，如果参数 obj 为 null 的话。 Optional.orElse()：存在即返回, 无则提供默认值。 这样看起来比之前的写法要简单很多，改写成一行，更简洁： 1String name = Optional.ofNullable(student).orElse(&quot;&quot;); 比如我们还可以这样，将 Steam 与 Optional 结合起来使用： 12345678public List&lt;User&gt; getUsers(Collection&lt;Integer&gt; studentIds) { return studentIds.stream() //此处 getStudentById 返回的是 Optional&lt;Student&gt; .map(this::getStudentById) // 获得 Stream&lt;Optional&lt;Student&gt;&gt; .filter(Optional::isPresent) // 去掉不包含值的 Optional .map(Optional::get) .collect(Collectors.toList());} Java 8 的 Optional 除了上述的一些方法，还有一些其它的，像 flatMap 和 orElseThrow 等，可以详细了解一下，找到合适的使用场景。","link":"/2020/08/17/%E4%BD%BF%E7%94%A8Java8%E7%9A%84Optional%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"回顾Java类加载器(续)","text":"回顾 Java 类加载器(续) 类加载器负责读取 Java 字节代码，并转换成 java.lang.Class 类的一个实例。Class 对象是在加载类时由 Java 虚拟机以及通过调用类加载器中的 defineClass 方法自动构造的，因此不能显式地声明一个Class对象。 显式加载类 在代码中显式加载某个类，有三种方法： this.getClass().getClassLoader().loadClass() Class.forName() CustomClassLoader.findClass() loadClass会调用findClass。 loadClass() ClassLoader.loadClass()的加载步骤为： 调用 findLoadedClass(String) 来检查是否已经加载类。 在父类加载器上调用 loadClass 方法。如果父类加载器为 null，则使用虚拟机的内置类加载器。 调用 findClass(String) 方法查找类。 12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException{ synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; }} ClassLoader.findClass() ClassLoader.loadClass()的最后一步是调用findClass()，这个方法在ClassLoader中并未实现，由其子类负责实现。findClass()的功能是找到class文件并把字节码加载到内存中。自定义的ClassLoader一般会覆盖这个方法，以便使用不同的加载路径。 URLClassLoader 的findClass 代码： 1234567891011121314151617181920212223242526272829protected Class&lt;?&gt; findClass(final String name) throws ClassNotFoundException{ final Class&lt;?&gt; result; try { result = AccessController.doPrivileged( new PrivilegedExceptionAction&lt;Class&lt;?&gt;&gt;() { public Class&lt;?&gt; run() throws ClassNotFoundException { String path = name.replace('.', '/').concat(&quot;.class&quot;); Resource res = ucp.getResource(path, false); if (res != null) { try { return defineClass(name, res); } catch (IOException e) { throw new ClassNotFoundException(name, e); } } else { return null; } } }, acc); } catch (java.security.PrivilegedActionException pae) { throw (ClassNotFoundException) pae.getException(); } if (result == null) { throw new ClassNotFoundException(name); } return result;} ucp 是 URLClassPath 对象，帮助获取 class 文件字节流，URLClassPath 会用 FileLoader 或者 JarLoader 去加载字节码。 defineClass()，创建类对象，将类字节码解析成 JVM 能够识别的 Class 对象。 123456789101112131415161718192021222324252627private Class&lt;?&gt; defineClass(String name, Resource res) throws IOException { long t0 = System.nanoTime(); int i = name.lastIndexOf('.'); URL url = res.getCodeSourceURL(); if (i != -1) { String pkgname = name.substring(0, i); // Check if package already loaded. Manifest man = res.getManifest(); definePackageInternal(pkgname, man, url); } // Now read the class bytes and define the class java.nio.ByteBuffer bb = res.getByteBuffer(); if (bb != null) { // Use (direct) ByteBuffer: CodeSigner[] signers = res.getCodeSigners(); CodeSource cs = new CodeSource(url, signers); sun.misc.PerfCounter.getReadClassBytesTime().addElapsedTimeFrom(t0); return defineClass(name, bb, cs); } else { byte[] b = res.getBytes(); // must read certificates AFTER reading bytes. CodeSigner[] signers = res.getCodeSigners(); CodeSource cs = new CodeSource(url, signers); sun.misc.PerfCounter.getReadClassBytesTime().addElapsedTimeFrom(t0); return defineClass(name, b, 0, b.length, cs); }} Class.forName Class的装载分了三个阶段，loading，linking和initializing，分别定义在The Java Language Specification的12.2，12.3和12.4。 Class.forName(className)实际上是调用Class.forName(className, true, ClassLoader.getClassLoader(caller), caller)。注意第二个参数，是指Class被loading后是不是必须被初始化。 12345public static Class&lt;?&gt; forName(String className) throws ClassNotFoundException { Class&lt;?&gt; caller = Reflection.getCallerClass(); return forName0(className, true, ClassLoader.getClassLoader(caller), caller);} ClassLoader.loadClass(className)实际上调用的是ClassLoader.loadClass(name, false)，第二个参数指出Class是否被link。 123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException { return loadClass(name, false);} 区别就出来了。Class.forName(className)装载的class已经被初始化，会执行类中的 static 块和 static 方法，而ClassLoader.loadClass(className)装载的class还没有被link。 一般情况下，这两个方法效果一样，都能装载Class。但如果程序依赖于Class是否被初始化，就必须用Class.forName(name)了。","link":"/2018/04/12/%E5%9B%9E%E9%A1%BEJava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8-%E7%BB%AD/"},{"title":"使用git-svn管理svn代码仓库","text":"使用git-svn管理svn代码仓库 现在大部分代码仓库是采用git管理，但由于历史原因，有些代码仓库继续由svn管理。git提供了git-svn工具，可使用git管理svn代码。最后的效果相当于把svn仓库当作git的一个remote（远程仓库），而你本地的代码都是通过git来管理，只有push到svn时才会把你本地的commit同步到svn。 克隆 1234567git svn init http://ip/svn/demo/trunk demo -s --prefix=svn/ --username usergit svn fetch -r HEADgit svn fetch -r2:HEAD或者git svn clone http://ip/svn/demo/trunk -s --prefix=svn/ --username user clone执行Runs init and fetch -s 告诉 Git 该 Subversion 仓库遵循了基本的分支和标签命名法则，也就是标准布局。如果你的主干(trunk，相当于非分布式版本控制里的master分支，代表开发的主线），分支(branches)或者标签(tags)以不同的方式命名，则应做出相应改变。 -s参数其实是-T trunk -b branches -t tags的缩写，这些参数告诉git这些文件夹与git分支、tag、master的对应关系。 –prefix=svn/ 给svn的所有remote名称增加了一个前缀svn，这样比较统一，而且可以防止warning: refname ‘xxx’ is ambiguous. 只下载指定版本之后的历史 1234git svn clone -r&lt;开始版本号&gt;:&lt;结束版本号&gt; &lt;svn项目地址&gt; [其他参数]例：git svn clone -r2:HEAD file:///d/Projects/svn_repo proj1_git -s 其中2为svn版本号，HEAD代表最新版本号，就是只下载svn服务器上版本2到最新的版本的代码。 一般情况下git svn clone会从第一个版本开始同步，如果版本号已经到了好几万（或更高？），这个操作会相当的费时。这时可以使用参数-r$REVNUMBER:HEAD检出指定版本后的代码。 svn info svn_repository_url, 记录最后的版本号，假设是260 假设要检出最后5个版本，做个简单的减法： 260 – 5 = 255 开始clone操作了： git-svn clone -r255:HEAD svn_repository_url 更新 1git svn rebase 从svn上更新代码, 相当于svn的update 提交 1git svn dcommit 提交commit到svn远程仓库，建议提交前都先运行下git svn rebase","link":"/2019/07/01/%E4%BD%BF%E7%94%A8git-svn%E7%AE%A1%E7%90%86svn%E4%BB%A3%E7%A0%81%E4%BB%93%E5%BA%93/"},{"title":"微服务实践中的一些思考","text":"微服务实践中的一些思考 微服务的目的 微服务架构由SOA架构演变而来，SOA是怎样的一种架构，这里不再重复。简单的说微服务与SOA的区别，即微服务不再强调SOA架构里面比较重的ESB工业服务总线，同时强调单个业务系统内部实现真正的组件化。ESB的中心化带来了单点故障隐患；服务统一在ESB上进行部署，也限制了服务的水平扩展，不利于提升性能；此外ESB还包含很多业务相关的功能，如业务流程编排等，限制了业务扩展的灵活性。 业务的复杂度会随着人们日益增长需求而增长，但每个人能够认知的复杂度都是有限的，在面对高复杂度的时候我们会做关注点分离，这是一个最基本的哲学原则。显然在针对复杂业务场景进行建模时，我们也会应用此原则。这个时候去分离关注点一般可以从两个维度出发： 技术维度分离，采用类似MVC这样的分层思想进行层次的划分。 业务维度分离，根据不同的业态划分系统，比如按售前、销售、售后划分。 以上两个维度没有孰优孰劣之分，在处理复杂问题的时候一定都会用上，SOA时代的ESB（工业服务总线）就是一个典型的技术关注点分离出来的中间件。ESB里面封装了大量的业务规则和流程，让ESB成为了掉入焦油坑的怪兽，成为了各种问题之源。微服务架构某种程度上解决了这些问题（但同时也带来了新的问题）。 微服务是为了着追求高响应力目标而从业务视角去分离复杂度的手段之一。 为了能够高效响应业务的变化，微服务的架构更强调业务维度的关注点分离，来应对高复杂度。 微服务的目的是有效的拆分应用，实现敏捷开发和部署，可以使用不同的编程语言编写的。 微服务对应用的拆分采用术语来说，就是业务维度的分离。比如根据商品、订单拆分和部署，甚至数据库也按照这个维度进行拆分，不同的服务不同的库。 微服务是去ESB、去中心化、分布式的。 微服务是自洽的。 自洽实际上意味着我们的每一个微服务都是一个独立的个体，就像峰群中的蜜蜂一样，他们能单独维护，单独迭代，单独部署，单独测试，单独发布。 微服务的实践 去中心 国内进行微服务架构一般会采用spring cloud或者dubbo。这两种孰优孰劣不讨论，在功能上它们有一些共性：服务治理和服务调用。为什么服务治理和服务调用成为微服务架构框架的共性呢，原因是微服务的去ESB、去中心、分布式。没有服务治理和调用，访问者不知道服务在哪儿，怎样调用。但这样一来，还是会多出一个服务发现的服务器，如spring采用的是eureka，dubbo采用的是zookeeper。 由于拆分了很多的服务，服务的配置成了问题，这又多出了提供分布式配置的服务器。 虽然多了服务发现和服务配置，但是这两个服务没有与业务耦合，从业务的角度确实分离了复杂度。 业务分离 业务分离大概是微服务中最难的问题了，业务该怎样分，拆分到哪种力度才算合适。既要考虑产品的需求、产品的升级换代，还要考虑团队的技术能力、开发周期，此外还要考虑运维部署等问题。好的架构是进化而来的，业务分离作为架构的一部分，它也应该有个进化和演进的过程，前期可以相对力度较粗，后期再根据情况适当细化。 数据一致性 分布式是微服务的特性之一，但分布式在提升系统响应力的同时，带来了数据一致性的问题，而且这个问题很不好解决。像分布式事务、分布式锁都有它们各自的问题，都解决的不彻底。采用最终一致性应该是目前微服务解决数据一致性较好的方案。但实现最终一致性有时会影响用户体验，比如用户取消订单，可能涉及到退还用户优惠券等，采用最终一致性的话，可能出现订单已经取消但优惠券还买有退还到账，用户立即继续购买却没有优惠券可用的情况。 实现最终一致性一般会用到MQ或者kafka等分布式发布订阅消息系统，它们用来解耦服务之间的调用（主要是用于通知）。比如上面的用户取消订单，订单服务需要通知优惠券服务有优惠券需要退还了。由于消息系统一般是异步的，所以出现了上面体验的问题。当然也可以改成同步，但这样就失去了服务拆分的意义了。 自洽性 自洽性要求每个微服务都是独立的个体，相互之间没有没有依赖。但是在实践过程中，很难做到这点。比如在电商微服务中要推送消息，消息的来源有些是订单服务，有些是商品服务，接收方是用户，这样就出现了订单服务可能需要访问用户服务的情况，服务之间形成了依赖。解决的方法通过MQ或者Kafka等消息中间件进行解耦。但是这样一来似乎又多了一个中心，和“去中心”就矛盾了。 微服务的问题 在微服务的实践过程中，如上文所述已经暴露出了一些问题。此外，为了支撑微服务，还增加很多的新的模块或服务，例如：服务发现、服务配置、熔断、日志跟踪、API网关等。增加的服务也就成为了微服务的问题。不过，它们只是在技术的维度增加了复杂度，在业务的维度是降低了复杂度。角度不同，结果也就不同。 上面提及的问题虽然也提出了方案可以解决，但是并觉得不够优雅，这是否与微服务无关，与采用的框架有关？像spring，把“行为”（也称为逻辑、过程）和“状态”（可理解为数据）分离到不同的对象之中，只有状态的对象就是所谓的“贫血对象”（常称为VO——Value Object），而那个只有行为的对象就是我们常用的分层架构中的Logic/Service层。这种开发方式称之为“贫血模型”。当然spring也不是固定就只能使用“贫血模型”做开发，但一般情况spring就是“贫血模型”。 “贫血模型”不符合面对对象设计的本质：“对象应该拥有状态和行为”。比如汽车，汽车有车厢和轮子这是它的状态，汽车可以开动，可以载人这是它的行为。与“贫血模型”对应的是“充血模型”举个简单的例子，设计一个与汽车（Car）相关的功能： 贫血的设计一般是：类Car+类CarService 驾驶：CarService.driving(Car car); 充血的设计则可能会是：类Car 驾驶：car.driving(); Car有一个行为是：驾驶 “充血模型”符合面向对象的设计，采用这种模型应该能够让技术更好的根据业务来建模，从而技术架构能更好的响应业务架构，使技术架构能够根据业务的变化而自发的改变。","link":"/2018/03/02/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"title":"回顾Java类加载器","text":"回顾 Java 类加载器 类加载器使得 Java 类可以被动态加载到 Java 虚拟机中并执行。类加载器从 JDK 1.0 就出现了，最初是为了满足 Java Applet 的需要而开发出来的。Java Applet 需要从远程下载 Java 类文件到浏览器中并执行。现在类加载器在 Web 容器和 OSGi 中得到了广泛的使用。一般来说，Java 应用的开发人员不需要直接同类加载器进行交互。Java 虚拟机默认的行为就已经足够满足大多数情况的需求了。不过如果遇到了需要与类加载器进行交互的情况，而对类加载器的机制又不是很了解的话，就很容易花大量的时间去调试 ClassNotFoundException和 NoClassDefFoundError等异常。 基本概念 类加载器（class loader）用来加载 Java 类到 Java 虚拟机中。一般来说，Java 虚拟机使用 Java 类的方式如下：Java 源程序（.java 文件）在经过 Java 编译器编译之后就被转换成 Java 字节代码（.class 文件）。类加载器负责读取 Java 字节代码，并转换成 java.lang.Class 类的一个实例。每个这样的实例用来表示一个 Java 类。通过此实例的 newInstance()方法就可以创建出该类的一个对象。实际的情况可能更加复杂，比如 Java 字节代码可能是通过工具动态生成的，也可能是通过网络下载的。 基本上所有的类加载器都是 java.lang.ClassLoader 类的一个实例。 ClassLoader 加载类的原理 类加载器的组织结构 Java 中的类加载器大致可以分成两类，一类是系统提供的，另外一类则是由 Java 应用开发人员编写的。系统提供的类加载器主要有下面三个： 引导类加载器（bootstrap class loader）：它用来加载 Java 的核心库，是用原生代码来实现的，并不继承自 java.lang.ClassLoader。 负责加载 JDK 中的核心类库，如：rt.jar、resources.jar、charsets.jar等。 扩展类加载器（extensions class loader）：它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。 负责加载 Java 的扩展类库，默认加载 JAVA_HOME/jre/lib/ext 目下的所有 jar。 系统类加载器（system class loader \\ app class loader）：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。 除了系统提供的类加载器以外，可以通过继承 java.lang.ClassLoader 类的方式实现自己的类加载器，以满足一些特殊的需求。 类加载器树状组织结构示意图: 原理 ClassLoader 使用的是双亲委托模型来搜索类的，每个 ClassLoader 实例都有一个父类加载器的引用（不是继承的关系，是一个包含的关系），虚拟机内置的类加载器（Bootstrap ClassLoader）本身没有父类加载器，但可以用作其它 ClassLoader 实例的的父类加载器。当一个 ClassLoader 实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器 Bootstrap ClassLoader 试图加载，如果没加载到，则把任务转交给 Extension ClassLoader 试图加载，如果也没加载到，则转交给 App ClassLoader 进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等 URL 中加载该类。如果它们都没有加载到这个类时，则抛出 ClassNotFoundException 异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的 Class 实例对象。 如何判定两个 Java 类是相同的 Java 虚拟机不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即便是同样的字节代码，被不同的类加载器加载之后所得到的类，也是不同的。比如一个 Java 类 com.example.Sample，编译之后生成了字节代码文件 Sample.class。两个不同的类加载器 ClassLoaderA 和 ClassLoaderB 分别读取了这个 Sample.class文件，并定义出两个 java.lang.Class 类的实例来表示这个类。这两个实例是不相同的。对于 Java 虚拟机来说，它们是不同的类。 为什么是双亲委托 双亲委托也可称为代理模式，理解了上诉 Java 虚拟机判断 Java 类是否相同的原理就可明白，采用这种模式的原因是为了保证 Java 核心库的类型安全。所有 Java 应用都至少需要引用 java.lang.Object 类，也就是说在运行的时候，java.lang.Object 这个类需要被加载到 Java 虚拟机中。如果这个加载过程由 Java 应用自己的类加载器来完成的话，很可能就存在多个版本的 java.lang.Object 类，而且这些类之间是不兼容的。通过代理模式，对于 Java 核心库的类的加载工作由引导类加载器来统一完成，保证了 Java 应用所使用的都是同一个版本的 Java 核心库的类，是互相兼容的。 不同的类加载器为相同名称的类创建了额外的名称空间。相同名称的类可以并存在 Java 虚拟机中，只需要用不同的类加载器来加载它们即可。不同类加载器加载的类之间是不兼容的，这就相当于在 Java 虚拟机内部创建了一个个相互隔离的 Java 类空间。这种技术在许多框架中都被用到，如 tomcat。 参考 深入探讨 Java 类加载器","link":"/2018/03/30/%E5%9B%9E%E9%A1%BEJava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/"},{"title":"源代码管理规范","text":"源代码管理规范 代码完整性 所有软件的源代码文件及相应的开发设计文档均必须及时加入到源代码服务器中指定的代码库。 支撑系统正常运行的第三方软件、控件及其他支撑库源代码及相关文档必须及时加入到源代码服务器中指定的代码库中。 源代码必须进行现在备份操作，并由专人进行保管，定期进行恢复测试，以保障源代码的安全性和完整性。 代码操作 在签入源代码时，必须添加签入注释，方便其他技术人员知道更改内容。 在签入源代码时，若出现代码冲突，必须及时解决代码冲突，防止源代码被该更或覆盖造成软件异常。 在编辑源代码时，必须对源代码进行签处更新，保证代码处于最新版本状体或是指定版本状态。 在编辑源代码时，应加入相关注释，方便其他技术人员理解内容。 在合并源代码时，若出现代码冲突，应谨慎选择解决冲突的版本，避免造成源代码被覆盖或修改内容丢失。 在更改代码内容后，不确定修改的代码能否进行签入操作或是不能判断代码内容是否正确时，需选择搁置挂起更改，请求团队其他成员确认修改的是否允许签入或是代码是否正确。 代码发布 代码发布应该从服务器上取对应版本的源码进行编译、打包、发布。拒绝从开发人员的电脑上打包发布。 每次发布版本，需要做相应的发布版本记录。 在版本发布后，修改BUG，应将源代码进行分支，在分支上进行BUG修复操作，BUG修复成功后，应将分支的代码合并到源代码中。","link":"/2018/07/09/%E6%BA%90%E4%BB%A3%E7%A0%81%E7%AE%A1%E7%90%86%E8%A7%84%E8%8C%83/"},{"title":"版本号匹配一正则","text":"Java 利用正则表达式校验版本号格式是否正确。要求为3段式，格式为：x.x.x。第一段为1-99，第二段为0-999，第三段为0-999。 表达式 1^[1-9][0-9]{0,2}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}$ 测试 1234567891011121314151617public static boolean isMatched(final String version) { String regex = &quot;^[1-9][0-9]{0,2}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}$&quot;; return Pattern.matches(regex,version);}public static void main(String[] args) { String v1 = &quot;100.0.0&quot;; System.out.println(&quot;version:&quot;+ v1 + &quot; isMatched: &quot; + isMatched(v1)); String v2 = &quot;010.0.0&quot;; System.out.println(&quot;version:&quot;+ v2 + &quot; isMatched: &quot; + isMatched(v2)); String v3 = &quot;10.0000.0&quot;; System.out.println(&quot;version:&quot;+ v3 + &quot; isMatched: &quot; + isMatched(v3)); String v4 = &quot;10.0.0000&quot;; System.out.println(&quot;version:&quot;+ v4 + &quot; isMatched: &quot; + isMatched(v4)); String v5 = &quot;10.0.000&quot;; System.out.println(&quot;version:&quot;+ v5 + &quot; isMatched: &quot; + isMatched(v5));} 输出： 12345version:100.0.0 isMatched: trueversion:010.0.0 isMatched: falseversion:10.0000.0 isMatched: falseversion:10.0.0000 isMatched: falseversion:10.0.000 isMatched: true","link":"/2022/11/04/%E7%89%88%E6%9C%AC%E5%8F%B7%E5%8C%B9%E9%85%8D%E4%B8%80%E6%AD%A3%E5%88%99/"},{"title":"简介Java8的流","text":"流是Java API的新成员，它允许以声明性方式处理数据集合（通过查询语句来表达，而不是临时编写一个实现）。 流还可以透明地并行处理，无需编写任何多线程代码了。 使用流的例子 下面两段代码都是用来返回成绩好的学生的姓名，并按照得分排序，一个是用Java 7写的，另一个是用Java 8的流写的。 Java7 123456789101112131415161718public void studentNames() { List&lt;Student&gt; goodStudents = new ArrayList&lt;&gt;(); for (Student d : students) { if (d.getScore() &gt;= GOOD) { goodStudents.add(d); } } goodStudents.sort(new Comparator&lt;Student&gt;() { public int compare(Student d1, Student d2) { return Integer.compare(d1.getScore(), d2.getScore()); } }); List&lt;String&gt; goodStudentNames = new ArrayList&lt;&gt;(); for (Student d : goodStudents) { goodStudentNames.add(d.getName()); } System.out.println(goodStudentNames);} Java8采用流的代码： 123456789public void studentNamesJ8() { List&lt;String&gt; goodStudentNames = students.stream() .filter(s -&gt; s.getScore() &gt;= GOOD) .sorted(comparing(Student::getScore)) .map(Student::getName) .collect(Collectors.toList()); System.out.println(goodStudentNames);} 为了利用多核架构并行执行这段代码，只需要把stream()换成parallelStream(): 123456789public void studentNamesJ8() { List&lt;String&gt; goodStudentNames = students.parallelStream() .filter(s -&gt; s.getScore() &gt;= GOOD) .sorted(comparing(Student::getScore)) .map(Student::getName) .collect(Collectors.toList()); System.out.println(goodStudentNames);} 采用流方法的好处: 声明性方式 说明想要完成什么（筛选成绩好的学生）而不是说明如何实现（循环和if条件等控制流语句）。 代码清晰易读 把几个基础操作链接起来，来表达复杂的数据处理流水线。filter 的结果被传给了 sorted 方法，再传给 map 方法，最后传给 collect 方法。 可轻松利用多核 只需把stream()换成parallelStream()。 流是什么 Java 8中的集合支持一个新的 stream 方法，它会返回一个流（接口定义在java.util.stream.Stream里）。还有很多其他的方法可以得到流， 比如利用数值范围或从I/O资源生成流元素。流的简短定义就是“从支持数据处理操作的源生成的元素序列”。 进一步说明这个定义： 元素序列 就像集合一样，流也提供了一个接口，可以访问特定元素类型的一组有序值。但流的目的在于表达计算，比如前面见到的 filter、sorted 和 map。 与集合不同地方是集合讲的是数据，流讲的是计算。 源 流会使用一个提供数据的源，如集合、数组或输入/输出资源。 数据处理操作 流的数据处理功能支持类似于数据库的操作，以及函数式编程语言中的常用操作，如filter、map、reduce、find、match、sort等。流操作可以 顺序执行，也可并行执行。 此外，流还有两个重要特点： 流水线 很多流操作本身会返回一个流，这样多个操作就可以链接起来，形成一个大的流水线。 内部迭代 流的迭代操作是在背后进行的。 一段体现这些概念的代码示例： 12345678910public void studentNamesLimit() { List&lt;String&gt; goodStudentNames = students.stream() .filter(s -&gt; s.getScore() &gt;= GOOD) .sorted(comparing(Student::getScore)) .map(Student::getName) .limit(3) .collect(Collectors.toList()); System.out.println(goodStudentNames);} 在本例中，我们先是对 students 调用 stream 方法，由学生列表得到一个流。数据源是学生列表，它给流提供一个元素序列。 接下来，对流应用一系列数据处理操作：filter、sorted、map、limit 和 collect。 除了 collect 之外，所有这些操作都会返回另一个流，这样它们就可以接成一条流水线，于是就可以看作对源的一个查询。 最后，collect 操作开始处理流水线，并返回结果（它和别的操作不一样，因为它返回的不是流，在这里是一个 List ）。 在调用 collect 之前，没有任何结果产生，实际上根本就没有从 students 里选择元素。你可以这么理解：链中的方法调用都在排队等待，直到调用collect。 在这里我们并没有去实现筛选（filter）、排序（sorted）、提取（map）或截断（limit）功能，Streams库已经自带了。 流与集合 Java 现有的集合和新的流都提供了接口，来配合代表元素型有序值的数据接口。所谓有序，就是说我们一般是按顺序取用值，而不是随机取用的。 集合与流之间的差异就在于什么时候进行计算。集合是一个内存中的数据结构，它包含数据结构中目前所有的值 —— 集合中的每个元素都得先算出来才能添加到集合中。 （可以往集合里加东西或者删东西，但是不管什么时候，集合中的每个元素都是放在内存里的，元素都得先算出来才能成为集合的一部分。）相比之下， 流则是在概念上固定的数据结构（不能添加或删除元素），其元素则是按需计算的。这是一种生产者－消费者的关系。从另一个角度来说，流就像是一个 延迟创建的集合：只有在消费者要求的时候才会计算值（用管理学的话说这就是需求驱动，甚至是实时制造)。 流只能遍历一次，遍历完了后，遍历完之后，我们就说这个流已经被消费掉了。例如，以下代码会抛出一个异常，说流已被消费掉了： 1234List&lt;String&gt; title = Arrays.asList(&quot;Java8&quot;, &quot;Stream&quot;, &quot;Example&quot;);Stream&lt;String&gt; s = title.stream();s.forEach(System.out::println);s.forEach(System.out::println); 输出： 1234Java8InActionException in thread &quot;main&quot; java.lang.IllegalStateException: stream has already been operated upon or closed 流操作 可以连接起来的流操作称为中间操作，关闭流的操作称为终端操作。 诸如 filter 或 sorted 等中间操作会返回另一个流。这让多个操作可以连接起来形成一个查询。重要的是，除非流水线上触发一个终端操作， 否则中间操作不会执行任何处理。这是因为中间操作一般都可以合并起来，在终端操作时一次性全部处理。看一个例子，在每个 Lambda 都打印当前处理的学生名： 12345678910111213List&lt;String&gt; goodStudentNames = students.stream() .filter(s -&gt; { System.out.println(&quot;filter-&gt; &quot; + s.getName()); return s.getScore() &gt;= GOOD; }) .map(s -&gt; { System.out.println(&quot;map-&gt; &quot; + s.getName()); return s.getName(); }) .limit(3) .collect(toList());System.out.println(goodStudentNames); 输出结果： 123456789101112filter-&gt; tomfilter-&gt; jerryfilter-&gt; 小李map-&gt; 小李filter-&gt; 小张map-&gt; 小张filter-&gt; 小明filter-&gt; 大熊filter-&gt; 小雨filter-&gt; 二狗map-&gt; 二狗[小李, 小张, 二狗] 可以看到 filter 的操作次数没有列表那么长，说明 filter 和 map 的操作有合并。","link":"/2020/06/08/%E7%AE%80%E4%BB%8BJava8%E7%9A%84%E6%B5%81/"},{"title":"redis5伪集群搭建","text":"Redis的发行版本解释 格式: major.minor.patchlevel 说明: major 主版本号 minor 次版本号，如果为偶数表示当前版本是一个稳定版本，否则是一个非稳定版本(不适合生产环境使用) patchlevel 补丁bug修复 编译 redis-6.x 开始的多线程代码依赖 C标准库中的新增类型 _Atomic 。但是 gcc 从 4.9 版本才开始正式和完整地支持 stdatomic（gcc-4.8.5 部分支持），而 centos7 默认的 gcc 版本为：4.8.5，无法正确的编译，所以需要升级 gcc 才能正确的编译 redis-6.x。升级方法： 1234567yum -y install centos-release-sclyum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils#临时有效，退出 shell 或重启会恢复原 gcc 版本scl enable devtoolset-9 bash#长期有效echo &quot;source /opt/rh/devtoolset-9/enable&quot; &gt;&gt;/etc/profile redis 的编译方法见下文。 单机安装 下载redis5.0.6二进制安装包 wget http://download.redis.io/releases/redis-5.0.6.tar.gz 解压到/usr/local目录下 tar -xzvf redis-5.0.6.tar.gz -C /usr/local 编译 cd /usr/local/redis-5.0.6 &amp;&amp; make 如果make出错，尝试改用 make MALLOC=libc 编译。 指定安装位置 make install PREFIX=/usr/local/redis 拷贝安装目录下配置文件到 /usr/local/redis/conf mkdir /usr/local/redis/conf cp /usr/local/redis-5.0.6/redis.conf /usr/local/redis/conf/ 修改配置文件 /usr/local/redis/conf/redis.conf vi /usr/local/redis/conf/redis.conf 1234567891011121314# 关闭保护模式protected-mode no# 以守护进程后台模式运行daemonize yes# 绑定本机ipbind 172.18.203.30# redis进程文件pidfile /usr/local/redis/redis_6379.pid# 日志文件logfile /usr/local/redis/log/redis_6379.log# 快照数据存放目录,一定是目录dir /usr/local/redis/data/# 认证密码requirepass 123456 默认的bind 接口是127.0.0.1，也就是本地回环地址。这样的话，访问redis服务只能通过本机的客户端连接，而无法通过远程连接。 改成监听的网卡IP或者 0.0.0.0 可使任意IP均可访问。 在redis目录创建log和data目录 7. 启动redis /usr/local/redis/bin/redis-server /usr/local/redis/conf/redis.conf 8. 查看是否启动成功 查看进程: ps aux | grep redis 查看日志: tail -fn 500 /usr/local/redis/log/redis_6379.log 命令端验证: /usr/local/redis/bin/redis-cli -h 172.18.203.30 -p 6379 172.18.203.30:6379&gt; ping PONG 9. 停止redis服务 /usr/local/redis/bin/redis-cli shutdown 集群搭建 redis建议三主三从共6个节点组成redis集群，测试环境可一台物理上启动6个redis节点，但生产环境至少要准备3台物理机。 服务器上搭建有6个节点的 Redis集群，在路径为/usr/local/redis/redis-cluster下创建6个文件夹代表6个实例。 123mkdir /usr/local/redis/redis-clustercd /usr/local/redis/redis-clustermkdir 7000 7001 7002 7003 7004 7005 分别给这6个文件夹，创建日志、数据存放路径。配置文件放实例路径（如：7000） 123456789101112mkdir /usr/local/redis/redis-cluster/7000/logmkdir /usr/local/redis/redis-cluster/7000/datamkdir /usr/local/redis/redis-cluster/7001/logmkdir /usr/local/redis/redis-cluster/7001/datamkdir /usr/local/redis/redis-cluster/7002/logmkdir /usr/local/redis/redis-cluster/7002/datamkdir /usr/local/redis/redis-cluster/7003/logmkdir /usr/local/redis/redis-cluster/7003/datamkdir /usr/local/redis/redis-cluster/7004/logmkdir /usr/local/redis/redis-cluster/7004/datamkdir /usr/local/redis/redis-cluster/7005/logmkdir /usr/local/redis/redis-cluster/7005/data 进安装redis-cluster的实例目录将 redis.conf 配置文件拷贝到 7000 这个目录，并重名命为 7000.conf 1cp /usr/local/redis-5.0.6/redis.conf /usr/local/redis/redis-cluster/7000/7000.conf 分别进入 7000 目录修改配置文件 1234567891011121314151617181920212223242526272829# 修改端口号对应目录的端口号port 7000# Ip绑定 绑定监听的网卡IP或者改为 0.0.0.0，bind 0.0.0.0# 数据位置dir ./ 改为&gt;dir /usr/local/redis/redis-cluster/7000/datadir /usr/local/redis/redis-cluster/7000/data# 启用集群模式cluster-enabled yes# 集群模式中节点的配置文件，文件不指定路径会在data生成cluster-config-file nodes-7000.conf# 超时时间cluster-node-timeout 5000# redis数据持久化开启，开启AOF模式appendonly yes# 后台运行daemonize yes# 非保护模式，允许 Redis 远程访问protected-mode no# pidfile 需要随着文件夹的不同调增pidfile /usr/local/redis/redis-cluster/7000/redis_7000.pid# 日志文件logfile /usr/local/redis/redis-cluster/7000/log/redis_7000.log# 如需密码则修改如下配置# 在：# requirepass foobared 下新增密码配置requirepass 123456# masterauth &lt;master-password&gt; 下新增密码配置masterauth 654321# nat或容器，内外网地址不一样的情况下，配置 cluster-announce-ip 为外网地址cluster-announce-ip 212.64.5.128 拷贝 7000.conf 到其它配置目录 12345cp /usr/local/redis/redis-cluster/7000/redis.conf /usr/local/redis/redis-cluster/7001/7001.confcp /usr/local/redis/redis-cluster/7000/redis.conf /usr/local/redis/redis-cluster/7002/7002.confcp /usr/local/redis/redis-cluster/7000/redis.conf /usr/local/redis/redis-cluster/7003/7003.confcp /usr/local/redis/redis-cluster/7000/redis.conf /usr/local/redis/redis-cluster/7004/7004.confcp /usr/local/redis/redis-cluster/7000/redis.conf /usr/local/redis/redis-cluster/7005/7005.conf 如上例修改剩下的 5 个配置，将 7000 改成对应的目录，如7001、7002等 加载 6 个redis配置文件启动 123456789# 进入redis的src目录启动redis (加载制定配置文件启动的方式) 6个都要启动，注意换配置文件位置/usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7000/7000.conf/usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7001/7001.conf/usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7002/7002.conf/usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7003/7003.conf/usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7004/7004.conf/usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7005/7005.conf# ps进程看看是否都启动ps -ef|grep redis 创建redis集群 创建redis4.x集群 12# 进入redis的src目录./redis-trib.rb create --replicas 1 0.0.0.0:7000 0.0.0.0:7001 0.0.0.0:7002 安装过程中，输入 yes，无报错，结尾出现[OK]即创建成功！ 创建redis5.x集群 12# redis5.x用redis-cli方式 不用redis4.x用的redis-trib.rb方式/usr/local/redis/bin/redis-cli --cluster create 0.0.0.0:7000 0.0.0.0:7001 0.0.0.0:7002 0.0.0.0:7003 0.0.0.0:7004 0.0.0.0:7005 --cluster-replicas 1 -a '123456' 需先启动redis。 –cluster-replicas 1 表示为集群中的每一个主节点指定一个从节点，即一比一的复制。 安装过程中，输入 yes，Reids5 集群搭建完成。 重建集群 先停止服务，再把各个节点下的 appendonly.aof，dump.rdb，nodes.conf 删除后，重建集群即可。删除各节点 data 中的文件。 123456rm -f /usr/local/redis/redis-cluster/7000/data/*rm -f /usr/local/redis/redis-cluster/7001/data/*rm -f /usr/local/redis/redis-cluster/7002/data/*rm -f /usr/local/redis/redis-cluster/7003/data/*rm -f /usr/local/redis/redis-cluster/7004/data/*rm -f /usr/local/redis/redis-cluster/7005/data/* Redis5集群其他操作 Redis5 提供了关闭集群的工具，在如下目录： /usr/local/redis-5.0.6/utils/create-cluster 打开此文件修改端口为我们自己的，如 6999。端口PROT设置为6999，NODES为6，工具会自动累加1 生成 7000-7005 六个节点 用于操作。 修改stop代码块： 123456789if [ &quot;$1&quot; == &quot;stop&quot; ]then while [ $((PORT &lt; ENDPORT)) != &quot;0&quot; ]; do PORT=$((PORT+1)) echo &quot;Stopping $PORT&quot; ./redis-cli -p $PORT -a &quot;123456&quot; shutdown nosave done exit 0fi 查看集群状态 1234// 查看节点详细信息redis-cli -a '123456' -p 7000 cluster info// 查看所有节点redis-cli -a '123456' -p 7000 cluster nodes 关闭集群 执行 create-cluster stop 可关闭集群。 启动集群 执行 create-cluster start 可启动集群 建议启动脚本自己编写。 123456789101112131415161718192021222324252627282930313233#!/bin/bashPORT=6999NODES=6ENDPORT=$((PORT+NODES))if [ &quot;$1&quot; == &quot;start&quot; ]then /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7000/7000.conf /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7001/7001.conf /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7002/7002.conf /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7003/7003.conf /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7004/7004.conf /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/7005/7005.conf# 配置集群时执行一次# /usr/local/redis/bin/redis-cli --cluster create 0.0.0.0:7000 0.0.0.0:7001 0.0.0.0:7002 0.0.0.0:7003 0.0.0.0:7004 0.0.0.0:7005 --cluster-replicas 1 -a '123456' exit 0fiif [ &quot;$1&quot; == &quot;stop&quot; ]then while [ $((PORT &lt; ENDPORT)) != &quot;0&quot; ]; do PORT=$((PORT+1)) echo &quot;Stopping $PORT&quot; ./redis-cli -p $PORT -a &quot;123456&quot; shutdown nosave done exit 0fiecho &quot;Usage: $0 [start|stop]&quot;echo &quot;start -- Launch Redis Cluster instances.&quot;echo &quot;stop -- Stop Redis Cluster instances.&quot; redis 启动 3 个警告 overcommit_memory 报警 WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add ‘vm.overcommit_memory = 1’ to /etc/sysctl.conf and then reboot or run the command ‘sysctl vm.overcommit_memory=1’ for this to take effect. 内核参数 overcommit_memory，它是 内存分配策略 可选值：0、1、2。 0: 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败， 并把错误返回给应用进程。 1: 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。 2: 表示内核允许分配超过所有物理内存和交换空间总和的内存 Linux 对大部分申请内存的请求都回复&quot;yes&quot;，以便能跑更多更大的程序。因为申请内存后，并不会马上使用内存。这种技术叫做 Overcommit。当 linux 发现内存不足时，会发生OOM killer(OOM=out-of-memory)。它会选择杀死一些进程(用户态进程，不是内核线程)，以便释放内存。 当 oom-killer 发生时，linux 会选择杀死哪些进程？选择进程的函数是 oom_badness 函数(在 mm/oom_kill.c 中)，该函数会计算每个进程的点数(0~1000)。点数越高，这个进程越有可能被杀死。每个进程的点数跟 oom_score_adj 有关，而且 oom_score_adj 可以被设置(-1000最低，1000最高)。 解决方法： 按提示的操作（将vm.overcommit_memory 设为1）即可： 有三种方式修改内核参数，但要有root权限：（直接修改宿主机的配置文件） 编辑/etc/sysctl.conf ，添加 vm.overcommit_memory=1，然后 sysctl -p 使配置文件生效 sysctl vm.overcommit_memory=1 echo 1 &gt; /proc/sys/vm/overcommit_memory 关闭THP WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command ‘echo madvise &gt; /sys/kernel/mm/transparent_hugepage/enabled’ as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled (set to ‘madvise’ or ‘never’). Linux kernel 在2.6.38内核增加了 THP 特性， 支持大内存页（2MB） 分配， 默认开启。 当开启时可以降低 fork 子进程的速度， 但 fork 操作之后， 每个内存页从原来4KB变为2MB， 会大幅增加重写期间父进程内存消耗。 同时每次写命令引起的复制内存页单位放大了512倍， 会拖慢写操作的执行时间， 导致大量写操作慢查询， 例如简单的 incr 命令也会出现在慢查询中。 因此 Redis 日志中建议将此特性进行禁用。 redis 给出的解决方案是将 THP 设置为 madvise 或 never。echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled。 为了使机器重启后 THP 配置依然生效，可以在 /etc/rc.local 中追加&quot;echo never&gt;/sys/kernel/mm/transparent_hugepage/enabled&quot;。 The TCP backlog setting WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 将 net.core.somaxconn=1024 添加到 /etc/sysctl.conf 中，然后执行 sysctl -p 生效配置。","link":"/2019/11/18/redis5%E4%BC%AA%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"title":"伪共享和Java缓存行","text":"对于多线程编程来说，一般要注意线程安全的问题，如果是要实现超高并发的中间件，特别是需要多线程处理列表、数组和队列的时候，就需要注意伪共享的问题。否则可能无法发挥多线程的优势，性能可能比单线程还差。 伪共享 介绍伪共享前先说说 SMP、Cache、MESI 几个概念。 SMP 系统 操作系统主要分下面两种： SMP——Symmetric Multi-Processing (SMP)，即对称多处理器结构 AMP——Asymmetric Multi-Processing (AMP) ，非对称多处理器结构 SMP的特征是：只有一个操作系统实例，运行在多个CPU上，每个CPU的结构都是一样的，内存、资源共享。这种系统有一个最大的特点就是共享所有资源。 AMP的特征是：多个CPU，各个CPU在架构上不一样，每个CPU内核运行一个独立的操作系统或同一操作系统的独立实例，每个CPU拥有自己的独立资源。这种结构最大的特点在于不共享资源。 我们平时使用的机器基本都是 SMP 系统。 Cache CPU 和主内存之间的运算速度是差异巨大的，在现今的 SMP 系统 中，会在 CPU 和主存间设置三级高速缓存，L1、L2 和 L3，读取顺序由先到后。可以简单理解为，L1 Cache分为指令缓存和数据缓存两种，L2 Cache只存储数据，L1 和 L2 都是每个核心都有，而 L3 被多核共享。 缓存系统中是以缓存行（cache line）为单位存储的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。 MESI MESI 是一致性协议，研究过 Java volatile 可能会比较熟悉，因为L1 L2是每个核心自己使用，而L3一般是多核共享，而不同核心又可能涉及共享变量问题，所以各个高速缓存间势必会有一致性的问题。MESI就是解决这些问题的一种协议或规范。 下面是关于 MESI 的一段说明： 在MESI协议中，每个Cache line有4个状态，可用2个bit表示，它们分别是: M(Modified)：这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中； E(Exclusive)：这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中； S(Shared)：这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中； I(Invalid)：这行数据无效。 什么是伪共享 到现在什么是伪共享，为什么它会影响到性能呢？ 先看一个图: 从图中可看到，thread0,thread1 分别由 core0，core1 调度，两线程都想更新彼此独立的两个变量，但是由于两个变量位于同一个cache line中，根据MESI cache line 的状态应该都是 Shared，而对于同一 cache line 的操作，core 间必须争夺主导权（ownership），如果 core0 抢到了，thread0 因此去更新cache line，会导致core1中的 cache line 状态变为 Invalid，随后 thread1 去更新时必须通知 core0 将 cache line 刷回主存，然后它再从主内存中 load 该 cache line 进高速缓存之后再进行修改，但该修改又会使得 core0 的 cache line 失效，thread0 重复上演 thread1 历史，这样导致了高速缓存并未起到应有的作用，反而影响了性能。 这就是对称多处理器（SMP）系统中一个著名的性能问题：伪共享。 Java 缓存行 对于出现伪共享的问题，根据上文介绍出现的原因，我们可以采用填充的方式来保证某个热点对象被隔离在不同的缓存行中，从而避免了多线程互相抢同一个 cache line，这样性能也就不会造成影响。 Java 8 方案 Java的各个版本在减少伪共享的做法都有区别，Java 8 以前的版本可以采用填充的方案，这里只具体介绍 Java 8 的实现方案。 JAVA 8中添加了一个 @Contended 的注解，对某字段加上该注解则表示该字段会单独占用一个缓存行（Cache Line）。 举个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public final class FalseSharing implements Runnable { public static int NUM_THREADS = 4; public final static long ITERATIONS = 500L * 1000L * 1000L; private final int arrayIndex; private static VolatileLong[] longs; public FalseSharing(final int arrayIndex) { this.arrayIndex = arrayIndex; } @Override public void run() { long i = ITERATIONS + 1; while (0 != --i) { longs[arrayIndex].value = i; } } public static void main(final String[] args) throws Exception { System.out.println(&quot;starting....&quot;); longs = new VolatileLong[NUM_THREADS]; for (int i = 0; i &lt; longs.length; i++) { longs[i] = new VolatileLong(); } final long start = System.nanoTime(); runTest(); System.out.println(&quot;duration = &quot; + (System.nanoTime() - start)); } private static void runTest() throws InterruptedException { Thread[] threads = new Thread[NUM_THREADS]; for (int i = 0; i &lt; threads.length; i++) { threads[i] = new Thread(new FalseSharing(i)); } for (Thread t : threads) { t.start(); } for (Thread t : threads) { t.join(); } }} 1234@Contendedpublic class VolatileLong { public volatile long value = 0L;} 这个例子是两个线程同时对同一个数组进写操作，见 runTest 函数 和 FalseSharing 类 的 run 函数。VolatileLong 类的 value 被 volatile 修饰， 在 run 函数中没有线程安全的问题。 测试情况： 当在 VolatileLong 类上加了 @Contended 注解时，输出： duration = 3581736100 当把 VolatileLong 类上的 @Contended 注解删除时，输出： duration = 20545682900 可以看到，不加@Contended 注解时，所消耗的时间大概是加 @Contended 注解 时的5倍。 注意 @Contended 注解要生效，需要加上虚拟机参数 -XX:-RestrictContended。 对于伪共享的问题呢，解决方案本质上就是填充，某种程度就是以空间换时间，这值得我们去思考。 伪共享的问题是程序性能的问题，虽然很重要，但优先级不要拔高，不要过早优化。","link":"/2020/12/07/%E4%BC%AA%E5%85%B1%E4%BA%AB%E5%92%8CJava%E7%BC%93%E5%AD%98%E8%A1%8C/"},{"title":"简介Java反射","text":"在介绍 Java 的反射之前，先对类对象、类加载做个简单说明，大概可以了解 Java 反射的基本原理。 类文件、类对象和元数据 类文件是编译 Java 源码文件（也可能是其他语言的源码文件）得到的中间格式，供 JVM 使用。类文件是二进制文件，目的不是供人类阅读。 运行时通过包含元数据的类对象（Class 对象）表示类文件，而类对象表示的是从中创建类文件的 Java 类型。 Class 对象保存了类相关的类型信息，当 new 一个新的对象或者引用静态成员变量时，JVM的类加载器会将 对应 Class 对象加载在 JVM 中，然后 JVM 再根据这个类型信息相关的 Class 对象构造实例对象或者提供静态变量的引用值。 在 Java 中，获取类对象有多种方式。其中最简单的方式是： 1Class&lt;?&gt; myCl = getClass(); 上述代码返回调用 getClass() 方法的实例对应的类对象。 类对象包含指定类型的元数据，包括这个类中定义的方法、字段和构造方法等。 例如，可以找出类文件中所有的弃用方法（弃用方法使用 @Deprecated 注解标记）： 12345678Class&lt;?&gt; clz = getClassFromDisk();for (Method m : clz.getMethods()) { for (Annotation a : m.getAnnotations()) { if (a.annotationType() == Deprecated.class) { System.out.println(m.getName()); } }} 类文件必须符合非常明确的布局才算合法，JVM 才能加载。 类加载 类加载是把新类型添加到运行中的 JVM 进程里的过程。这是新代码进入 Java 系统的唯一方式，也是 Java 平台中把数据变成代码的唯一方式。 Java 的类加载子系统实现了很多安全功能。类加载架构的核心安全机制是，只允许使用一种方式把可执行的代码传入进程——类。 因为创建新类只有一种方式，即使用 Classloader 类提供的功能，从字节流中加载类。 反射 反射是在运行时审查、操作和修改对象的能力，可以修改对象的结构和行为，甚至还能自我修改。 即便编译时不知道类型和方法名称，也能使用反射。反射使用类对象提供的基本元数据，能从类对象中找出方法或字段的名称，然后获取表示方法或字段的对象。 （使用 Class::newInstance() 或另一个构造方法）创建实例时也能让实例具有反射功能。如果有一个能反射的对象和一个 Method 对象，我们就能在之前 类型未知的对象上调用任何方法。 如何使用反射 任何反射操作的第一步都是获取一个 Class 对象，表示要处理的类型。有了这个对象，就能访问表示字段、方法或构造方法的对象，并将其应用于未知类型的实例。 获取未知类型的实例，最简单的方式是使用没有参数的构造方法，这个构造方法可以直接在 Class 对象上调用： 12Class&lt;?&gt; clz = getSomeClassObject();Object rcvr = clz.newInstance(); 如果构造方法有参数，必须找到具体需要使用的构造方法，并使用 Constructor 对象表示。 Method 对象是反射 API 提供的对象中最常使用的。Constructor 和 Field 对象在很多方面都和 Method 对象类似。 下面这个示例在 String 对象上调用 hashCode() 方法： 123456789101112131415public static void invokeHashCode() { Object rcvr = &quot;a&quot;; try { Class&lt;?&gt;[] argTypes = new Class[]{}; Object[] args = null; Method method = rcvr.getClass().getMethod(&quot;hashCode&quot;, argTypes); Object ret = method.invoke(rcvr, args); System.out.println(ret); } catch (IllegalArgumentException | NoSuchMethodException | SecurityException | IllegalAccessException | InvocationTargetException e) { e.printStackTrace(); }} 为了获取想使用的 Method 对象，我们在类对象上调用 getMethod() 方法，得到的是一个 Method 对象的引用，指向这个类中对应的公开方法。 下面这个例子是 通过反射构造一个 String 对象，在上面调用 hashCode() 方法： 123456789101112131415161718public static void invokeHashCode1() { Class&lt;?&gt; clz = String.class; try { Class&lt;?&gt;[] argTypes = new Class[]{}; Object[] args = null; Constructor&lt;?&gt; c = clz.getConstructor(String.class); Object rcvr = c.newInstance(&quot;b&quot;); Method method = rcvr.getClass().getMethod(&quot;hashCode&quot;, argTypes); Object ret = method.invoke(rcvr, args); System.out.println(ret); } catch (IllegalArgumentException | NoSuchMethodException | SecurityException | IllegalAccessException | InvocationTargetException | InstantiationException e) { e.printStackTrace(); }} 处理非公开方法的例子： 1234567891011121314public static void invokeSameSeed() { Class&lt;?&gt; clz = RandomTest.class; try { Constructor&lt;?&gt; c = clz.getConstructor(); Object rcvr = c.newInstance(); Method method = rcvr.getClass().getDeclaredMethod(&quot;sameSeed&quot;); method.setAccessible(true); method.invoke(rcvr); } catch (IllegalArgumentException | NoSuchMethodException | SecurityException | IllegalAccessException | InvocationTargetException | InstantiationException e) { e.printStackTrace(); }} 静态方法不构造实例处理非公开方法的例子： 12345678910public static void invokeSameSeed1() { try { Method method = RandomTest.class.getDeclaredMethod(&quot;sameSeed&quot;); method.setAccessible(true); method.invoke(null); } catch (IllegalArgumentException | NoSuchMethodException | SecurityException | IllegalAccessException | InvocationTargetException e) { e.printStackTrace(); }} 反射的问题 Java 的反射 API 往往是处理动态加载代码的唯一方式，不过 API 中有些让人头疼的地方， 处理起来稍微有点困难： 大量使用 Object[] 表示调用参数和其他实例； 大量使用 Class[] 表示类型； 同名方法可以重载，所以需要维护一个类型组成的数组，区分不同的方法； 不能很好地表示基本类型——需要手动打包和拆包。 何时使用反射 多数 Java 框架都会适度使用反射。如果编写的架构足够灵活，在运行时之前都不知道要处理什么代码，那么通常会使用反射。 反射在测试中也有广泛应用，例如，JUnit 和 TestNG 库都用到了反射，而且创建模拟对象也要使用反射。 如果你用过任何一个 Java 框架，即便没有意识到，也几乎可以确定，你使用的是具有反射功能的代码，比如 spring framework。","link":"/2020/05/08/%E7%AE%80%E4%BB%8BJava%E5%8F%8D%E5%B0%84/"},{"title":"charles手机端无法访问chls.pro&#x2F;ssl解决方案","text":"PC端 charles 启动后，手机端也配置了代理，但是无法访问 chls.pro/ssl，不能下载证书，可用下面几个步骤解决。 确认手机端和PC端已连接同一局域网，且配置代理ip和端口号与PC端 charles 的配置对应无误； charles中 access control settings中已匹配了手机端ip 关闭PC防火墙或者防火墙规则添加允许Charles Web Debugging Proxy连接 从PC端 charles 导出根证书，并安装到手机、信任证书","link":"/2024/11/16/charles%E6%89%8B%E6%9C%BA%E7%AB%AF%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AEchls-pro-ssl%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"NIO","slug":"NIO","link":"/tags/NIO/"},{"name":"software","slug":"software","link":"/tags/software/"},{"name":"cache","slug":"cache","link":"/tags/cache/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"microservice","slug":"microservice","link":"/tags/microservice/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"c++","slug":"c","link":"/tags/c/"}],"categories":[]}